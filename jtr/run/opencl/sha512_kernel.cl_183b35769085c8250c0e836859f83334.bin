//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.6
.target sm_86, texmode_independent
.address_size 64

	// .globl	kernel_sha512
.const .align 4 .b8 h[32] = {103, 230, 9, 106, 133, 174, 103, 187, 114, 243, 110, 60, 58, 245, 79, 165, 127, 82, 14, 81, 140, 104, 5, 155, 171, 217, 131, 31, 25, 205, 224, 91};
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};
.const .align 8 .b8 K[640] = {34, 174, 40, 215, 152, 47, 138, 66, 205, 101, 239, 35, 145, 68, 55, 113, 47, 59, 77, 236, 207, 251, 192, 181, 188, 219, 137, 129, 165, 219, 181, 233, 56, 181, 72, 243, 91, 194, 86, 57, 25, 208, 5, 182, 241, 17, 241, 89, 155, 79, 25, 175, 164, 130, 63, 146, 24, 129, 109, 218, 213, 94, 28, 171, 66, 2, 3, 163, 152, 170, 7, 216, 190, 111, 112, 69, 1, 91, 131, 18, 140, 178, 228, 78, 190, 133, 49, 36, 226, 180, 255, 213, 195, 125, 12, 85, 111, 137, 123, 242, 116, 93, 190, 114, 177, 150, 22, 59, 254, 177, 222, 128, 53, 18, 199, 37, 167, 6, 220, 155, 148, 38, 105, 207, 116, 241, 155, 193, 210, 74, 241, 158, 193, 105, 155, 228, 227, 37, 79, 56, 134, 71, 190, 239, 181, 213, 140, 139, 198, 157, 193, 15, 101, 156, 172, 119, 204, 161, 12, 36, 117, 2, 43, 89, 111, 44, 233, 45, 131, 228, 166, 110, 170, 132, 116, 74, 212, 251, 65, 189, 220, 169, 176, 92, 181, 83, 17, 131, 218, 136, 249, 118, 171, 223, 102, 238, 82, 81, 62, 152, 16, 50, 180, 45, 109, 198, 49, 168, 63, 33, 251, 152, 200, 39, 3, 176, 228, 14, 239, 190, 199, 127, 89, 191, 194, 143, 168, 61, 243, 11, 224, 198, 37, 167, 10, 147, 71, 145, 167, 213, 111, 130, 3, 224, 81, 99, 202, 6, 112, 110, 14, 10, 103, 41, 41, 20, 252, 47, 210, 70, 133, 10, 183, 39, 38, 201, 38, 92, 56, 33, 27, 46, 237, 42, 196, 90, 252, 109, 44, 77, 223, 179, 149, 157, 19, 13, 56, 83, 222, 99, 175, 139, 84, 115, 10, 101, 168, 178, 119, 60, 187, 10, 106, 118, 230, 174, 237, 71, 46, 201, 194, 129, 59, 53, 130, 20, 133, 44, 114, 146, 100, 3, 241, 76, 161, 232, 191, 162, 1, 48, 66, 188, 75, 102, 26, 168, 145, 151, 248, 208, 112, 139, 75, 194, 48, 190, 84, 6, 163, 81, 108, 199, 24, 82, 239, 214, 25, 232, 146, 209, 16, 169, 101, 85, 36, 6, 153, 214, 42, 32, 113, 87, 133, 53, 14, 244, 184, 209, 187, 50, 112, 160, 106, 16, 200, 208, 210, 184, 22, 193, 164, 25, 83, 171, 65, 81, 8, 108, 55, 30, 153, 235, 142, 223, 76, 119, 72, 39, 168, 72, 155, 225, 181, 188, 176, 52, 99, 90, 201, 197, 179, 12, 28, 57, 203, 138, 65, 227, 74, 170, 216, 78, 115, 227, 99, 119, 79, 202, 156, 91, 163, 184, 178, 214, 243, 111, 46, 104, 252, 178, 239, 93, 238, 130, 143, 116, 96, 47, 23, 67, 111, 99, 165, 120, 114, 171, 240, 161, 20, 120, 200, 132, 236, 57, 100, 26, 8, 2, 199, 140, 40, 30, 99, 35, 250, 255, 190, 144, 233, 189, 130, 222, 235, 108, 80, 164, 21, 121, 198, 178, 247, 163, 249, 190, 43, 83, 114, 227, 242, 120, 113, 198, 156, 97, 38, 234, 206, 62, 39, 202, 7, 194, 192, 33, 199, 184, 134, 209, 30, 235, 224, 205, 214, 125, 218, 234, 120, 209, 110, 238, 127, 79, 125, 245, 186, 111, 23, 114, 170, 103, 240, 6, 166, 152, 200, 162, 197, 125, 99, 10, 174, 13, 249, 190, 4, 152, 63, 17, 27, 71, 28, 19, 53, 11, 113, 27, 132, 125, 4, 35, 245, 119, 219, 40, 147, 36, 199, 64, 123, 171, 202, 50, 188, 190, 201, 21, 10, 190, 158, 60, 76, 13, 16, 156, 196, 103, 29, 67, 182, 66, 62, 203, 190, 212, 197, 76, 42, 126, 101, 252, 156, 41, 127, 89, 236, 250, 214, 58, 171, 111, 203, 95, 23, 88, 71, 74, 140, 25, 68, 108};

.entry kernel_sha512(
	.param .u64 .ptr .global .align 1 kernel_sha512_param_0,
	.param .u64 .ptr .global .align 8 kernel_sha512_param_1
)
{
	.local .align 8 .b8 	__local_depot0[136];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<19>;
	.reg .b32 	%r<1987>;
	.reg .b64 	%rd<2722>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd17, [kernel_sha512_param_0];
	ld.param.u64 	%rd18, [kernel_sha512_param_1];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r26, %ctaid.x;
	mov.u32 	%r27, %ntid.x;
	mov.u32 	%r28, %tid.x;
	mov.b32 	%r29, %envreg3;
	add.s32 	%r30, %r28, %r29;
	mad.lo.s32 	%r31, %r27, %r26, %r30;
	cvt.u64.u32 	%rd2, %r31;
	mul.wide.u32 	%rd20, %r31, 22;
	add.s64 	%rd21, %rd17, %rd20;
	ld.global.u8 	%rs1, [%rd21];
	cvt.u32.u16 	%r1, %rs1;
	setp.eq.s16 	%p1, %rs1, 0;
	@%p1 bra 	$L__BB0_7;

	add.s32 	%r33, %r1, -1;
	and.b32  	%r1981, %r1, 3;
	setp.lt.u32 	%p2, %r33, 3;
	mov.u32 	%r1980, 0;
	@%p2 bra 	$L__BB0_4;

	sub.s32 	%r1979, %r1, %r1981;
	mov.u32 	%r1980, 0;
	mul.lo.s64 	%rd23, %rd2, 22;
	add.s64 	%rd24, %rd17, %rd23;

$L__BB0_3:
	cvt.u64.u32 	%rd22, %r1980;
	add.s64 	%rd25, %rd24, %rd22;
	ld.global.u8 	%rs2, [%rd25+1];
	add.s64 	%rd26, %rd1, %rd22;
	st.local.u8 	[%rd26], %rs2;
	ld.global.u8 	%rs3, [%rd25+2];
	st.local.u8 	[%rd26+1], %rs3;
	ld.global.u8 	%rs4, [%rd25+3];
	st.local.u8 	[%rd26+2], %rs4;
	ld.global.u8 	%rs5, [%rd25+4];
	st.local.u8 	[%rd26+3], %rs5;
	add.s32 	%r1980, %r1980, 4;
	add.s32 	%r1979, %r1979, -4;
	setp.ne.s32 	%p3, %r1979, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s32 	%p4, %r1981, 0;
	@%p4 bra 	$L__BB0_7;

	cvt.u64.u32 	%rd27, %r1980;
	add.s64 	%rd2718, %rd1, %rd27;
	mul.lo.s64 	%rd28, %rd2, 22;
	add.s64 	%rd29, %rd17, %rd28;
	add.s64 	%rd30, %rd29, %rd27;
	add.s64 	%rd2717, %rd30, 1;

$L__BB0_6:
	.pragma "nounroll";
	ld.global.u8 	%rs6, [%rd2717];
	st.local.u8 	[%rd2718], %rs6;
	add.s64 	%rd2718, %rd2718, 1;
	add.s64 	%rd2717, %rd2717, 1;
	add.s32 	%r1981, %r1981, -1;
	setp.ne.s32 	%p5, %r1981, 0;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	add.s64 	%rd9, %rd1, 128;
	st.local.u32 	[%rd1+128], %r1;
	cvt.u64.u16 	%rd31, %rs1;
	add.s64 	%rd10, %rd1, %rd31;
	mov.u16 	%rs7, 128;
	st.local.u8 	[%rd10], %rs7;
	add.s32 	%r1982, %r1, 1;
	add.s16 	%rs8, %rs1, 1;
	and.b16  	%rs9, %rs8, 3;
	setp.eq.s16 	%p6, %rs9, 0;
	@%p6 bra 	$L__BB0_12;

	mov.u16 	%rs10, 0;
	st.local.u8 	[%rd10+1], %rs10;
	add.s32 	%r1982, %r1, 2;
	add.s16 	%rs11, %rs1, 2;
	and.b16  	%rs12, %rs11, 3;
	setp.eq.s16 	%p7, %rs12, 0;
	@%p7 bra 	$L__BB0_12;

	st.local.u8 	[%rd10+2], %rs10;
	add.s32 	%r1982, %r1, 3;
	add.s16 	%rs14, %rs1, -1;
	and.b16  	%rs15, %rs14, 3;
	setp.eq.s16 	%p8, %rs15, 0;
	@%p8 bra 	$L__BB0_12;

	mov.u16 	%rs16, 0;
	st.local.u8 	[%rd10+3], %rs16;
	add.s32 	%r1982, %r1, 4;
	and.b16  	%rs17, %rs1, 3;
	setp.eq.s16 	%p9, %rs17, 0;
	@%p9 bra 	$L__BB0_12;

	st.local.u8 	[%rd10+4], %rs16;
	add.s32 	%r1982, %r1, 5;

$L__BB0_12:
	setp.gt.u32 	%p10, %r1982, 127;
	@%p10 bra 	$L__BB0_18;

	and.b32  	%r35, %r1982, 508;
	cvt.u64.u32 	%rd32, %r35;
	add.s64 	%rd2721, %rd1, %rd32;
	mov.u32 	%r36, 127;
	sub.s32 	%r17, %r36, %r1982;
	shr.u32 	%r37, %r17, 2;
	add.s32 	%r38, %r37, 1;
	and.b32  	%r1984, %r38, 3;
	setp.eq.s32 	%p11, %r1984, 0;
	@%p11 bra 	$L__BB0_16;

	mov.u64 	%rd2719, %rd2721;

$L__BB0_15:
	.pragma "nounroll";
	add.s64 	%rd2721, %rd2719, 4;
	mov.u32 	%r39, 0;
	st.local.u32 	[%rd2719], %r39;
	add.s32 	%r1982, %r1982, 4;
	add.s32 	%r1984, %r1984, -1;
	setp.ne.s32 	%p12, %r1984, 0;
	mov.u64 	%rd2719, %rd2721;
	@%p12 bra 	$L__BB0_15;

$L__BB0_16:
	setp.lt.u32 	%p13, %r17, 12;
	@%p13 bra 	$L__BB0_18;

$L__BB0_17:
	mov.u32 	%r40, 0;
	st.local.u32 	[%rd2721], %r40;
	st.local.u32 	[%rd2721+4], %r40;
	st.local.u32 	[%rd2721+8], %r40;
	st.local.u32 	[%rd2721+12], %r40;
	add.s32 	%r1982, %r1982, 16;
	setp.lt.u32 	%p14, %r1982, 128;
	add.s64 	%rd2721, %rd2721, 16;
	@%p14 bra 	$L__BB0_17;

$L__BB0_18:
	ld.local.u32 	%r41, [%rd9];
	cvt.u64.u32 	%rd33, %r41;
	shl.b64 	%rd34, %rd33, 59;
	shl.b64 	%rd35, %rd33, 43;
	and.b64  	%rd36, %rd35, 71776119061217280;
	or.b64  	%rd37, %rd36, %rd34;
	mul.wide.u32 	%rd38, %r41, 134217728;
	and.b64  	%rd39, %rd38, 280375465082880;
	or.b64  	%rd40, %rd37, %rd39;
	mul.wide.u32 	%rd41, %r41, 2048;
	and.b64  	%rd42, %rd41, 1095216660480;
	or.b64  	%rd43, %rd40, %rd42;
	shr.u32 	%r42, %r41, 5;
	and.b32  	%r43, %r42, 117440512;
	cvt.u64.u32 	%rd44, %r43;
	or.b64  	%rd45, %rd43, %rd44;
	st.local.u64 	[%rd9+-8], %rd45;
	ld.local.u64 	%rd46, [%rd9+-128];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %rd46;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %rd46;
	}
	mov.u32 	%r46, 291;
	mov.u32 	%r47, 0;
	prmt.b32 	%r48, %r44, %r47, %r46;
	prmt.b32 	%r49, %r45, %r47, %r46;
	mov.b64 	%rd47, {%r49, %r48};
	ld.local.u64 	%rd48, [%rd9+-120];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %rd48;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %rd48;
	}
	prmt.b32 	%r52, %r50, %r47, %r46;
	prmt.b32 	%r53, %r51, %r47, %r46;
	mov.b64 	%rd49, {%r53, %r52};
	ld.local.u64 	%rd50, [%rd9+-112];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %rd50;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %rd50;
	}
	prmt.b32 	%r56, %r54, %r47, %r46;
	prmt.b32 	%r57, %r55, %r47, %r46;
	mov.b64 	%rd51, {%r57, %r56};
	ld.local.u64 	%rd52, [%rd9+-104];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r58, %temp}, %rd52;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %rd52;
	}
	prmt.b32 	%r60, %r58, %r47, %r46;
	prmt.b32 	%r61, %r59, %r47, %r46;
	mov.b64 	%rd53, {%r61, %r60};
	ld.local.u64 	%rd54, [%rd9+-96];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r62, %temp}, %rd54;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r63}, %rd54;
	}
	prmt.b32 	%r64, %r62, %r47, %r46;
	prmt.b32 	%r65, %r63, %r47, %r46;
	mov.b64 	%rd55, {%r65, %r64};
	ld.local.u64 	%rd56, [%rd9+-88];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %rd56;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %rd56;
	}
	prmt.b32 	%r68, %r66, %r47, %r46;
	prmt.b32 	%r69, %r67, %r47, %r46;
	mov.b64 	%rd57, {%r69, %r68};
	ld.local.u64 	%rd58, [%rd9+-80];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r70, %temp}, %rd58;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %rd58;
	}
	prmt.b32 	%r72, %r70, %r47, %r46;
	prmt.b32 	%r73, %r71, %r47, %r46;
	mov.b64 	%rd59, {%r73, %r72};
	ld.local.u64 	%rd60, [%rd9+-72];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r74, %temp}, %rd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r75}, %rd60;
	}
	prmt.b32 	%r76, %r74, %r47, %r46;
	prmt.b32 	%r77, %r75, %r47, %r46;
	mov.b64 	%rd61, {%r77, %r76};
	ld.local.u64 	%rd62, [%rd9+-64];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r78, %temp}, %rd62;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r79}, %rd62;
	}
	prmt.b32 	%r80, %r78, %r47, %r46;
	prmt.b32 	%r81, %r79, %r47, %r46;
	mov.b64 	%rd63, {%r81, %r80};
	ld.local.u64 	%rd64, [%rd9+-56];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r82, %temp}, %rd64;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r83}, %rd64;
	}
	prmt.b32 	%r84, %r82, %r47, %r46;
	prmt.b32 	%r85, %r83, %r47, %r46;
	mov.b64 	%rd65, {%r85, %r84};
	ld.local.u64 	%rd66, [%rd9+-48];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r86, %temp}, %rd66;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r87}, %rd66;
	}
	prmt.b32 	%r88, %r86, %r47, %r46;
	prmt.b32 	%r89, %r87, %r47, %r46;
	mov.b64 	%rd67, {%r89, %r88};
	ld.local.u64 	%rd68, [%rd9+-40];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r90, %temp}, %rd68;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r91}, %rd68;
	}
	prmt.b32 	%r92, %r90, %r47, %r46;
	prmt.b32 	%r93, %r91, %r47, %r46;
	mov.b64 	%rd69, {%r93, %r92};
	ld.local.u64 	%rd70, [%rd9+-32];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r94, %temp}, %rd70;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r95}, %rd70;
	}
	prmt.b32 	%r96, %r94, %r47, %r46;
	prmt.b32 	%r97, %r95, %r47, %r46;
	mov.b64 	%rd71, {%r97, %r96};
	ld.local.u64 	%rd72, [%rd9+-24];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r98, %temp}, %rd72;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r99}, %rd72;
	}
	prmt.b32 	%r100, %r98, %r47, %r46;
	prmt.b32 	%r101, %r99, %r47, %r46;
	mov.b64 	%rd73, {%r101, %r100};
	ld.local.u64 	%rd74, [%rd9+-16];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r102, %temp}, %rd74;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r103}, %rd74;
	}
	prmt.b32 	%r104, %r102, %r47, %r46;
	prmt.b32 	%r105, %r103, %r47, %r46;
	mov.b64 	%rd75, {%r105, %r104};
	cvt.u64.u32 	%rd76, %r42;
	shl.b64 	%rd77, %rd76, 8;
	and.b64  	%rd78, %rd77, 30064771072;
	mul.wide.u32 	%rd79, %r41, 8;
	and.b64  	%rd80, %rd79, 4278190080;
	or.b64  	%rd81, %rd78, %rd80;
	and.b64  	%rd82, %rd79, 16711680;
	or.b64  	%rd83, %rd81, %rd82;
	and.b64  	%rd84, %rd79, 65280;
	or.b64  	%rd85, %rd83, %rd84;
	and.b64  	%rd86, %rd79, 248;
	or.b64  	%rd87, %rd85, %rd86;
	mov.u64 	%rd88, 5840696475078001361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r106,%dummy}, %rd88;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r107}, %rd88;
	}
	shf.r.wrap.b32 	%r108, %r107, %r106, 18;
	shf.r.wrap.b32 	%r109, %r106, %r107, 18;
	mov.b64 	%rd89, {%r109, %r108};
	shf.r.wrap.b32 	%r110, %r107, %r106, 14;
	shf.r.wrap.b32 	%r111, %r106, %r107, 14;
	mov.b64 	%rd90, {%r111, %r110};
	xor.b64  	%rd91, %rd90, %rd89;
	shf.l.wrap.b32 	%r112, %r106, %r107, 23;
	shf.l.wrap.b32 	%r113, %r107, %r106, 23;
	mov.b64 	%rd92, {%r113, %r112};
	xor.b64  	%rd93, %rd91, %rd92;
	add.s64 	%rd94, %rd93, %rd47;
	mov.u64 	%rd95, 7640891576956012808;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r114}, %rd95;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r115,%dummy}, %rd95;
	}
	shf.l.wrap.b32 	%r116, %r115, %r114, 30;
	shf.l.wrap.b32 	%r117, %r114, %r115, 30;
	mov.b64 	%rd96, {%r117, %r116};
	shf.r.wrap.b32 	%r118, %r114, %r115, 28;
	shf.r.wrap.b32 	%r119, %r115, %r114, 28;
	mov.b64 	%rd97, {%r119, %r118};
	xor.b64  	%rd98, %rd97, %rd96;
	shf.l.wrap.b32 	%r120, %r115, %r114, 25;
	shf.l.wrap.b32 	%r121, %r114, %r115, 25;
	mov.b64 	%rd99, {%r121, %r120};
	xor.b64  	%rd100, %rd98, %rd99;
	add.s64 	%rd101, %rd100, %rd94;
	add.s64 	%rd102, %rd94, 7151922335638569927;
	add.s64 	%rd103, %rd101, -549249324024138239;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r122,%dummy}, %rd102;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r123}, %rd102;
	}
	shf.r.wrap.b32 	%r124, %r123, %r122, 14;
	shf.r.wrap.b32 	%r125, %r122, %r123, 14;
	mov.b64 	%rd104, {%r125, %r124};
	shf.r.wrap.b32 	%r126, %r123, %r122, 18;
	shf.r.wrap.b32 	%r127, %r122, %r123, 18;
	mov.b64 	%rd105, {%r127, %r126};
	xor.b64  	%rd106, %rd104, %rd105;
	shf.l.wrap.b32 	%r128, %r122, %r123, 23;
	shf.l.wrap.b32 	%r129, %r123, %r122, 23;
	mov.b64 	%rd107, {%r129, %r128};
	xor.b64  	%rd108, %rd106, %rd107;
	and.b64  	%rd109, %rd102, -3887949035690463538;
	xor.b64  	%rd110, %rd109, -7276294671716946913;
	add.s64 	%rd111, %rd108, %rd110;
	add.s64 	%rd112, %rd111, %rd49;
	and.b64  	%rd113, %rd103, -3355664534840381901;
	or.b64  	%rd114, %rd113, 3026882967131160840;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r130,%dummy}, %rd103;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r131}, %rd103;
	}
	shf.r.wrap.b32 	%r132, %r131, %r130, 28;
	shf.r.wrap.b32 	%r133, %r130, %r131, 28;
	mov.b64 	%rd115, {%r133, %r132};
	shf.l.wrap.b32 	%r134, %r130, %r131, 30;
	shf.l.wrap.b32 	%r135, %r131, %r130, 30;
	mov.b64 	%rd116, {%r135, %r134};
	xor.b64  	%rd117, %rd115, %rd116;
	shf.l.wrap.b32 	%r136, %r130, %r131, 25;
	shf.l.wrap.b32 	%r137, %r131, %r130, 25;
	mov.b64 	%rd118, {%r137, %r136};
	xor.b64  	%rd119, %rd117, %rd118;
	add.s64 	%rd120, %rd114, %rd119;
	add.s64 	%rd121, %rd120, %rd112;
	add.s64 	%rd122, %rd112, -3663095898801038493;
	add.s64 	%rd123, %rd121, -8017781463737883848;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r138,%dummy}, %rd122;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r139}, %rd122;
	}
	shf.r.wrap.b32 	%r140, %r139, %r138, 14;
	shf.r.wrap.b32 	%r141, %r138, %r139, 14;
	mov.b64 	%rd124, {%r141, %r140};
	shf.r.wrap.b32 	%r142, %r139, %r138, 18;
	shf.r.wrap.b32 	%r143, %r138, %r139, 18;
	mov.b64 	%rd125, {%r143, %r142};
	xor.b64  	%rd126, %rd124, %rd125;
	shf.l.wrap.b32 	%r144, %r138, %r139, 23;
	shf.l.wrap.b32 	%r145, %r139, %r138, 23;
	mov.b64 	%rd127, {%r145, %r144};
	xor.b64  	%rd128, %rd126, %rd127;
	xor.b64  	%rd129, %rd102, 5840696475078001361;
	and.b64  	%rd130, %rd129, %rd122;
	xor.b64  	%rd131, %rd130, 5840696475078001361;
	add.s64 	%rd132, %rd128, %rd131;
	add.s64 	%rd133, %rd132, %rd51;
	and.b64  	%rd134, %rd123, %rd103;
	or.b64  	%rd135, %rd123, %rd103;
	and.b64  	%rd136, %rd135, 7640891576956012808;
	or.b64  	%rd137, %rd136, %rd134;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r146,%dummy}, %rd123;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r147}, %rd123;
	}
	shf.r.wrap.b32 	%r148, %r147, %r146, 28;
	shf.r.wrap.b32 	%r149, %r146, %r147, 28;
	mov.b64 	%rd138, {%r149, %r148};
	shf.l.wrap.b32 	%r150, %r146, %r147, 30;
	shf.l.wrap.b32 	%r151, %r147, %r146, 30;
	mov.b64 	%rd139, {%r151, %r150};
	xor.b64  	%rd140, %rd138, %rd139;
	shf.l.wrap.b32 	%r152, %r146, %r147, 25;
	shf.l.wrap.b32 	%r153, %r147, %r146, 25;
	mov.b64 	%rd141, {%r153, %r152};
	xor.b64  	%rd142, %rd140, %rd141;
	add.s64 	%rd143, %rd137, %rd142;
	add.s64 	%rd144, %rd143, %rd133;
	add.s64 	%rd145, %rd133, 877659737583668873;
	add.s64 	%rd146, %rd144, 5820449915117741902;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r154,%dummy}, %rd145;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r155}, %rd145;
	}
	shf.r.wrap.b32 	%r156, %r155, %r154, 14;
	shf.r.wrap.b32 	%r157, %r154, %r155, 14;
	mov.b64 	%rd147, {%r157, %r156};
	shf.r.wrap.b32 	%r158, %r155, %r154, 18;
	shf.r.wrap.b32 	%r159, %r154, %r155, 18;
	mov.b64 	%rd148, {%r159, %r158};
	xor.b64  	%rd149, %rd147, %rd148;
	shf.l.wrap.b32 	%r160, %r154, %r155, 23;
	shf.l.wrap.b32 	%r161, %r155, %r154, 23;
	mov.b64 	%rd150, {%r161, %r160};
	xor.b64  	%rd151, %rd149, %rd150;
	xor.b64  	%rd152, %rd102, %rd122;
	and.b64  	%rd153, %rd152, %rd145;
	xor.b64  	%rd154, %rd153, %rd102;
	add.s64 	%rd155, %rd151, %rd154;
	add.s64 	%rd156, %rd155, %rd53;
	and.b64  	%rd157, %rd146, %rd123;
	or.b64  	%rd158, %rd146, %rd123;
	and.b64  	%rd159, %rd158, %rd103;
	or.b64  	%rd160, %rd159, %rd157;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r162,%dummy}, %rd146;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r163}, %rd146;
	}
	shf.r.wrap.b32 	%r164, %r163, %r162, 28;
	shf.r.wrap.b32 	%r165, %r162, %r163, 28;
	mov.b64 	%rd161, {%r165, %r164};
	shf.l.wrap.b32 	%r166, %r162, %r163, 30;
	shf.l.wrap.b32 	%r167, %r163, %r162, 30;
	mov.b64 	%rd162, {%r167, %r166};
	xor.b64  	%rd163, %rd161, %rd162;
	shf.l.wrap.b32 	%r168, %r162, %r163, 25;
	shf.l.wrap.b32 	%r169, %r163, %r162, 25;
	mov.b64 	%rd164, {%r169, %r168};
	xor.b64  	%rd165, %rd163, %rd164;
	add.s64 	%rd166, %rd160, %rd165;
	add.s64 	%rd167, %rd166, %rd156;
	add.s64 	%rd168, %rd156, -6571292209873868907;
	add.s64 	%rd169, %rd167, 4234560286879669901;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r170,%dummy}, %rd168;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r171}, %rd168;
	}
	shf.r.wrap.b32 	%r172, %r171, %r170, 14;
	shf.r.wrap.b32 	%r173, %r170, %r171, 14;
	mov.b64 	%rd170, {%r173, %r172};
	shf.r.wrap.b32 	%r174, %r171, %r170, 18;
	shf.r.wrap.b32 	%r175, %r170, %r171, 18;
	mov.b64 	%rd171, {%r175, %r174};
	xor.b64  	%rd172, %rd170, %rd171;
	shf.l.wrap.b32 	%r176, %r170, %r171, 23;
	shf.l.wrap.b32 	%r177, %r171, %r170, 23;
	mov.b64 	%rd173, {%r177, %r176};
	xor.b64  	%rd174, %rd172, %rd173;
	xor.b64  	%rd175, %rd122, %rd145;
	and.b64  	%rd176, %rd175, %rd168;
	xor.b64  	%rd177, %rd176, %rd122;
	add.s64 	%rd178, %rd174, %rd102;
	add.s64 	%rd179, %rd178, %rd177;
	add.s64 	%rd180, %rd179, %rd55;
	add.s64 	%rd181, %rd180, 4131703408338449720;
	and.b64  	%rd182, %rd169, %rd146;
	or.b64  	%rd183, %rd169, %rd146;
	and.b64  	%rd184, %rd183, %rd123;
	or.b64  	%rd185, %rd184, %rd182;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r178,%dummy}, %rd169;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r179}, %rd169;
	}
	shf.r.wrap.b32 	%r180, %r179, %r178, 28;
	shf.r.wrap.b32 	%r181, %r178, %r179, 28;
	mov.b64 	%rd186, {%r181, %r180};
	shf.l.wrap.b32 	%r182, %r178, %r179, 30;
	shf.l.wrap.b32 	%r183, %r179, %r178, 30;
	mov.b64 	%rd187, {%r183, %r182};
	xor.b64  	%rd188, %rd186, %rd187;
	shf.l.wrap.b32 	%r184, %r178, %r179, 25;
	shf.l.wrap.b32 	%r185, %r179, %r178, 25;
	mov.b64 	%rd189, {%r185, %r184};
	xor.b64  	%rd190, %rd188, %rd189;
	add.s64 	%rd191, %rd185, %rd190;
	add.s64 	%rd192, %rd181, %rd103;
	add.s64 	%rd193, %rd191, %rd181;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r186,%dummy}, %rd192;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r187}, %rd192;
	}
	shf.r.wrap.b32 	%r188, %r187, %r186, 14;
	shf.r.wrap.b32 	%r189, %r186, %r187, 14;
	mov.b64 	%rd194, {%r189, %r188};
	shf.r.wrap.b32 	%r190, %r187, %r186, 18;
	shf.r.wrap.b32 	%r191, %r186, %r187, 18;
	mov.b64 	%rd195, {%r191, %r190};
	xor.b64  	%rd196, %rd194, %rd195;
	shf.l.wrap.b32 	%r192, %r186, %r187, 23;
	shf.l.wrap.b32 	%r193, %r187, %r186, 23;
	mov.b64 	%rd197, {%r193, %r192};
	xor.b64  	%rd198, %rd196, %rd197;
	xor.b64  	%rd199, %rd145, %rd168;
	and.b64  	%rd200, %rd199, %rd192;
	xor.b64  	%rd201, %rd200, %rd145;
	add.s64 	%rd202, %rd198, %rd122;
	add.s64 	%rd203, %rd202, %rd201;
	add.s64 	%rd204, %rd203, %rd57;
	add.s64 	%rd205, %rd204, 6480981068601479193;
	and.b64  	%rd206, %rd193, %rd169;
	or.b64  	%rd207, %rd193, %rd169;
	and.b64  	%rd208, %rd207, %rd146;
	or.b64  	%rd209, %rd208, %rd206;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r194,%dummy}, %rd193;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r195}, %rd193;
	}
	shf.r.wrap.b32 	%r196, %r195, %r194, 28;
	shf.r.wrap.b32 	%r197, %r194, %r195, 28;
	mov.b64 	%rd210, {%r197, %r196};
	shf.l.wrap.b32 	%r198, %r194, %r195, 30;
	shf.l.wrap.b32 	%r199, %r195, %r194, 30;
	mov.b64 	%rd211, {%r199, %r198};
	xor.b64  	%rd212, %rd210, %rd211;
	shf.l.wrap.b32 	%r200, %r194, %r195, 25;
	shf.l.wrap.b32 	%r201, %r195, %r194, 25;
	mov.b64 	%rd213, {%r201, %r200};
	xor.b64  	%rd214, %rd212, %rd213;
	add.s64 	%rd215, %rd209, %rd214;
	add.s64 	%rd216, %rd205, %rd123;
	add.s64 	%rd217, %rd215, %rd205;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r202,%dummy}, %rd216;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r203}, %rd216;
	}
	shf.r.wrap.b32 	%r204, %r203, %r202, 14;
	shf.r.wrap.b32 	%r205, %r202, %r203, 14;
	mov.b64 	%rd218, {%r205, %r204};
	shf.r.wrap.b32 	%r206, %r203, %r202, 18;
	shf.r.wrap.b32 	%r207, %r202, %r203, 18;
	mov.b64 	%rd219, {%r207, %r206};
	xor.b64  	%rd220, %rd218, %rd219;
	shf.l.wrap.b32 	%r208, %r202, %r203, 23;
	shf.l.wrap.b32 	%r209, %r203, %r202, 23;
	mov.b64 	%rd221, {%r209, %r208};
	xor.b64  	%rd222, %rd220, %rd221;
	xor.b64  	%rd223, %rd168, %rd192;
	and.b64  	%rd224, %rd223, %rd216;
	xor.b64  	%rd225, %rd224, %rd168;
	add.s64 	%rd226, %rd222, %rd145;
	add.s64 	%rd227, %rd226, %rd225;
	add.s64 	%rd228, %rd227, %rd59;
	add.s64 	%rd229, %rd228, -7908458776815382629;
	and.b64  	%rd230, %rd217, %rd193;
	or.b64  	%rd231, %rd217, %rd193;
	and.b64  	%rd232, %rd231, %rd169;
	or.b64  	%rd233, %rd232, %rd230;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r210,%dummy}, %rd217;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r211}, %rd217;
	}
	shf.r.wrap.b32 	%r212, %r211, %r210, 28;
	shf.r.wrap.b32 	%r213, %r210, %r211, 28;
	mov.b64 	%rd234, {%r213, %r212};
	shf.l.wrap.b32 	%r214, %r210, %r211, 30;
	shf.l.wrap.b32 	%r215, %r211, %r210, 30;
	mov.b64 	%rd235, {%r215, %r214};
	xor.b64  	%rd236, %rd234, %rd235;
	shf.l.wrap.b32 	%r216, %r210, %r211, 25;
	shf.l.wrap.b32 	%r217, %r211, %r210, 25;
	mov.b64 	%rd237, {%r217, %r216};
	xor.b64  	%rd238, %rd236, %rd237;
	add.s64 	%rd239, %rd233, %rd238;
	add.s64 	%rd240, %rd229, %rd146;
	add.s64 	%rd241, %rd239, %rd229;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r218,%dummy}, %rd240;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r219}, %rd240;
	}
	shf.r.wrap.b32 	%r220, %r219, %r218, 14;
	shf.r.wrap.b32 	%r221, %r218, %r219, 14;
	mov.b64 	%rd242, {%r221, %r220};
	shf.r.wrap.b32 	%r222, %r219, %r218, 18;
	shf.r.wrap.b32 	%r223, %r218, %r219, 18;
	mov.b64 	%rd243, {%r223, %r222};
	xor.b64  	%rd244, %rd242, %rd243;
	shf.l.wrap.b32 	%r224, %r218, %r219, 23;
	shf.l.wrap.b32 	%r225, %r219, %r218, 23;
	mov.b64 	%rd245, {%r225, %r224};
	xor.b64  	%rd246, %rd244, %rd245;
	xor.b64  	%rd247, %rd192, %rd216;
	and.b64  	%rd248, %rd247, %rd240;
	xor.b64  	%rd249, %rd248, %rd192;
	add.s64 	%rd250, %rd246, %rd168;
	add.s64 	%rd251, %rd250, %rd249;
	add.s64 	%rd252, %rd251, %rd61;
	add.s64 	%rd253, %rd252, -6116909921290321640;
	and.b64  	%rd254, %rd241, %rd217;
	or.b64  	%rd255, %rd241, %rd217;
	and.b64  	%rd256, %rd255, %rd193;
	or.b64  	%rd257, %rd256, %rd254;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r226,%dummy}, %rd241;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r227}, %rd241;
	}
	shf.r.wrap.b32 	%r228, %r227, %r226, 28;
	shf.r.wrap.b32 	%r229, %r226, %r227, 28;
	mov.b64 	%rd258, {%r229, %r228};
	shf.l.wrap.b32 	%r230, %r226, %r227, 30;
	shf.l.wrap.b32 	%r231, %r227, %r226, 30;
	mov.b64 	%rd259, {%r231, %r230};
	xor.b64  	%rd260, %rd258, %rd259;
	shf.l.wrap.b32 	%r232, %r226, %r227, 25;
	shf.l.wrap.b32 	%r233, %r227, %r226, 25;
	mov.b64 	%rd261, {%r233, %r232};
	xor.b64  	%rd262, %rd260, %rd261;
	add.s64 	%rd263, %rd257, %rd262;
	add.s64 	%rd264, %rd253, %rd169;
	add.s64 	%rd265, %rd263, %rd253;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r234,%dummy}, %rd264;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r235}, %rd264;
	}
	shf.r.wrap.b32 	%r236, %r235, %r234, 14;
	shf.r.wrap.b32 	%r237, %r234, %r235, 14;
	mov.b64 	%rd266, {%r237, %r236};
	shf.r.wrap.b32 	%r238, %r235, %r234, 18;
	shf.r.wrap.b32 	%r239, %r234, %r235, 18;
	mov.b64 	%rd267, {%r239, %r238};
	xor.b64  	%rd268, %rd266, %rd267;
	shf.l.wrap.b32 	%r240, %r234, %r235, 23;
	shf.l.wrap.b32 	%r241, %r235, %r234, 23;
	mov.b64 	%rd269, {%r241, %r240};
	xor.b64  	%rd270, %rd268, %rd269;
	xor.b64  	%rd271, %rd216, %rd240;
	and.b64  	%rd272, %rd271, %rd264;
	xor.b64  	%rd273, %rd272, %rd216;
	add.s64 	%rd274, %rd270, %rd192;
	add.s64 	%rd275, %rd274, %rd273;
	add.s64 	%rd276, %rd275, %rd63;
	add.s64 	%rd277, %rd276, -2880145864133508542;
	and.b64  	%rd278, %rd265, %rd241;
	or.b64  	%rd279, %rd265, %rd241;
	and.b64  	%rd280, %rd279, %rd217;
	or.b64  	%rd281, %rd280, %rd278;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r242,%dummy}, %rd265;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r243}, %rd265;
	}
	shf.r.wrap.b32 	%r244, %r243, %r242, 28;
	shf.r.wrap.b32 	%r245, %r242, %r243, 28;
	mov.b64 	%rd282, {%r245, %r244};
	shf.l.wrap.b32 	%r246, %r242, %r243, 30;
	shf.l.wrap.b32 	%r247, %r243, %r242, 30;
	mov.b64 	%rd283, {%r247, %r246};
	xor.b64  	%rd284, %rd282, %rd283;
	shf.l.wrap.b32 	%r248, %r242, %r243, 25;
	shf.l.wrap.b32 	%r249, %r243, %r242, 25;
	mov.b64 	%rd285, {%r249, %r248};
	xor.b64  	%rd286, %rd284, %rd285;
	add.s64 	%rd287, %rd281, %rd286;
	add.s64 	%rd288, %rd277, %rd193;
	add.s64 	%rd289, %rd287, %rd277;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r250,%dummy}, %rd288;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r251}, %rd288;
	}
	shf.r.wrap.b32 	%r252, %r251, %r250, 14;
	shf.r.wrap.b32 	%r253, %r250, %r251, 14;
	mov.b64 	%rd290, {%r253, %r252};
	shf.r.wrap.b32 	%r254, %r251, %r250, 18;
	shf.r.wrap.b32 	%r255, %r250, %r251, 18;
	mov.b64 	%rd291, {%r255, %r254};
	xor.b64  	%rd292, %rd290, %rd291;
	shf.l.wrap.b32 	%r256, %r250, %r251, 23;
	shf.l.wrap.b32 	%r257, %r251, %r250, 23;
	mov.b64 	%rd293, {%r257, %r256};
	xor.b64  	%rd294, %rd292, %rd293;
	xor.b64  	%rd295, %rd240, %rd264;
	and.b64  	%rd296, %rd295, %rd288;
	xor.b64  	%rd297, %rd296, %rd240;
	add.s64 	%rd298, %rd294, %rd216;
	add.s64 	%rd299, %rd298, %rd297;
	add.s64 	%rd300, %rd299, %rd65;
	add.s64 	%rd301, %rd300, 1334009975649890238;
	and.b64  	%rd302, %rd289, %rd265;
	or.b64  	%rd303, %rd289, %rd265;
	and.b64  	%rd304, %rd303, %rd241;
	or.b64  	%rd305, %rd304, %rd302;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r258,%dummy}, %rd289;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r259}, %rd289;
	}
	shf.r.wrap.b32 	%r260, %r259, %r258, 28;
	shf.r.wrap.b32 	%r261, %r258, %r259, 28;
	mov.b64 	%rd306, {%r261, %r260};
	shf.l.wrap.b32 	%r262, %r258, %r259, 30;
	shf.l.wrap.b32 	%r263, %r259, %r258, 30;
	mov.b64 	%rd307, {%r263, %r262};
	xor.b64  	%rd308, %rd306, %rd307;
	shf.l.wrap.b32 	%r264, %r258, %r259, 25;
	shf.l.wrap.b32 	%r265, %r259, %r258, 25;
	mov.b64 	%rd309, {%r265, %r264};
	xor.b64  	%rd310, %rd308, %rd309;
	add.s64 	%rd311, %rd305, %rd310;
	add.s64 	%rd312, %rd301, %rd217;
	add.s64 	%rd313, %rd311, %rd301;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r266,%dummy}, %rd312;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r267}, %rd312;
	}
	shf.r.wrap.b32 	%r268, %r267, %r266, 14;
	shf.r.wrap.b32 	%r269, %r266, %r267, 14;
	mov.b64 	%rd314, {%r269, %r268};
	shf.r.wrap.b32 	%r270, %r267, %r266, 18;
	shf.r.wrap.b32 	%r271, %r266, %r267, 18;
	mov.b64 	%rd315, {%r271, %r270};
	xor.b64  	%rd316, %rd314, %rd315;
	shf.l.wrap.b32 	%r272, %r266, %r267, 23;
	shf.l.wrap.b32 	%r273, %r267, %r266, 23;
	mov.b64 	%rd317, {%r273, %r272};
	xor.b64  	%rd318, %rd316, %rd317;
	xor.b64  	%rd319, %rd264, %rd288;
	and.b64  	%rd320, %rd319, %rd312;
	xor.b64  	%rd321, %rd320, %rd264;
	add.s64 	%rd322, %rd318, %rd240;
	add.s64 	%rd323, %rd322, %rd321;
	add.s64 	%rd324, %rd323, %rd67;
	add.s64 	%rd325, %rd324, 2608012711638119052;
	and.b64  	%rd326, %rd313, %rd289;
	or.b64  	%rd327, %rd313, %rd289;
	and.b64  	%rd328, %rd327, %rd265;
	or.b64  	%rd329, %rd328, %rd326;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r274,%dummy}, %rd313;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r275}, %rd313;
	}
	shf.r.wrap.b32 	%r276, %r275, %r274, 28;
	shf.r.wrap.b32 	%r277, %r274, %r275, 28;
	mov.b64 	%rd330, {%r277, %r276};
	shf.l.wrap.b32 	%r278, %r274, %r275, 30;
	shf.l.wrap.b32 	%r279, %r275, %r274, 30;
	mov.b64 	%rd331, {%r279, %r278};
	xor.b64  	%rd332, %rd330, %rd331;
	shf.l.wrap.b32 	%r280, %r274, %r275, 25;
	shf.l.wrap.b32 	%r281, %r275, %r274, 25;
	mov.b64 	%rd333, {%r281, %r280};
	xor.b64  	%rd334, %rd332, %rd333;
	add.s64 	%rd335, %rd329, %rd334;
	add.s64 	%rd336, %rd325, %rd241;
	add.s64 	%rd337, %rd335, %rd325;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r282,%dummy}, %rd336;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r283}, %rd336;
	}
	shf.r.wrap.b32 	%r284, %r283, %r282, 14;
	shf.r.wrap.b32 	%r285, %r282, %r283, 14;
	mov.b64 	%rd338, {%r285, %r284};
	shf.r.wrap.b32 	%r286, %r283, %r282, 18;
	shf.r.wrap.b32 	%r287, %r282, %r283, 18;
	mov.b64 	%rd339, {%r287, %r286};
	xor.b64  	%rd340, %rd338, %rd339;
	shf.l.wrap.b32 	%r288, %r282, %r283, 23;
	shf.l.wrap.b32 	%r289, %r283, %r282, 23;
	mov.b64 	%rd341, {%r289, %r288};
	xor.b64  	%rd342, %rd340, %rd341;
	xor.b64  	%rd343, %rd288, %rd312;
	and.b64  	%rd344, %rd343, %rd336;
	xor.b64  	%rd345, %rd344, %rd288;
	add.s64 	%rd346, %rd342, %rd264;
	add.s64 	%rd347, %rd346, %rd345;
	add.s64 	%rd348, %rd347, %rd69;
	add.s64 	%rd349, %rd348, 6128411473006802146;
	and.b64  	%rd350, %rd337, %rd313;
	or.b64  	%rd351, %rd337, %rd313;
	and.b64  	%rd352, %rd351, %rd289;
	or.b64  	%rd353, %rd352, %rd350;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r290,%dummy}, %rd337;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r291}, %rd337;
	}
	shf.r.wrap.b32 	%r292, %r291, %r290, 28;
	shf.r.wrap.b32 	%r293, %r290, %r291, 28;
	mov.b64 	%rd354, {%r293, %r292};
	shf.l.wrap.b32 	%r294, %r290, %r291, 30;
	shf.l.wrap.b32 	%r295, %r291, %r290, 30;
	mov.b64 	%rd355, {%r295, %r294};
	xor.b64  	%rd356, %rd354, %rd355;
	shf.l.wrap.b32 	%r296, %r290, %r291, 25;
	shf.l.wrap.b32 	%r297, %r291, %r290, 25;
	mov.b64 	%rd357, {%r297, %r296};
	xor.b64  	%rd358, %rd356, %rd357;
	add.s64 	%rd359, %rd353, %rd358;
	add.s64 	%rd360, %rd349, %rd265;
	add.s64 	%rd361, %rd359, %rd349;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r298,%dummy}, %rd360;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r299}, %rd360;
	}
	shf.r.wrap.b32 	%r300, %r299, %r298, 14;
	shf.r.wrap.b32 	%r301, %r298, %r299, 14;
	mov.b64 	%rd362, {%r301, %r300};
	shf.r.wrap.b32 	%r302, %r299, %r298, 18;
	shf.r.wrap.b32 	%r303, %r298, %r299, 18;
	mov.b64 	%rd363, {%r303, %r302};
	xor.b64  	%rd364, %rd362, %rd363;
	shf.l.wrap.b32 	%r304, %r298, %r299, 23;
	shf.l.wrap.b32 	%r305, %r299, %r298, 23;
	mov.b64 	%rd365, {%r305, %r304};
	xor.b64  	%rd366, %rd364, %rd365;
	xor.b64  	%rd367, %rd312, %rd336;
	and.b64  	%rd368, %rd367, %rd360;
	xor.b64  	%rd369, %rd368, %rd312;
	add.s64 	%rd370, %rd366, %rd288;
	add.s64 	%rd371, %rd370, %rd369;
	add.s64 	%rd372, %rd371, %rd71;
	add.s64 	%rd373, %rd372, 8268148722764581231;
	and.b64  	%rd374, %rd361, %rd337;
	or.b64  	%rd375, %rd361, %rd337;
	and.b64  	%rd376, %rd375, %rd313;
	or.b64  	%rd377, %rd376, %rd374;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r306,%dummy}, %rd361;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r307}, %rd361;
	}
	shf.r.wrap.b32 	%r308, %r307, %r306, 28;
	shf.r.wrap.b32 	%r309, %r306, %r307, 28;
	mov.b64 	%rd378, {%r309, %r308};
	shf.l.wrap.b32 	%r310, %r306, %r307, 30;
	shf.l.wrap.b32 	%r311, %r307, %r306, 30;
	mov.b64 	%rd379, {%r311, %r310};
	xor.b64  	%rd380, %rd378, %rd379;
	shf.l.wrap.b32 	%r312, %r306, %r307, 25;
	shf.l.wrap.b32 	%r313, %r307, %r306, 25;
	mov.b64 	%rd381, {%r313, %r312};
	xor.b64  	%rd382, %rd380, %rd381;
	add.s64 	%rd383, %rd377, %rd382;
	add.s64 	%rd384, %rd373, %rd289;
	add.s64 	%rd385, %rd383, %rd373;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r314,%dummy}, %rd384;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r315}, %rd384;
	}
	shf.r.wrap.b32 	%r316, %r315, %r314, 14;
	shf.r.wrap.b32 	%r317, %r314, %r315, 14;
	mov.b64 	%rd386, {%r317, %r316};
	shf.r.wrap.b32 	%r318, %r315, %r314, 18;
	shf.r.wrap.b32 	%r319, %r314, %r315, 18;
	mov.b64 	%rd387, {%r319, %r318};
	xor.b64  	%rd388, %rd386, %rd387;
	shf.l.wrap.b32 	%r320, %r314, %r315, 23;
	shf.l.wrap.b32 	%r321, %r315, %r314, 23;
	mov.b64 	%rd389, {%r321, %r320};
	xor.b64  	%rd390, %rd388, %rd389;
	xor.b64  	%rd391, %rd336, %rd360;
	and.b64  	%rd392, %rd391, %rd384;
	xor.b64  	%rd393, %rd392, %rd336;
	add.s64 	%rd394, %rd390, %rd312;
	add.s64 	%rd395, %rd394, %rd393;
	add.s64 	%rd396, %rd395, %rd73;
	add.s64 	%rd397, %rd396, -9160688886553864527;
	and.b64  	%rd398, %rd385, %rd361;
	or.b64  	%rd399, %rd385, %rd361;
	and.b64  	%rd400, %rd399, %rd337;
	or.b64  	%rd401, %rd400, %rd398;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r322,%dummy}, %rd385;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r323}, %rd385;
	}
	shf.r.wrap.b32 	%r324, %r323, %r322, 28;
	shf.r.wrap.b32 	%r325, %r322, %r323, 28;
	mov.b64 	%rd402, {%r325, %r324};
	shf.l.wrap.b32 	%r326, %r322, %r323, 30;
	shf.l.wrap.b32 	%r327, %r323, %r322, 30;
	mov.b64 	%rd403, {%r327, %r326};
	xor.b64  	%rd404, %rd402, %rd403;
	shf.l.wrap.b32 	%r328, %r322, %r323, 25;
	shf.l.wrap.b32 	%r329, %r323, %r322, 25;
	mov.b64 	%rd405, {%r329, %r328};
	xor.b64  	%rd406, %rd404, %rd405;
	add.s64 	%rd407, %rd401, %rd406;
	add.s64 	%rd408, %rd397, %rd313;
	add.s64 	%rd409, %rd407, %rd397;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r330,%dummy}, %rd408;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r331}, %rd408;
	}
	shf.r.wrap.b32 	%r332, %r331, %r330, 14;
	shf.r.wrap.b32 	%r333, %r330, %r331, 14;
	mov.b64 	%rd410, {%r333, %r332};
	shf.r.wrap.b32 	%r334, %r331, %r330, 18;
	shf.r.wrap.b32 	%r335, %r330, %r331, 18;
	mov.b64 	%rd411, {%r335, %r334};
	xor.b64  	%rd412, %rd410, %rd411;
	shf.l.wrap.b32 	%r336, %r330, %r331, 23;
	shf.l.wrap.b32 	%r337, %r331, %r330, 23;
	mov.b64 	%rd413, {%r337, %r336};
	xor.b64  	%rd414, %rd412, %rd413;
	xor.b64  	%rd415, %rd360, %rd384;
	and.b64  	%rd416, %rd415, %rd408;
	xor.b64  	%rd417, %rd416, %rd360;
	add.s64 	%rd418, %rd414, %rd336;
	add.s64 	%rd419, %rd418, %rd417;
	add.s64 	%rd420, %rd419, %rd75;
	add.s64 	%rd421, %rd420, -7215885187991268811;
	and.b64  	%rd422, %rd409, %rd385;
	or.b64  	%rd423, %rd409, %rd385;
	and.b64  	%rd424, %rd423, %rd361;
	or.b64  	%rd425, %rd424, %rd422;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r338,%dummy}, %rd409;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r339}, %rd409;
	}
	shf.r.wrap.b32 	%r340, %r339, %r338, 28;
	shf.r.wrap.b32 	%r341, %r338, %r339, 28;
	mov.b64 	%rd426, {%r341, %r340};
	shf.l.wrap.b32 	%r342, %r338, %r339, 30;
	shf.l.wrap.b32 	%r343, %r339, %r338, 30;
	mov.b64 	%rd427, {%r343, %r342};
	xor.b64  	%rd428, %rd426, %rd427;
	shf.l.wrap.b32 	%r344, %r338, %r339, 25;
	shf.l.wrap.b32 	%r345, %r339, %r338, 25;
	mov.b64 	%rd429, {%r345, %r344};
	xor.b64  	%rd430, %rd428, %rd429;
	add.s64 	%rd431, %rd425, %rd430;
	add.s64 	%rd432, %rd421, %rd337;
	add.s64 	%rd433, %rd431, %rd421;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r346,%dummy}, %rd432;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r347}, %rd432;
	}
	shf.r.wrap.b32 	%r348, %r347, %r346, 14;
	shf.r.wrap.b32 	%r349, %r346, %r347, 14;
	mov.b64 	%rd434, {%r349, %r348};
	shf.r.wrap.b32 	%r350, %r347, %r346, 18;
	shf.r.wrap.b32 	%r351, %r346, %r347, 18;
	mov.b64 	%rd435, {%r351, %r350};
	xor.b64  	%rd436, %rd434, %rd435;
	shf.l.wrap.b32 	%r352, %r346, %r347, 23;
	shf.l.wrap.b32 	%r353, %r347, %r346, 23;
	mov.b64 	%rd437, {%r353, %r352};
	xor.b64  	%rd438, %rd436, %rd437;
	xor.b64  	%rd439, %rd384, %rd408;
	and.b64  	%rd440, %rd439, %rd432;
	xor.b64  	%rd441, %rd440, %rd384;
	add.s64 	%rd442, %rd438, %rd360;
	add.s64 	%rd443, %rd442, %rd441;
	add.s64 	%rd444, %rd443, %rd87;
	add.s64 	%rd445, %rd444, -4495734319001033068;
	and.b64  	%rd446, %rd433, %rd409;
	or.b64  	%rd447, %rd433, %rd409;
	and.b64  	%rd448, %rd447, %rd385;
	or.b64  	%rd449, %rd448, %rd446;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r354,%dummy}, %rd433;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r355}, %rd433;
	}
	shf.r.wrap.b32 	%r356, %r355, %r354, 28;
	shf.r.wrap.b32 	%r357, %r354, %r355, 28;
	mov.b64 	%rd450, {%r357, %r356};
	shf.l.wrap.b32 	%r358, %r354, %r355, 30;
	shf.l.wrap.b32 	%r359, %r355, %r354, 30;
	mov.b64 	%rd451, {%r359, %r358};
	xor.b64  	%rd452, %rd450, %rd451;
	shf.l.wrap.b32 	%r360, %r354, %r355, 25;
	shf.l.wrap.b32 	%r361, %r355, %r354, 25;
	mov.b64 	%rd453, {%r361, %r360};
	xor.b64  	%rd454, %rd452, %rd453;
	add.s64 	%rd455, %rd449, %rd454;
	add.s64 	%rd456, %rd445, %rd361;
	add.s64 	%rd457, %rd455, %rd445;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r362,%dummy}, %rd75;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r363}, %rd75;
	}
	shf.r.wrap.b32 	%r364, %r363, %r362, 19;
	shf.r.wrap.b32 	%r365, %r362, %r363, 19;
	mov.b64 	%rd458, {%r365, %r364};
	shf.l.wrap.b32 	%r366, %r362, %r363, 3;
	shf.l.wrap.b32 	%r367, %r363, %r362, 3;
	mov.b64 	%rd459, {%r367, %r366};
	xor.b64  	%rd460, %rd458, %rd459;
	shr.u64 	%rd461, %rd75, 6;
	xor.b64  	%rd462, %rd460, %rd461;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r368,%dummy}, %rd49;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r369}, %rd49;
	}
	shf.r.wrap.b32 	%r370, %r369, %r368, 1;
	shf.r.wrap.b32 	%r371, %r368, %r369, 1;
	mov.b64 	%rd463, {%r371, %r370};
	shf.r.wrap.b32 	%r372, %r369, %r368, 8;
	shf.r.wrap.b32 	%r373, %r368, %r369, 8;
	mov.b64 	%rd464, {%r373, %r372};
	xor.b64  	%rd465, %rd463, %rd464;
	shr.u64 	%rd466, %rd49, 7;
	xor.b64  	%rd467, %rd465, %rd466;
	add.s64 	%rd468, %rd462, %rd47;
	add.s64 	%rd469, %rd468, %rd65;
	add.s64 	%rd470, %rd469, %rd467;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r374,%dummy}, %rd456;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r375}, %rd456;
	}
	shf.r.wrap.b32 	%r376, %r375, %r374, 14;
	shf.r.wrap.b32 	%r377, %r374, %r375, 14;
	mov.b64 	%rd471, {%r377, %r376};
	shf.r.wrap.b32 	%r378, %r375, %r374, 18;
	shf.r.wrap.b32 	%r379, %r374, %r375, 18;
	mov.b64 	%rd472, {%r379, %r378};
	xor.b64  	%rd473, %rd471, %rd472;
	shf.l.wrap.b32 	%r380, %r374, %r375, 23;
	shf.l.wrap.b32 	%r381, %r375, %r374, 23;
	mov.b64 	%rd474, {%r381, %r380};
	xor.b64  	%rd475, %rd473, %rd474;
	xor.b64  	%rd476, %rd408, %rd432;
	and.b64  	%rd477, %rd476, %rd456;
	xor.b64  	%rd478, %rd477, %rd408;
	add.s64 	%rd479, %rd475, %rd384;
	add.s64 	%rd480, %rd479, %rd478;
	add.s64 	%rd481, %rd480, %rd470;
	add.s64 	%rd482, %rd481, -1973867731355612462;
	and.b64  	%rd483, %rd457, %rd433;
	or.b64  	%rd484, %rd457, %rd433;
	and.b64  	%rd485, %rd484, %rd409;
	or.b64  	%rd486, %rd485, %rd483;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r382,%dummy}, %rd457;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r383}, %rd457;
	}
	shf.r.wrap.b32 	%r384, %r383, %r382, 28;
	shf.r.wrap.b32 	%r385, %r382, %r383, 28;
	mov.b64 	%rd487, {%r385, %r384};
	shf.l.wrap.b32 	%r386, %r382, %r383, 30;
	shf.l.wrap.b32 	%r387, %r383, %r382, 30;
	mov.b64 	%rd488, {%r387, %r386};
	xor.b64  	%rd489, %rd487, %rd488;
	shf.l.wrap.b32 	%r388, %r382, %r383, 25;
	shf.l.wrap.b32 	%r389, %r383, %r382, 25;
	mov.b64 	%rd490, {%r389, %r388};
	xor.b64  	%rd491, %rd489, %rd490;
	add.s64 	%rd492, %rd486, %rd491;
	add.s64 	%rd493, %rd482, %rd385;
	add.s64 	%rd494, %rd492, %rd482;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r390,%dummy}, %rd87;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r391}, %rd87;
	}
	shf.r.wrap.b32 	%r392, %r391, %r390, 19;
	shf.r.wrap.b32 	%r393, %r390, %r391, 19;
	mov.b64 	%rd495, {%r393, %r392};
	shf.l.wrap.b32 	%r394, %r390, %r391, 3;
	shf.l.wrap.b32 	%r395, %r391, %r390, 3;
	mov.b64 	%rd496, {%r395, %r394};
	xor.b64  	%rd497, %rd495, %rd496;
	shr.u64 	%rd498, %rd87, 6;
	xor.b64  	%rd499, %rd497, %rd498;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r396,%dummy}, %rd51;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r397}, %rd51;
	}
	shf.r.wrap.b32 	%r398, %r397, %r396, 1;
	shf.r.wrap.b32 	%r399, %r396, %r397, 1;
	mov.b64 	%rd500, {%r399, %r398};
	shf.r.wrap.b32 	%r400, %r397, %r396, 8;
	shf.r.wrap.b32 	%r401, %r396, %r397, 8;
	mov.b64 	%rd501, {%r401, %r400};
	xor.b64  	%rd502, %rd500, %rd501;
	shr.u64 	%rd503, %rd51, 7;
	xor.b64  	%rd504, %rd502, %rd503;
	add.s64 	%rd505, %rd499, %rd49;
	add.s64 	%rd506, %rd505, %rd67;
	add.s64 	%rd507, %rd506, %rd504;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r402,%dummy}, %rd493;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r403}, %rd493;
	}
	shf.r.wrap.b32 	%r404, %r403, %r402, 14;
	shf.r.wrap.b32 	%r405, %r402, %r403, 14;
	mov.b64 	%rd508, {%r405, %r404};
	shf.r.wrap.b32 	%r406, %r403, %r402, 18;
	shf.r.wrap.b32 	%r407, %r402, %r403, 18;
	mov.b64 	%rd509, {%r407, %r406};
	xor.b64  	%rd510, %rd508, %rd509;
	shf.l.wrap.b32 	%r408, %r402, %r403, 23;
	shf.l.wrap.b32 	%r409, %r403, %r402, 23;
	mov.b64 	%rd511, {%r409, %r408};
	xor.b64  	%rd512, %rd510, %rd511;
	xor.b64  	%rd513, %rd432, %rd456;
	and.b64  	%rd514, %rd513, %rd493;
	xor.b64  	%rd515, %rd514, %rd432;
	add.s64 	%rd516, %rd512, %rd408;
	add.s64 	%rd517, %rd516, %rd515;
	add.s64 	%rd518, %rd517, %rd507;
	add.s64 	%rd519, %rd518, -1171420211273849373;
	and.b64  	%rd520, %rd494, %rd457;
	or.b64  	%rd521, %rd494, %rd457;
	and.b64  	%rd522, %rd521, %rd433;
	or.b64  	%rd523, %rd522, %rd520;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r410,%dummy}, %rd494;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r411}, %rd494;
	}
	shf.r.wrap.b32 	%r412, %r411, %r410, 28;
	shf.r.wrap.b32 	%r413, %r410, %r411, 28;
	mov.b64 	%rd524, {%r413, %r412};
	shf.l.wrap.b32 	%r414, %r410, %r411, 30;
	shf.l.wrap.b32 	%r415, %r411, %r410, 30;
	mov.b64 	%rd525, {%r415, %r414};
	xor.b64  	%rd526, %rd524, %rd525;
	shf.l.wrap.b32 	%r416, %r410, %r411, 25;
	shf.l.wrap.b32 	%r417, %r411, %r410, 25;
	mov.b64 	%rd527, {%r417, %r416};
	xor.b64  	%rd528, %rd526, %rd527;
	add.s64 	%rd529, %rd523, %rd528;
	add.s64 	%rd530, %rd519, %rd409;
	add.s64 	%rd531, %rd529, %rd519;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r418,%dummy}, %rd470;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r419}, %rd470;
	}
	shf.r.wrap.b32 	%r420, %r419, %r418, 19;
	shf.r.wrap.b32 	%r421, %r418, %r419, 19;
	mov.b64 	%rd532, {%r421, %r420};
	shf.l.wrap.b32 	%r422, %r418, %r419, 3;
	shf.l.wrap.b32 	%r423, %r419, %r418, 3;
	mov.b64 	%rd533, {%r423, %r422};
	xor.b64  	%rd534, %rd532, %rd533;
	shr.u64 	%rd535, %rd470, 6;
	xor.b64  	%rd536, %rd534, %rd535;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r424,%dummy}, %rd53;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r425}, %rd53;
	}
	shf.r.wrap.b32 	%r426, %r425, %r424, 1;
	shf.r.wrap.b32 	%r427, %r424, %r425, 1;
	mov.b64 	%rd537, {%r427, %r426};
	shf.r.wrap.b32 	%r428, %r425, %r424, 8;
	shf.r.wrap.b32 	%r429, %r424, %r425, 8;
	mov.b64 	%rd538, {%r429, %r428};
	xor.b64  	%rd539, %rd537, %rd538;
	shr.u64 	%rd540, %rd53, 7;
	xor.b64  	%rd541, %rd539, %rd540;
	add.s64 	%rd542, %rd536, %rd51;
	add.s64 	%rd543, %rd542, %rd69;
	add.s64 	%rd544, %rd543, %rd541;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r430,%dummy}, %rd530;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r431}, %rd530;
	}
	shf.r.wrap.b32 	%r432, %r431, %r430, 14;
	shf.r.wrap.b32 	%r433, %r430, %r431, 14;
	mov.b64 	%rd545, {%r433, %r432};
	shf.r.wrap.b32 	%r434, %r431, %r430, 18;
	shf.r.wrap.b32 	%r435, %r430, %r431, 18;
	mov.b64 	%rd546, {%r435, %r434};
	xor.b64  	%rd547, %rd545, %rd546;
	shf.l.wrap.b32 	%r436, %r430, %r431, 23;
	shf.l.wrap.b32 	%r437, %r431, %r430, 23;
	mov.b64 	%rd548, {%r437, %r436};
	xor.b64  	%rd549, %rd547, %rd548;
	xor.b64  	%rd550, %rd456, %rd493;
	and.b64  	%rd551, %rd550, %rd530;
	xor.b64  	%rd552, %rd551, %rd456;
	add.s64 	%rd553, %rd549, %rd432;
	add.s64 	%rd554, %rd553, %rd552;
	add.s64 	%rd555, %rd554, %rd544;
	add.s64 	%rd556, %rd555, 1135362057144423861;
	and.b64  	%rd557, %rd531, %rd494;
	or.b64  	%rd558, %rd531, %rd494;
	and.b64  	%rd559, %rd558, %rd457;
	or.b64  	%rd560, %rd559, %rd557;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r438,%dummy}, %rd531;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r439}, %rd531;
	}
	shf.r.wrap.b32 	%r440, %r439, %r438, 28;
	shf.r.wrap.b32 	%r441, %r438, %r439, 28;
	mov.b64 	%rd561, {%r441, %r440};
	shf.l.wrap.b32 	%r442, %r438, %r439, 30;
	shf.l.wrap.b32 	%r443, %r439, %r438, 30;
	mov.b64 	%rd562, {%r443, %r442};
	xor.b64  	%rd563, %rd561, %rd562;
	shf.l.wrap.b32 	%r444, %r438, %r439, 25;
	shf.l.wrap.b32 	%r445, %r439, %r438, 25;
	mov.b64 	%rd564, {%r445, %r444};
	xor.b64  	%rd565, %rd563, %rd564;
	add.s64 	%rd566, %rd560, %rd565;
	add.s64 	%rd567, %rd556, %rd433;
	add.s64 	%rd568, %rd566, %rd556;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r446,%dummy}, %rd507;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r447}, %rd507;
	}
	shf.r.wrap.b32 	%r448, %r447, %r446, 19;
	shf.r.wrap.b32 	%r449, %r446, %r447, 19;
	mov.b64 	%rd569, {%r449, %r448};
	shf.l.wrap.b32 	%r450, %r446, %r447, 3;
	shf.l.wrap.b32 	%r451, %r447, %r446, 3;
	mov.b64 	%rd570, {%r451, %r450};
	xor.b64  	%rd571, %rd569, %rd570;
	shr.u64 	%rd572, %rd507, 6;
	xor.b64  	%rd573, %rd571, %rd572;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r452,%dummy}, %rd55;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r453}, %rd55;
	}
	shf.r.wrap.b32 	%r454, %r453, %r452, 1;
	shf.r.wrap.b32 	%r455, %r452, %r453, 1;
	mov.b64 	%rd574, {%r455, %r454};
	shf.r.wrap.b32 	%r456, %r453, %r452, 8;
	shf.r.wrap.b32 	%r457, %r452, %r453, 8;
	mov.b64 	%rd575, {%r457, %r456};
	xor.b64  	%rd576, %rd574, %rd575;
	shr.u64 	%rd577, %rd55, 7;
	xor.b64  	%rd578, %rd576, %rd577;
	add.s64 	%rd579, %rd573, %rd53;
	add.s64 	%rd580, %rd579, %rd71;
	add.s64 	%rd581, %rd580, %rd578;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r458,%dummy}, %rd567;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r459}, %rd567;
	}
	shf.r.wrap.b32 	%r460, %r459, %r458, 14;
	shf.r.wrap.b32 	%r461, %r458, %r459, 14;
	mov.b64 	%rd582, {%r461, %r460};
	shf.r.wrap.b32 	%r462, %r459, %r458, 18;
	shf.r.wrap.b32 	%r463, %r458, %r459, 18;
	mov.b64 	%rd583, {%r463, %r462};
	xor.b64  	%rd584, %rd582, %rd583;
	shf.l.wrap.b32 	%r464, %r458, %r459, 23;
	shf.l.wrap.b32 	%r465, %r459, %r458, 23;
	mov.b64 	%rd585, {%r465, %r464};
	xor.b64  	%rd586, %rd584, %rd585;
	xor.b64  	%rd587, %rd493, %rd530;
	and.b64  	%rd588, %rd587, %rd567;
	xor.b64  	%rd589, %rd588, %rd493;
	add.s64 	%rd590, %rd586, %rd456;
	add.s64 	%rd591, %rd590, %rd589;
	add.s64 	%rd592, %rd591, %rd581;
	add.s64 	%rd593, %rd592, 2597628984639134821;
	and.b64  	%rd594, %rd568, %rd531;
	or.b64  	%rd595, %rd568, %rd531;
	and.b64  	%rd596, %rd595, %rd494;
	or.b64  	%rd597, %rd596, %rd594;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r466,%dummy}, %rd568;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r467}, %rd568;
	}
	shf.r.wrap.b32 	%r468, %r467, %r466, 28;
	shf.r.wrap.b32 	%r469, %r466, %r467, 28;
	mov.b64 	%rd598, {%r469, %r468};
	shf.l.wrap.b32 	%r470, %r466, %r467, 30;
	shf.l.wrap.b32 	%r471, %r467, %r466, 30;
	mov.b64 	%rd599, {%r471, %r470};
	xor.b64  	%rd600, %rd598, %rd599;
	shf.l.wrap.b32 	%r472, %r466, %r467, 25;
	shf.l.wrap.b32 	%r473, %r467, %r466, 25;
	mov.b64 	%rd601, {%r473, %r472};
	xor.b64  	%rd602, %rd600, %rd601;
	add.s64 	%rd603, %rd597, %rd602;
	add.s64 	%rd604, %rd593, %rd457;
	add.s64 	%rd605, %rd603, %rd593;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r474,%dummy}, %rd544;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r475}, %rd544;
	}
	shf.r.wrap.b32 	%r476, %r475, %r474, 19;
	shf.r.wrap.b32 	%r477, %r474, %r475, 19;
	mov.b64 	%rd606, {%r477, %r476};
	shf.l.wrap.b32 	%r478, %r474, %r475, 3;
	shf.l.wrap.b32 	%r479, %r475, %r474, 3;
	mov.b64 	%rd607, {%r479, %r478};
	xor.b64  	%rd608, %rd606, %rd607;
	shr.u64 	%rd609, %rd544, 6;
	xor.b64  	%rd610, %rd608, %rd609;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r480,%dummy}, %rd57;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r481}, %rd57;
	}
	shf.r.wrap.b32 	%r482, %r481, %r480, 1;
	shf.r.wrap.b32 	%r483, %r480, %r481, 1;
	mov.b64 	%rd611, {%r483, %r482};
	shf.r.wrap.b32 	%r484, %r481, %r480, 8;
	shf.r.wrap.b32 	%r485, %r480, %r481, 8;
	mov.b64 	%rd612, {%r485, %r484};
	xor.b64  	%rd613, %rd611, %rd612;
	shr.u64 	%rd614, %rd57, 7;
	xor.b64  	%rd615, %rd613, %rd614;
	add.s64 	%rd616, %rd610, %rd55;
	add.s64 	%rd617, %rd616, %rd73;
	add.s64 	%rd618, %rd617, %rd615;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r486,%dummy}, %rd604;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r487}, %rd604;
	}
	shf.r.wrap.b32 	%r488, %r487, %r486, 14;
	shf.r.wrap.b32 	%r489, %r486, %r487, 14;
	mov.b64 	%rd619, {%r489, %r488};
	shf.r.wrap.b32 	%r490, %r487, %r486, 18;
	shf.r.wrap.b32 	%r491, %r486, %r487, 18;
	mov.b64 	%rd620, {%r491, %r490};
	xor.b64  	%rd621, %rd619, %rd620;
	shf.l.wrap.b32 	%r492, %r486, %r487, 23;
	shf.l.wrap.b32 	%r493, %r487, %r486, 23;
	mov.b64 	%rd622, {%r493, %r492};
	xor.b64  	%rd623, %rd621, %rd622;
	xor.b64  	%rd624, %rd530, %rd567;
	and.b64  	%rd625, %rd624, %rd604;
	xor.b64  	%rd626, %rd625, %rd530;
	add.s64 	%rd627, %rd623, %rd493;
	add.s64 	%rd628, %rd627, %rd626;
	add.s64 	%rd629, %rd628, %rd618;
	add.s64 	%rd630, %rd629, 3308224258029322869;
	and.b64  	%rd631, %rd605, %rd568;
	or.b64  	%rd632, %rd605, %rd568;
	and.b64  	%rd633, %rd632, %rd531;
	or.b64  	%rd634, %rd633, %rd631;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r494,%dummy}, %rd605;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r495}, %rd605;
	}
	shf.r.wrap.b32 	%r496, %r495, %r494, 28;
	shf.r.wrap.b32 	%r497, %r494, %r495, 28;
	mov.b64 	%rd635, {%r497, %r496};
	shf.l.wrap.b32 	%r498, %r494, %r495, 30;
	shf.l.wrap.b32 	%r499, %r495, %r494, 30;
	mov.b64 	%rd636, {%r499, %r498};
	xor.b64  	%rd637, %rd635, %rd636;
	shf.l.wrap.b32 	%r500, %r494, %r495, 25;
	shf.l.wrap.b32 	%r501, %r495, %r494, 25;
	mov.b64 	%rd638, {%r501, %r500};
	xor.b64  	%rd639, %rd637, %rd638;
	add.s64 	%rd640, %rd634, %rd639;
	add.s64 	%rd641, %rd630, %rd494;
	add.s64 	%rd642, %rd640, %rd630;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r502,%dummy}, %rd581;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r503}, %rd581;
	}
	shf.r.wrap.b32 	%r504, %r503, %r502, 19;
	shf.r.wrap.b32 	%r505, %r502, %r503, 19;
	mov.b64 	%rd643, {%r505, %r504};
	shf.l.wrap.b32 	%r506, %r502, %r503, 3;
	shf.l.wrap.b32 	%r507, %r503, %r502, 3;
	mov.b64 	%rd644, {%r507, %r506};
	xor.b64  	%rd645, %rd643, %rd644;
	shr.u64 	%rd646, %rd581, 6;
	xor.b64  	%rd647, %rd645, %rd646;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r508,%dummy}, %rd59;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r509}, %rd59;
	}
	shf.r.wrap.b32 	%r510, %r509, %r508, 1;
	shf.r.wrap.b32 	%r511, %r508, %r509, 1;
	mov.b64 	%rd648, {%r511, %r510};
	shf.r.wrap.b32 	%r512, %r509, %r508, 8;
	shf.r.wrap.b32 	%r513, %r508, %r509, 8;
	mov.b64 	%rd649, {%r513, %r512};
	xor.b64  	%rd650, %rd648, %rd649;
	shr.u64 	%rd651, %rd59, 7;
	xor.b64  	%rd652, %rd650, %rd651;
	add.s64 	%rd653, %rd647, %rd57;
	add.s64 	%rd654, %rd653, %rd75;
	add.s64 	%rd655, %rd654, %rd652;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r514,%dummy}, %rd641;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r515}, %rd641;
	}
	shf.r.wrap.b32 	%r516, %r515, %r514, 14;
	shf.r.wrap.b32 	%r517, %r514, %r515, 14;
	mov.b64 	%rd656, {%r517, %r516};
	shf.r.wrap.b32 	%r518, %r515, %r514, 18;
	shf.r.wrap.b32 	%r519, %r514, %r515, 18;
	mov.b64 	%rd657, {%r519, %r518};
	xor.b64  	%rd658, %rd656, %rd657;
	shf.l.wrap.b32 	%r520, %r514, %r515, 23;
	shf.l.wrap.b32 	%r521, %r515, %r514, 23;
	mov.b64 	%rd659, {%r521, %r520};
	xor.b64  	%rd660, %rd658, %rd659;
	xor.b64  	%rd661, %rd567, %rd604;
	and.b64  	%rd662, %rd661, %rd641;
	xor.b64  	%rd663, %rd662, %rd567;
	add.s64 	%rd664, %rd660, %rd530;
	add.s64 	%rd665, %rd664, %rd663;
	add.s64 	%rd666, %rd665, %rd655;
	add.s64 	%rd667, %rd666, 5365058923640841347;
	and.b64  	%rd668, %rd642, %rd605;
	or.b64  	%rd669, %rd642, %rd605;
	and.b64  	%rd670, %rd669, %rd568;
	or.b64  	%rd671, %rd670, %rd668;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r522,%dummy}, %rd642;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r523}, %rd642;
	}
	shf.r.wrap.b32 	%r524, %r523, %r522, 28;
	shf.r.wrap.b32 	%r525, %r522, %r523, 28;
	mov.b64 	%rd672, {%r525, %r524};
	shf.l.wrap.b32 	%r526, %r522, %r523, 30;
	shf.l.wrap.b32 	%r527, %r523, %r522, 30;
	mov.b64 	%rd673, {%r527, %r526};
	xor.b64  	%rd674, %rd672, %rd673;
	shf.l.wrap.b32 	%r528, %r522, %r523, 25;
	shf.l.wrap.b32 	%r529, %r523, %r522, 25;
	mov.b64 	%rd675, {%r529, %r528};
	xor.b64  	%rd676, %rd674, %rd675;
	add.s64 	%rd677, %rd671, %rd676;
	add.s64 	%rd678, %rd667, %rd531;
	add.s64 	%rd679, %rd677, %rd667;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r530,%dummy}, %rd618;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r531}, %rd618;
	}
	shf.r.wrap.b32 	%r532, %r531, %r530, 19;
	shf.r.wrap.b32 	%r533, %r530, %r531, 19;
	mov.b64 	%rd680, {%r533, %r532};
	shf.l.wrap.b32 	%r534, %r530, %r531, 3;
	shf.l.wrap.b32 	%r535, %r531, %r530, 3;
	mov.b64 	%rd681, {%r535, %r534};
	xor.b64  	%rd682, %rd680, %rd681;
	shr.u64 	%rd683, %rd618, 6;
	xor.b64  	%rd684, %rd682, %rd683;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r536,%dummy}, %rd61;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r537}, %rd61;
	}
	shf.r.wrap.b32 	%r538, %r537, %r536, 1;
	shf.r.wrap.b32 	%r539, %r536, %r537, 1;
	mov.b64 	%rd685, {%r539, %r538};
	shf.r.wrap.b32 	%r540, %r537, %r536, 8;
	shf.r.wrap.b32 	%r541, %r536, %r537, 8;
	mov.b64 	%rd686, {%r541, %r540};
	xor.b64  	%rd687, %rd685, %rd686;
	shr.u64 	%rd688, %rd61, 7;
	xor.b64  	%rd689, %rd687, %rd688;
	add.s64 	%rd690, %rd684, %rd59;
	add.s64 	%rd691, %rd690, %rd87;
	add.s64 	%rd692, %rd691, %rd689;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r542,%dummy}, %rd678;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r543}, %rd678;
	}
	shf.r.wrap.b32 	%r544, %r543, %r542, 14;
	shf.r.wrap.b32 	%r545, %r542, %r543, 14;
	mov.b64 	%rd693, {%r545, %r544};
	shf.r.wrap.b32 	%r546, %r543, %r542, 18;
	shf.r.wrap.b32 	%r547, %r542, %r543, 18;
	mov.b64 	%rd694, {%r547, %r546};
	xor.b64  	%rd695, %rd693, %rd694;
	shf.l.wrap.b32 	%r548, %r542, %r543, 23;
	shf.l.wrap.b32 	%r549, %r543, %r542, 23;
	mov.b64 	%rd696, {%r549, %r548};
	xor.b64  	%rd697, %rd695, %rd696;
	xor.b64  	%rd698, %rd604, %rd641;
	and.b64  	%rd699, %rd698, %rd678;
	xor.b64  	%rd700, %rd699, %rd604;
	add.s64 	%rd701, %rd697, %rd567;
	add.s64 	%rd702, %rd701, %rd700;
	add.s64 	%rd703, %rd702, %rd692;
	add.s64 	%rd704, %rd703, 6679025012923562964;
	and.b64  	%rd705, %rd679, %rd642;
	or.b64  	%rd706, %rd679, %rd642;
	and.b64  	%rd707, %rd706, %rd605;
	or.b64  	%rd708, %rd707, %rd705;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r550,%dummy}, %rd679;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r551}, %rd679;
	}
	shf.r.wrap.b32 	%r552, %r551, %r550, 28;
	shf.r.wrap.b32 	%r553, %r550, %r551, 28;
	mov.b64 	%rd709, {%r553, %r552};
	shf.l.wrap.b32 	%r554, %r550, %r551, 30;
	shf.l.wrap.b32 	%r555, %r551, %r550, 30;
	mov.b64 	%rd710, {%r555, %r554};
	xor.b64  	%rd711, %rd709, %rd710;
	shf.l.wrap.b32 	%r556, %r550, %r551, 25;
	shf.l.wrap.b32 	%r557, %r551, %r550, 25;
	mov.b64 	%rd712, {%r557, %r556};
	xor.b64  	%rd713, %rd711, %rd712;
	add.s64 	%rd714, %rd708, %rd713;
	add.s64 	%rd715, %rd704, %rd568;
	add.s64 	%rd716, %rd714, %rd704;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r558,%dummy}, %rd655;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r559}, %rd655;
	}
	shf.r.wrap.b32 	%r560, %r559, %r558, 19;
	shf.r.wrap.b32 	%r561, %r558, %r559, 19;
	mov.b64 	%rd717, {%r561, %r560};
	shf.l.wrap.b32 	%r562, %r558, %r559, 3;
	shf.l.wrap.b32 	%r563, %r559, %r558, 3;
	mov.b64 	%rd718, {%r563, %r562};
	xor.b64  	%rd719, %rd717, %rd718;
	shr.u64 	%rd720, %rd655, 6;
	xor.b64  	%rd721, %rd719, %rd720;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r564,%dummy}, %rd63;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r565}, %rd63;
	}
	shf.r.wrap.b32 	%r566, %r565, %r564, 1;
	shf.r.wrap.b32 	%r567, %r564, %r565, 1;
	mov.b64 	%rd722, {%r567, %r566};
	shf.r.wrap.b32 	%r568, %r565, %r564, 8;
	shf.r.wrap.b32 	%r569, %r564, %r565, 8;
	mov.b64 	%rd723, {%r569, %r568};
	xor.b64  	%rd724, %rd722, %rd723;
	shr.u64 	%rd725, %rd63, 7;
	xor.b64  	%rd726, %rd724, %rd725;
	add.s64 	%rd727, %rd721, %rd61;
	add.s64 	%rd728, %rd727, %rd470;
	add.s64 	%rd729, %rd728, %rd726;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r570,%dummy}, %rd715;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r571}, %rd715;
	}
	shf.r.wrap.b32 	%r572, %r571, %r570, 14;
	shf.r.wrap.b32 	%r573, %r570, %r571, 14;
	mov.b64 	%rd730, {%r573, %r572};
	shf.r.wrap.b32 	%r574, %r571, %r570, 18;
	shf.r.wrap.b32 	%r575, %r570, %r571, 18;
	mov.b64 	%rd731, {%r575, %r574};
	xor.b64  	%rd732, %rd730, %rd731;
	shf.l.wrap.b32 	%r576, %r570, %r571, 23;
	shf.l.wrap.b32 	%r577, %r571, %r570, 23;
	mov.b64 	%rd733, {%r577, %r576};
	xor.b64  	%rd734, %rd732, %rd733;
	xor.b64  	%rd735, %rd641, %rd678;
	and.b64  	%rd736, %rd735, %rd715;
	xor.b64  	%rd737, %rd736, %rd641;
	add.s64 	%rd738, %rd734, %rd604;
	add.s64 	%rd739, %rd738, %rd737;
	add.s64 	%rd740, %rd739, %rd729;
	add.s64 	%rd741, %rd740, 8573033837759648693;
	and.b64  	%rd742, %rd716, %rd679;
	or.b64  	%rd743, %rd716, %rd679;
	and.b64  	%rd744, %rd743, %rd642;
	or.b64  	%rd745, %rd744, %rd742;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r578,%dummy}, %rd716;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r579}, %rd716;
	}
	shf.r.wrap.b32 	%r580, %r579, %r578, 28;
	shf.r.wrap.b32 	%r581, %r578, %r579, 28;
	mov.b64 	%rd746, {%r581, %r580};
	shf.l.wrap.b32 	%r582, %r578, %r579, 30;
	shf.l.wrap.b32 	%r583, %r579, %r578, 30;
	mov.b64 	%rd747, {%r583, %r582};
	xor.b64  	%rd748, %rd746, %rd747;
	shf.l.wrap.b32 	%r584, %r578, %r579, 25;
	shf.l.wrap.b32 	%r585, %r579, %r578, 25;
	mov.b64 	%rd749, {%r585, %r584};
	xor.b64  	%rd750, %rd748, %rd749;
	add.s64 	%rd751, %rd745, %rd750;
	add.s64 	%rd752, %rd741, %rd605;
	add.s64 	%rd753, %rd751, %rd741;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r586,%dummy}, %rd692;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r587}, %rd692;
	}
	shf.r.wrap.b32 	%r588, %r587, %r586, 19;
	shf.r.wrap.b32 	%r589, %r586, %r587, 19;
	mov.b64 	%rd754, {%r589, %r588};
	shf.l.wrap.b32 	%r590, %r586, %r587, 3;
	shf.l.wrap.b32 	%r591, %r587, %r586, 3;
	mov.b64 	%rd755, {%r591, %r590};
	xor.b64  	%rd756, %rd754, %rd755;
	shr.u64 	%rd757, %rd692, 6;
	xor.b64  	%rd758, %rd756, %rd757;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r592,%dummy}, %rd65;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r593}, %rd65;
	}
	shf.r.wrap.b32 	%r594, %r593, %r592, 1;
	shf.r.wrap.b32 	%r595, %r592, %r593, 1;
	mov.b64 	%rd759, {%r595, %r594};
	shf.r.wrap.b32 	%r596, %r593, %r592, 8;
	shf.r.wrap.b32 	%r597, %r592, %r593, 8;
	mov.b64 	%rd760, {%r597, %r596};
	xor.b64  	%rd761, %rd759, %rd760;
	shr.u64 	%rd762, %rd65, 7;
	xor.b64  	%rd763, %rd761, %rd762;
	add.s64 	%rd764, %rd758, %rd63;
	add.s64 	%rd765, %rd764, %rd507;
	add.s64 	%rd766, %rd765, %rd763;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r598,%dummy}, %rd752;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r599}, %rd752;
	}
	shf.r.wrap.b32 	%r600, %r599, %r598, 14;
	shf.r.wrap.b32 	%r601, %r598, %r599, 14;
	mov.b64 	%rd767, {%r601, %r600};
	shf.r.wrap.b32 	%r602, %r599, %r598, 18;
	shf.r.wrap.b32 	%r603, %r598, %r599, 18;
	mov.b64 	%rd768, {%r603, %r602};
	xor.b64  	%rd769, %rd767, %rd768;
	shf.l.wrap.b32 	%r604, %r598, %r599, 23;
	shf.l.wrap.b32 	%r605, %r599, %r598, 23;
	mov.b64 	%rd770, {%r605, %r604};
	xor.b64  	%rd771, %rd769, %rd770;
	xor.b64  	%rd772, %rd678, %rd715;
	and.b64  	%rd773, %rd772, %rd752;
	xor.b64  	%rd774, %rd773, %rd678;
	add.s64 	%rd775, %rd771, %rd641;
	add.s64 	%rd776, %rd775, %rd774;
	add.s64 	%rd777, %rd776, %rd766;
	add.s64 	%rd778, %rd777, -7476448914759557205;
	and.b64  	%rd779, %rd753, %rd716;
	or.b64  	%rd780, %rd753, %rd716;
	and.b64  	%rd781, %rd780, %rd679;
	or.b64  	%rd782, %rd781, %rd779;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r606,%dummy}, %rd753;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r607}, %rd753;
	}
	shf.r.wrap.b32 	%r608, %r607, %r606, 28;
	shf.r.wrap.b32 	%r609, %r606, %r607, 28;
	mov.b64 	%rd783, {%r609, %r608};
	shf.l.wrap.b32 	%r610, %r606, %r607, 30;
	shf.l.wrap.b32 	%r611, %r607, %r606, 30;
	mov.b64 	%rd784, {%r611, %r610};
	xor.b64  	%rd785, %rd783, %rd784;
	shf.l.wrap.b32 	%r612, %r606, %r607, 25;
	shf.l.wrap.b32 	%r613, %r607, %r606, 25;
	mov.b64 	%rd786, {%r613, %r612};
	xor.b64  	%rd787, %rd785, %rd786;
	add.s64 	%rd788, %rd782, %rd787;
	add.s64 	%rd789, %rd778, %rd642;
	add.s64 	%rd790, %rd788, %rd778;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r614,%dummy}, %rd729;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r615}, %rd729;
	}
	shf.r.wrap.b32 	%r616, %r615, %r614, 19;
	shf.r.wrap.b32 	%r617, %r614, %r615, 19;
	mov.b64 	%rd791, {%r617, %r616};
	shf.l.wrap.b32 	%r618, %r614, %r615, 3;
	shf.l.wrap.b32 	%r619, %r615, %r614, 3;
	mov.b64 	%rd792, {%r619, %r618};
	xor.b64  	%rd793, %rd791, %rd792;
	shr.u64 	%rd794, %rd729, 6;
	xor.b64  	%rd795, %rd793, %rd794;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r620,%dummy}, %rd67;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r621}, %rd67;
	}
	shf.r.wrap.b32 	%r622, %r621, %r620, 1;
	shf.r.wrap.b32 	%r623, %r620, %r621, 1;
	mov.b64 	%rd796, {%r623, %r622};
	shf.r.wrap.b32 	%r624, %r621, %r620, 8;
	shf.r.wrap.b32 	%r625, %r620, %r621, 8;
	mov.b64 	%rd797, {%r625, %r624};
	xor.b64  	%rd798, %rd796, %rd797;
	shr.u64 	%rd799, %rd67, 7;
	xor.b64  	%rd800, %rd798, %rd799;
	add.s64 	%rd801, %rd795, %rd65;
	add.s64 	%rd802, %rd801, %rd544;
	add.s64 	%rd803, %rd802, %rd800;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r626,%dummy}, %rd789;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r627}, %rd789;
	}
	shf.r.wrap.b32 	%r628, %r627, %r626, 14;
	shf.r.wrap.b32 	%r629, %r626, %r627, 14;
	mov.b64 	%rd804, {%r629, %r628};
	shf.r.wrap.b32 	%r630, %r627, %r626, 18;
	shf.r.wrap.b32 	%r631, %r626, %r627, 18;
	mov.b64 	%rd805, {%r631, %r630};
	xor.b64  	%rd806, %rd804, %rd805;
	shf.l.wrap.b32 	%r632, %r626, %r627, 23;
	shf.l.wrap.b32 	%r633, %r627, %r626, 23;
	mov.b64 	%rd807, {%r633, %r632};
	xor.b64  	%rd808, %rd806, %rd807;
	xor.b64  	%rd809, %rd715, %rd752;
	and.b64  	%rd810, %rd809, %rd789;
	xor.b64  	%rd811, %rd810, %rd715;
	add.s64 	%rd812, %rd808, %rd678;
	add.s64 	%rd813, %rd812, %rd811;
	add.s64 	%rd814, %rd813, %rd803;
	add.s64 	%rd815, %rd814, -6327057829258317296;
	and.b64  	%rd816, %rd790, %rd753;
	or.b64  	%rd817, %rd790, %rd753;
	and.b64  	%rd818, %rd817, %rd716;
	or.b64  	%rd819, %rd818, %rd816;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r634,%dummy}, %rd790;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r635}, %rd790;
	}
	shf.r.wrap.b32 	%r636, %r635, %r634, 28;
	shf.r.wrap.b32 	%r637, %r634, %r635, 28;
	mov.b64 	%rd820, {%r637, %r636};
	shf.l.wrap.b32 	%r638, %r634, %r635, 30;
	shf.l.wrap.b32 	%r639, %r635, %r634, 30;
	mov.b64 	%rd821, {%r639, %r638};
	xor.b64  	%rd822, %rd820, %rd821;
	shf.l.wrap.b32 	%r640, %r634, %r635, 25;
	shf.l.wrap.b32 	%r641, %r635, %r634, 25;
	mov.b64 	%rd823, {%r641, %r640};
	xor.b64  	%rd824, %rd822, %rd823;
	add.s64 	%rd825, %rd819, %rd824;
	add.s64 	%rd826, %rd815, %rd679;
	add.s64 	%rd827, %rd825, %rd815;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r642,%dummy}, %rd766;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r643}, %rd766;
	}
	shf.r.wrap.b32 	%r644, %r643, %r642, 19;
	shf.r.wrap.b32 	%r645, %r642, %r643, 19;
	mov.b64 	%rd828, {%r645, %r644};
	shf.l.wrap.b32 	%r646, %r642, %r643, 3;
	shf.l.wrap.b32 	%r647, %r643, %r642, 3;
	mov.b64 	%rd829, {%r647, %r646};
	xor.b64  	%rd830, %rd828, %rd829;
	shr.u64 	%rd831, %rd766, 6;
	xor.b64  	%rd832, %rd830, %rd831;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r648,%dummy}, %rd69;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r649}, %rd69;
	}
	shf.r.wrap.b32 	%r650, %r649, %r648, 1;
	shf.r.wrap.b32 	%r651, %r648, %r649, 1;
	mov.b64 	%rd833, {%r651, %r650};
	shf.r.wrap.b32 	%r652, %r649, %r648, 8;
	shf.r.wrap.b32 	%r653, %r648, %r649, 8;
	mov.b64 	%rd834, {%r653, %r652};
	xor.b64  	%rd835, %rd833, %rd834;
	shr.u64 	%rd836, %rd69, 7;
	xor.b64  	%rd837, %rd835, %rd836;
	add.s64 	%rd838, %rd832, %rd67;
	add.s64 	%rd839, %rd838, %rd581;
	add.s64 	%rd840, %rd839, %rd837;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r654,%dummy}, %rd826;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r655}, %rd826;
	}
	shf.r.wrap.b32 	%r656, %r655, %r654, 14;
	shf.r.wrap.b32 	%r657, %r654, %r655, 14;
	mov.b64 	%rd841, {%r657, %r656};
	shf.r.wrap.b32 	%r658, %r655, %r654, 18;
	shf.r.wrap.b32 	%r659, %r654, %r655, 18;
	mov.b64 	%rd842, {%r659, %r658};
	xor.b64  	%rd843, %rd841, %rd842;
	shf.l.wrap.b32 	%r660, %r654, %r655, 23;
	shf.l.wrap.b32 	%r661, %r655, %r654, 23;
	mov.b64 	%rd844, {%r661, %r660};
	xor.b64  	%rd845, %rd843, %rd844;
	xor.b64  	%rd846, %rd752, %rd789;
	and.b64  	%rd847, %rd846, %rd826;
	xor.b64  	%rd848, %rd847, %rd752;
	add.s64 	%rd849, %rd845, %rd715;
	add.s64 	%rd850, %rd849, %rd848;
	add.s64 	%rd851, %rd850, %rd840;
	add.s64 	%rd852, %rd851, -5763719355590565569;
	and.b64  	%rd853, %rd827, %rd790;
	or.b64  	%rd854, %rd827, %rd790;
	and.b64  	%rd855, %rd854, %rd753;
	or.b64  	%rd856, %rd855, %rd853;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r662,%dummy}, %rd827;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r663}, %rd827;
	}
	shf.r.wrap.b32 	%r664, %r663, %r662, 28;
	shf.r.wrap.b32 	%r665, %r662, %r663, 28;
	mov.b64 	%rd857, {%r665, %r664};
	shf.l.wrap.b32 	%r666, %r662, %r663, 30;
	shf.l.wrap.b32 	%r667, %r663, %r662, 30;
	mov.b64 	%rd858, {%r667, %r666};
	xor.b64  	%rd859, %rd857, %rd858;
	shf.l.wrap.b32 	%r668, %r662, %r663, 25;
	shf.l.wrap.b32 	%r669, %r663, %r662, 25;
	mov.b64 	%rd860, {%r669, %r668};
	xor.b64  	%rd861, %rd859, %rd860;
	add.s64 	%rd862, %rd856, %rd861;
	add.s64 	%rd863, %rd852, %rd716;
	add.s64 	%rd864, %rd862, %rd852;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r670,%dummy}, %rd803;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r671}, %rd803;
	}
	shf.r.wrap.b32 	%r672, %r671, %r670, 19;
	shf.r.wrap.b32 	%r673, %r670, %r671, 19;
	mov.b64 	%rd865, {%r673, %r672};
	shf.l.wrap.b32 	%r674, %r670, %r671, 3;
	shf.l.wrap.b32 	%r675, %r671, %r670, 3;
	mov.b64 	%rd866, {%r675, %r674};
	xor.b64  	%rd867, %rd865, %rd866;
	shr.u64 	%rd868, %rd803, 6;
	xor.b64  	%rd869, %rd867, %rd868;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r676,%dummy}, %rd71;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r677}, %rd71;
	}
	shf.r.wrap.b32 	%r678, %r677, %r676, 1;
	shf.r.wrap.b32 	%r679, %r676, %r677, 1;
	mov.b64 	%rd870, {%r679, %r678};
	shf.r.wrap.b32 	%r680, %r677, %r676, 8;
	shf.r.wrap.b32 	%r681, %r676, %r677, 8;
	mov.b64 	%rd871, {%r681, %r680};
	xor.b64  	%rd872, %rd870, %rd871;
	shr.u64 	%rd873, %rd71, 7;
	xor.b64  	%rd874, %rd872, %rd873;
	add.s64 	%rd875, %rd869, %rd69;
	add.s64 	%rd876, %rd875, %rd618;
	add.s64 	%rd877, %rd876, %rd874;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r682,%dummy}, %rd863;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r683}, %rd863;
	}
	shf.r.wrap.b32 	%r684, %r683, %r682, 14;
	shf.r.wrap.b32 	%r685, %r682, %r683, 14;
	mov.b64 	%rd878, {%r685, %r684};
	shf.r.wrap.b32 	%r686, %r683, %r682, 18;
	shf.r.wrap.b32 	%r687, %r682, %r683, 18;
	mov.b64 	%rd879, {%r687, %r686};
	xor.b64  	%rd880, %rd878, %rd879;
	shf.l.wrap.b32 	%r688, %r682, %r683, 23;
	shf.l.wrap.b32 	%r689, %r683, %r682, 23;
	mov.b64 	%rd881, {%r689, %r688};
	xor.b64  	%rd882, %rd880, %rd881;
	xor.b64  	%rd883, %rd789, %rd826;
	and.b64  	%rd884, %rd883, %rd863;
	xor.b64  	%rd885, %rd884, %rd789;
	add.s64 	%rd886, %rd882, %rd752;
	add.s64 	%rd887, %rd886, %rd885;
	add.s64 	%rd888, %rd887, %rd877;
	add.s64 	%rd889, %rd888, -4658551843659510044;
	and.b64  	%rd890, %rd864, %rd827;
	or.b64  	%rd891, %rd864, %rd827;
	and.b64  	%rd892, %rd891, %rd790;
	or.b64  	%rd893, %rd892, %rd890;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r690,%dummy}, %rd864;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r691}, %rd864;
	}
	shf.r.wrap.b32 	%r692, %r691, %r690, 28;
	shf.r.wrap.b32 	%r693, %r690, %r691, 28;
	mov.b64 	%rd894, {%r693, %r692};
	shf.l.wrap.b32 	%r694, %r690, %r691, 30;
	shf.l.wrap.b32 	%r695, %r691, %r690, 30;
	mov.b64 	%rd895, {%r695, %r694};
	xor.b64  	%rd896, %rd894, %rd895;
	shf.l.wrap.b32 	%r696, %r690, %r691, 25;
	shf.l.wrap.b32 	%r697, %r691, %r690, 25;
	mov.b64 	%rd897, {%r697, %r696};
	xor.b64  	%rd898, %rd896, %rd897;
	add.s64 	%rd899, %rd893, %rd898;
	add.s64 	%rd900, %rd889, %rd753;
	add.s64 	%rd901, %rd899, %rd889;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r698,%dummy}, %rd840;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r699}, %rd840;
	}
	shf.r.wrap.b32 	%r700, %r699, %r698, 19;
	shf.r.wrap.b32 	%r701, %r698, %r699, 19;
	mov.b64 	%rd902, {%r701, %r700};
	shf.l.wrap.b32 	%r702, %r698, %r699, 3;
	shf.l.wrap.b32 	%r703, %r699, %r698, 3;
	mov.b64 	%rd903, {%r703, %r702};
	xor.b64  	%rd904, %rd902, %rd903;
	shr.u64 	%rd905, %rd840, 6;
	xor.b64  	%rd906, %rd904, %rd905;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r704,%dummy}, %rd73;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r705}, %rd73;
	}
	shf.r.wrap.b32 	%r706, %r705, %r704, 1;
	shf.r.wrap.b32 	%r707, %r704, %r705, 1;
	mov.b64 	%rd907, {%r707, %r706};
	shf.r.wrap.b32 	%r708, %r705, %r704, 8;
	shf.r.wrap.b32 	%r709, %r704, %r705, 8;
	mov.b64 	%rd908, {%r709, %r708};
	xor.b64  	%rd909, %rd907, %rd908;
	shr.u64 	%rd910, %rd73, 7;
	xor.b64  	%rd911, %rd909, %rd910;
	add.s64 	%rd912, %rd906, %rd71;
	add.s64 	%rd913, %rd912, %rd655;
	add.s64 	%rd914, %rd913, %rd911;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r710,%dummy}, %rd900;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r711}, %rd900;
	}
	shf.r.wrap.b32 	%r712, %r711, %r710, 14;
	shf.r.wrap.b32 	%r713, %r710, %r711, 14;
	mov.b64 	%rd915, {%r713, %r712};
	shf.r.wrap.b32 	%r714, %r711, %r710, 18;
	shf.r.wrap.b32 	%r715, %r710, %r711, 18;
	mov.b64 	%rd916, {%r715, %r714};
	xor.b64  	%rd917, %rd915, %rd916;
	shf.l.wrap.b32 	%r716, %r710, %r711, 23;
	shf.l.wrap.b32 	%r717, %r711, %r710, 23;
	mov.b64 	%rd918, {%r717, %r716};
	xor.b64  	%rd919, %rd917, %rd918;
	xor.b64  	%rd920, %rd826, %rd863;
	and.b64  	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd826;
	add.s64 	%rd923, %rd919, %rd789;
	add.s64 	%rd924, %rd923, %rd922;
	add.s64 	%rd925, %rd924, %rd914;
	add.s64 	%rd926, %rd925, -4116276920077217854;
	and.b64  	%rd927, %rd901, %rd864;
	or.b64  	%rd928, %rd901, %rd864;
	and.b64  	%rd929, %rd928, %rd827;
	or.b64  	%rd930, %rd929, %rd927;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r718,%dummy}, %rd901;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r719}, %rd901;
	}
	shf.r.wrap.b32 	%r720, %r719, %r718, 28;
	shf.r.wrap.b32 	%r721, %r718, %r719, 28;
	mov.b64 	%rd931, {%r721, %r720};
	shf.l.wrap.b32 	%r722, %r718, %r719, 30;
	shf.l.wrap.b32 	%r723, %r719, %r718, 30;
	mov.b64 	%rd932, {%r723, %r722};
	xor.b64  	%rd933, %rd931, %rd932;
	shf.l.wrap.b32 	%r724, %r718, %r719, 25;
	shf.l.wrap.b32 	%r725, %r719, %r718, 25;
	mov.b64 	%rd934, {%r725, %r724};
	xor.b64  	%rd935, %rd933, %rd934;
	add.s64 	%rd936, %rd930, %rd935;
	add.s64 	%rd937, %rd926, %rd790;
	add.s64 	%rd938, %rd936, %rd926;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r726,%dummy}, %rd877;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r727}, %rd877;
	}
	shf.r.wrap.b32 	%r728, %r727, %r726, 19;
	shf.r.wrap.b32 	%r729, %r726, %r727, 19;
	mov.b64 	%rd939, {%r729, %r728};
	shf.l.wrap.b32 	%r730, %r726, %r727, 3;
	shf.l.wrap.b32 	%r731, %r727, %r726, 3;
	mov.b64 	%rd940, {%r731, %r730};
	xor.b64  	%rd941, %rd939, %rd940;
	shr.u64 	%rd942, %rd877, 6;
	xor.b64  	%rd943, %rd941, %rd942;
	shf.r.wrap.b32 	%r732, %r363, %r362, 1;
	shf.r.wrap.b32 	%r733, %r362, %r363, 1;
	mov.b64 	%rd944, {%r733, %r732};
	shf.r.wrap.b32 	%r734, %r363, %r362, 8;
	shf.r.wrap.b32 	%r735, %r362, %r363, 8;
	mov.b64 	%rd945, {%r735, %r734};
	xor.b64  	%rd946, %rd944, %rd945;
	shr.u64 	%rd947, %rd75, 7;
	xor.b64  	%rd948, %rd946, %rd947;
	add.s64 	%rd949, %rd943, %rd73;
	add.s64 	%rd950, %rd949, %rd692;
	add.s64 	%rd951, %rd950, %rd948;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r736,%dummy}, %rd937;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r737}, %rd937;
	}
	shf.r.wrap.b32 	%r738, %r737, %r736, 14;
	shf.r.wrap.b32 	%r739, %r736, %r737, 14;
	mov.b64 	%rd952, {%r739, %r738};
	shf.r.wrap.b32 	%r740, %r737, %r736, 18;
	shf.r.wrap.b32 	%r741, %r736, %r737, 18;
	mov.b64 	%rd953, {%r741, %r740};
	xor.b64  	%rd954, %rd952, %rd953;
	shf.l.wrap.b32 	%r742, %r736, %r737, 23;
	shf.l.wrap.b32 	%r743, %r737, %r736, 23;
	mov.b64 	%rd955, {%r743, %r742};
	xor.b64  	%rd956, %rd954, %rd955;
	xor.b64  	%rd957, %rd863, %rd900;
	and.b64  	%rd958, %rd957, %rd937;
	xor.b64  	%rd959, %rd958, %rd863;
	add.s64 	%rd960, %rd956, %rd826;
	add.s64 	%rd961, %rd960, %rd959;
	add.s64 	%rd962, %rd961, %rd951;
	add.s64 	%rd963, %rd962, -3051310485924567259;
	and.b64  	%rd964, %rd938, %rd901;
	or.b64  	%rd965, %rd938, %rd901;
	and.b64  	%rd966, %rd965, %rd864;
	or.b64  	%rd967, %rd966, %rd964;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r744,%dummy}, %rd938;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r745}, %rd938;
	}
	shf.r.wrap.b32 	%r746, %r745, %r744, 28;
	shf.r.wrap.b32 	%r747, %r744, %r745, 28;
	mov.b64 	%rd968, {%r747, %r746};
	shf.l.wrap.b32 	%r748, %r744, %r745, 30;
	shf.l.wrap.b32 	%r749, %r745, %r744, 30;
	mov.b64 	%rd969, {%r749, %r748};
	xor.b64  	%rd970, %rd968, %rd969;
	shf.l.wrap.b32 	%r750, %r744, %r745, 25;
	shf.l.wrap.b32 	%r751, %r745, %r744, 25;
	mov.b64 	%rd971, {%r751, %r750};
	xor.b64  	%rd972, %rd970, %rd971;
	add.s64 	%rd973, %rd967, %rd972;
	add.s64 	%rd974, %rd963, %rd827;
	add.s64 	%rd975, %rd973, %rd963;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r752,%dummy}, %rd914;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r753}, %rd914;
	}
	shf.r.wrap.b32 	%r754, %r753, %r752, 19;
	shf.r.wrap.b32 	%r755, %r752, %r753, 19;
	mov.b64 	%rd976, {%r755, %r754};
	shf.l.wrap.b32 	%r756, %r752, %r753, 3;
	shf.l.wrap.b32 	%r757, %r753, %r752, 3;
	mov.b64 	%rd977, {%r757, %r756};
	xor.b64  	%rd978, %rd976, %rd977;
	shr.u64 	%rd979, %rd914, 6;
	xor.b64  	%rd980, %rd978, %rd979;
	shf.r.wrap.b32 	%r758, %r391, %r390, 1;
	shf.r.wrap.b32 	%r759, %r390, %r391, 1;
	mov.b64 	%rd981, {%r759, %r758};
	shf.r.wrap.b32 	%r760, %r391, %r390, 8;
	shf.r.wrap.b32 	%r761, %r390, %r391, 8;
	mov.b64 	%rd982, {%r761, %r760};
	xor.b64  	%rd983, %rd981, %rd982;
	shr.u64 	%rd984, %rd87, 7;
	xor.b64  	%rd985, %rd983, %rd984;
	add.s64 	%rd986, %rd980, %rd75;
	add.s64 	%rd987, %rd986, %rd729;
	add.s64 	%rd988, %rd987, %rd985;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r762,%dummy}, %rd974;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r763}, %rd974;
	}
	shf.r.wrap.b32 	%r764, %r763, %r762, 14;
	shf.r.wrap.b32 	%r765, %r762, %r763, 14;
	mov.b64 	%rd989, {%r765, %r764};
	shf.r.wrap.b32 	%r766, %r763, %r762, 18;
	shf.r.wrap.b32 	%r767, %r762, %r763, 18;
	mov.b64 	%rd990, {%r767, %r766};
	xor.b64  	%rd991, %rd989, %rd990;
	shf.l.wrap.b32 	%r768, %r762, %r763, 23;
	shf.l.wrap.b32 	%r769, %r763, %r762, 23;
	mov.b64 	%rd992, {%r769, %r768};
	xor.b64  	%rd993, %rd991, %rd992;
	xor.b64  	%rd994, %rd900, %rd937;
	and.b64  	%rd995, %rd994, %rd974;
	xor.b64  	%rd996, %rd995, %rd900;
	add.s64 	%rd997, %rd993, %rd863;
	add.s64 	%rd998, %rd997, %rd996;
	add.s64 	%rd999, %rd998, %rd988;
	add.s64 	%rd1000, %rd999, 489312712824947311;
	and.b64  	%rd1001, %rd975, %rd938;
	or.b64  	%rd1002, %rd975, %rd938;
	and.b64  	%rd1003, %rd1002, %rd901;
	or.b64  	%rd1004, %rd1003, %rd1001;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r770,%dummy}, %rd975;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r771}, %rd975;
	}
	shf.r.wrap.b32 	%r772, %r771, %r770, 28;
	shf.r.wrap.b32 	%r773, %r770, %r771, 28;
	mov.b64 	%rd1005, {%r773, %r772};
	shf.l.wrap.b32 	%r774, %r770, %r771, 30;
	shf.l.wrap.b32 	%r775, %r771, %r770, 30;
	mov.b64 	%rd1006, {%r775, %r774};
	xor.b64  	%rd1007, %rd1005, %rd1006;
	shf.l.wrap.b32 	%r776, %r770, %r771, 25;
	shf.l.wrap.b32 	%r777, %r771, %r770, 25;
	mov.b64 	%rd1008, {%r777, %r776};
	xor.b64  	%rd1009, %rd1007, %rd1008;
	add.s64 	%rd1010, %rd1004, %rd1009;
	add.s64 	%rd1011, %rd1000, %rd864;
	add.s64 	%rd1012, %rd1010, %rd1000;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r778,%dummy}, %rd951;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r779}, %rd951;
	}
	shf.r.wrap.b32 	%r780, %r779, %r778, 19;
	shf.r.wrap.b32 	%r781, %r778, %r779, 19;
	mov.b64 	%rd1013, {%r781, %r780};
	shf.l.wrap.b32 	%r782, %r778, %r779, 3;
	shf.l.wrap.b32 	%r783, %r779, %r778, 3;
	mov.b64 	%rd1014, {%r783, %r782};
	xor.b64  	%rd1015, %rd1013, %rd1014;
	shr.u64 	%rd1016, %rd951, 6;
	xor.b64  	%rd1017, %rd1015, %rd1016;
	shf.r.wrap.b32 	%r784, %r419, %r418, 1;
	shf.r.wrap.b32 	%r785, %r418, %r419, 1;
	mov.b64 	%rd1018, {%r785, %r784};
	shf.r.wrap.b32 	%r786, %r419, %r418, 8;
	shf.r.wrap.b32 	%r787, %r418, %r419, 8;
	mov.b64 	%rd1019, {%r787, %r786};
	xor.b64  	%rd1020, %rd1018, %rd1019;
	shr.u64 	%rd1021, %rd470, 7;
	xor.b64  	%rd1022, %rd1020, %rd1021;
	add.s64 	%rd1023, %rd1017, %rd87;
	add.s64 	%rd1024, %rd1023, %rd766;
	add.s64 	%rd1025, %rd1024, %rd1022;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r788,%dummy}, %rd1011;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r789}, %rd1011;
	}
	shf.r.wrap.b32 	%r790, %r789, %r788, 14;
	shf.r.wrap.b32 	%r791, %r788, %r789, 14;
	mov.b64 	%rd1026, {%r791, %r790};
	shf.r.wrap.b32 	%r792, %r789, %r788, 18;
	shf.r.wrap.b32 	%r793, %r788, %r789, 18;
	mov.b64 	%rd1027, {%r793, %r792};
	xor.b64  	%rd1028, %rd1026, %rd1027;
	shf.l.wrap.b32 	%r794, %r788, %r789, 23;
	shf.l.wrap.b32 	%r795, %r789, %r788, 23;
	mov.b64 	%rd1029, {%r795, %r794};
	xor.b64  	%rd1030, %rd1028, %rd1029;
	xor.b64  	%rd1031, %rd937, %rd974;
	and.b64  	%rd1032, %rd1031, %rd1011;
	xor.b64  	%rd1033, %rd1032, %rd937;
	add.s64 	%rd1034, %rd1030, %rd900;
	add.s64 	%rd1035, %rd1034, %rd1033;
	add.s64 	%rd1036, %rd1035, %rd1025;
	add.s64 	%rd1037, %rd1036, 1452737877330783856;
	and.b64  	%rd1038, %rd1012, %rd975;
	or.b64  	%rd1039, %rd1012, %rd975;
	and.b64  	%rd1040, %rd1039, %rd938;
	or.b64  	%rd1041, %rd1040, %rd1038;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r796,%dummy}, %rd1012;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r797}, %rd1012;
	}
	shf.r.wrap.b32 	%r798, %r797, %r796, 28;
	shf.r.wrap.b32 	%r799, %r796, %r797, 28;
	mov.b64 	%rd1042, {%r799, %r798};
	shf.l.wrap.b32 	%r800, %r796, %r797, 30;
	shf.l.wrap.b32 	%r801, %r797, %r796, 30;
	mov.b64 	%rd1043, {%r801, %r800};
	xor.b64  	%rd1044, %rd1042, %rd1043;
	shf.l.wrap.b32 	%r802, %r796, %r797, 25;
	shf.l.wrap.b32 	%r803, %r797, %r796, 25;
	mov.b64 	%rd1045, {%r803, %r802};
	xor.b64  	%rd1046, %rd1044, %rd1045;
	add.s64 	%rd1047, %rd1041, %rd1046;
	add.s64 	%rd1048, %rd1037, %rd901;
	add.s64 	%rd1049, %rd1047, %rd1037;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r804,%dummy}, %rd988;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r805}, %rd988;
	}
	shf.r.wrap.b32 	%r806, %r805, %r804, 19;
	shf.r.wrap.b32 	%r807, %r804, %r805, 19;
	mov.b64 	%rd1050, {%r807, %r806};
	shf.l.wrap.b32 	%r808, %r804, %r805, 3;
	shf.l.wrap.b32 	%r809, %r805, %r804, 3;
	mov.b64 	%rd1051, {%r809, %r808};
	xor.b64  	%rd1052, %rd1050, %rd1051;
	shr.u64 	%rd1053, %rd988, 6;
	xor.b64  	%rd1054, %rd1052, %rd1053;
	shf.r.wrap.b32 	%r810, %r447, %r446, 1;
	shf.r.wrap.b32 	%r811, %r446, %r447, 1;
	mov.b64 	%rd1055, {%r811, %r810};
	shf.r.wrap.b32 	%r812, %r447, %r446, 8;
	shf.r.wrap.b32 	%r813, %r446, %r447, 8;
	mov.b64 	%rd1056, {%r813, %r812};
	xor.b64  	%rd1057, %rd1055, %rd1056;
	shr.u64 	%rd1058, %rd507, 7;
	xor.b64  	%rd1059, %rd1057, %rd1058;
	add.s64 	%rd1060, %rd1054, %rd470;
	add.s64 	%rd1061, %rd1060, %rd803;
	add.s64 	%rd1062, %rd1061, %rd1059;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r814,%dummy}, %rd1048;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r815}, %rd1048;
	}
	shf.r.wrap.b32 	%r816, %r815, %r814, 14;
	shf.r.wrap.b32 	%r817, %r814, %r815, 14;
	mov.b64 	%rd1063, {%r817, %r816};
	shf.r.wrap.b32 	%r818, %r815, %r814, 18;
	shf.r.wrap.b32 	%r819, %r814, %r815, 18;
	mov.b64 	%rd1064, {%r819, %r818};
	xor.b64  	%rd1065, %rd1063, %rd1064;
	shf.l.wrap.b32 	%r820, %r814, %r815, 23;
	shf.l.wrap.b32 	%r821, %r815, %r814, 23;
	mov.b64 	%rd1066, {%r821, %r820};
	xor.b64  	%rd1067, %rd1065, %rd1066;
	xor.b64  	%rd1068, %rd974, %rd1011;
	and.b64  	%rd1069, %rd1068, %rd1048;
	xor.b64  	%rd1070, %rd1069, %rd974;
	add.s64 	%rd1071, %rd1067, %rd937;
	add.s64 	%rd1072, %rd1071, %rd1070;
	add.s64 	%rd1073, %rd1072, %rd1062;
	add.s64 	%rd1074, %rd1073, 2861767655752347644;
	and.b64  	%rd1075, %rd1049, %rd1012;
	or.b64  	%rd1076, %rd1049, %rd1012;
	and.b64  	%rd1077, %rd1076, %rd975;
	or.b64  	%rd1078, %rd1077, %rd1075;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r822,%dummy}, %rd1049;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r823}, %rd1049;
	}
	shf.r.wrap.b32 	%r824, %r823, %r822, 28;
	shf.r.wrap.b32 	%r825, %r822, %r823, 28;
	mov.b64 	%rd1079, {%r825, %r824};
	shf.l.wrap.b32 	%r826, %r822, %r823, 30;
	shf.l.wrap.b32 	%r827, %r823, %r822, 30;
	mov.b64 	%rd1080, {%r827, %r826};
	xor.b64  	%rd1081, %rd1079, %rd1080;
	shf.l.wrap.b32 	%r828, %r822, %r823, 25;
	shf.l.wrap.b32 	%r829, %r823, %r822, 25;
	mov.b64 	%rd1082, {%r829, %r828};
	xor.b64  	%rd1083, %rd1081, %rd1082;
	add.s64 	%rd1084, %rd1078, %rd1083;
	add.s64 	%rd1085, %rd1074, %rd938;
	add.s64 	%rd1086, %rd1084, %rd1074;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r830,%dummy}, %rd1025;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r831}, %rd1025;
	}
	shf.r.wrap.b32 	%r832, %r831, %r830, 19;
	shf.r.wrap.b32 	%r833, %r830, %r831, 19;
	mov.b64 	%rd1087, {%r833, %r832};
	shf.l.wrap.b32 	%r834, %r830, %r831, 3;
	shf.l.wrap.b32 	%r835, %r831, %r830, 3;
	mov.b64 	%rd1088, {%r835, %r834};
	xor.b64  	%rd1089, %rd1087, %rd1088;
	shr.u64 	%rd1090, %rd1025, 6;
	xor.b64  	%rd1091, %rd1089, %rd1090;
	shf.r.wrap.b32 	%r836, %r475, %r474, 1;
	shf.r.wrap.b32 	%r837, %r474, %r475, 1;
	mov.b64 	%rd1092, {%r837, %r836};
	shf.r.wrap.b32 	%r838, %r475, %r474, 8;
	shf.r.wrap.b32 	%r839, %r474, %r475, 8;
	mov.b64 	%rd1093, {%r839, %r838};
	xor.b64  	%rd1094, %rd1092, %rd1093;
	shr.u64 	%rd1095, %rd544, 7;
	xor.b64  	%rd1096, %rd1094, %rd1095;
	add.s64 	%rd1097, %rd1091, %rd507;
	add.s64 	%rd1098, %rd1097, %rd840;
	add.s64 	%rd1099, %rd1098, %rd1096;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r840,%dummy}, %rd1085;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r841}, %rd1085;
	}
	shf.r.wrap.b32 	%r842, %r841, %r840, 14;
	shf.r.wrap.b32 	%r843, %r840, %r841, 14;
	mov.b64 	%rd1100, {%r843, %r842};
	shf.r.wrap.b32 	%r844, %r841, %r840, 18;
	shf.r.wrap.b32 	%r845, %r840, %r841, 18;
	mov.b64 	%rd1101, {%r845, %r844};
	xor.b64  	%rd1102, %rd1100, %rd1101;
	shf.l.wrap.b32 	%r846, %r840, %r841, 23;
	shf.l.wrap.b32 	%r847, %r841, %r840, 23;
	mov.b64 	%rd1103, {%r847, %r846};
	xor.b64  	%rd1104, %rd1102, %rd1103;
	xor.b64  	%rd1105, %rd1011, %rd1048;
	and.b64  	%rd1106, %rd1105, %rd1085;
	xor.b64  	%rd1107, %rd1106, %rd1011;
	add.s64 	%rd1108, %rd1104, %rd974;
	add.s64 	%rd1109, %rd1108, %rd1107;
	add.s64 	%rd1110, %rd1109, %rd1099;
	add.s64 	%rd1111, %rd1110, 3322285676063803686;
	and.b64  	%rd1112, %rd1086, %rd1049;
	or.b64  	%rd1113, %rd1086, %rd1049;
	and.b64  	%rd1114, %rd1113, %rd1012;
	or.b64  	%rd1115, %rd1114, %rd1112;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r848,%dummy}, %rd1086;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r849}, %rd1086;
	}
	shf.r.wrap.b32 	%r850, %r849, %r848, 28;
	shf.r.wrap.b32 	%r851, %r848, %r849, 28;
	mov.b64 	%rd1116, {%r851, %r850};
	shf.l.wrap.b32 	%r852, %r848, %r849, 30;
	shf.l.wrap.b32 	%r853, %r849, %r848, 30;
	mov.b64 	%rd1117, {%r853, %r852};
	xor.b64  	%rd1118, %rd1116, %rd1117;
	shf.l.wrap.b32 	%r854, %r848, %r849, 25;
	shf.l.wrap.b32 	%r855, %r849, %r848, 25;
	mov.b64 	%rd1119, {%r855, %r854};
	xor.b64  	%rd1120, %rd1118, %rd1119;
	add.s64 	%rd1121, %rd1115, %rd1120;
	add.s64 	%rd1122, %rd1111, %rd975;
	add.s64 	%rd1123, %rd1121, %rd1111;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r856,%dummy}, %rd1062;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r857}, %rd1062;
	}
	shf.r.wrap.b32 	%r858, %r857, %r856, 19;
	shf.r.wrap.b32 	%r859, %r856, %r857, 19;
	mov.b64 	%rd1124, {%r859, %r858};
	shf.l.wrap.b32 	%r860, %r856, %r857, 3;
	shf.l.wrap.b32 	%r861, %r857, %r856, 3;
	mov.b64 	%rd1125, {%r861, %r860};
	xor.b64  	%rd1126, %rd1124, %rd1125;
	shr.u64 	%rd1127, %rd1062, 6;
	xor.b64  	%rd1128, %rd1126, %rd1127;
	shf.r.wrap.b32 	%r862, %r503, %r502, 1;
	shf.r.wrap.b32 	%r863, %r502, %r503, 1;
	mov.b64 	%rd1129, {%r863, %r862};
	shf.r.wrap.b32 	%r864, %r503, %r502, 8;
	shf.r.wrap.b32 	%r865, %r502, %r503, 8;
	mov.b64 	%rd1130, {%r865, %r864};
	xor.b64  	%rd1131, %rd1129, %rd1130;
	shr.u64 	%rd1132, %rd581, 7;
	xor.b64  	%rd1133, %rd1131, %rd1132;
	add.s64 	%rd1134, %rd1128, %rd544;
	add.s64 	%rd1135, %rd1134, %rd877;
	add.s64 	%rd1136, %rd1135, %rd1133;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r866,%dummy}, %rd1122;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r867}, %rd1122;
	}
	shf.r.wrap.b32 	%r868, %r867, %r866, 14;
	shf.r.wrap.b32 	%r869, %r866, %r867, 14;
	mov.b64 	%rd1137, {%r869, %r868};
	shf.r.wrap.b32 	%r870, %r867, %r866, 18;
	shf.r.wrap.b32 	%r871, %r866, %r867, 18;
	mov.b64 	%rd1138, {%r871, %r870};
	xor.b64  	%rd1139, %rd1137, %rd1138;
	shf.l.wrap.b32 	%r872, %r866, %r867, 23;
	shf.l.wrap.b32 	%r873, %r867, %r866, 23;
	mov.b64 	%rd1140, {%r873, %r872};
	xor.b64  	%rd1141, %rd1139, %rd1140;
	xor.b64  	%rd1142, %rd1048, %rd1085;
	and.b64  	%rd1143, %rd1142, %rd1122;
	xor.b64  	%rd1144, %rd1143, %rd1048;
	add.s64 	%rd1145, %rd1141, %rd1011;
	add.s64 	%rd1146, %rd1145, %rd1144;
	add.s64 	%rd1147, %rd1146, %rd1136;
	add.s64 	%rd1148, %rd1147, 5560940570517711597;
	and.b64  	%rd1149, %rd1123, %rd1086;
	or.b64  	%rd1150, %rd1123, %rd1086;
	and.b64  	%rd1151, %rd1150, %rd1049;
	or.b64  	%rd1152, %rd1151, %rd1149;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r874,%dummy}, %rd1123;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r875}, %rd1123;
	}
	shf.r.wrap.b32 	%r876, %r875, %r874, 28;
	shf.r.wrap.b32 	%r877, %r874, %r875, 28;
	mov.b64 	%rd1153, {%r877, %r876};
	shf.l.wrap.b32 	%r878, %r874, %r875, 30;
	shf.l.wrap.b32 	%r879, %r875, %r874, 30;
	mov.b64 	%rd1154, {%r879, %r878};
	xor.b64  	%rd1155, %rd1153, %rd1154;
	shf.l.wrap.b32 	%r880, %r874, %r875, 25;
	shf.l.wrap.b32 	%r881, %r875, %r874, 25;
	mov.b64 	%rd1156, {%r881, %r880};
	xor.b64  	%rd1157, %rd1155, %rd1156;
	add.s64 	%rd1158, %rd1152, %rd1157;
	add.s64 	%rd1159, %rd1148, %rd1012;
	add.s64 	%rd1160, %rd1158, %rd1148;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r882,%dummy}, %rd1099;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r883}, %rd1099;
	}
	shf.r.wrap.b32 	%r884, %r883, %r882, 19;
	shf.r.wrap.b32 	%r885, %r882, %r883, 19;
	mov.b64 	%rd1161, {%r885, %r884};
	shf.l.wrap.b32 	%r886, %r882, %r883, 3;
	shf.l.wrap.b32 	%r887, %r883, %r882, 3;
	mov.b64 	%rd1162, {%r887, %r886};
	xor.b64  	%rd1163, %rd1161, %rd1162;
	shr.u64 	%rd1164, %rd1099, 6;
	xor.b64  	%rd1165, %rd1163, %rd1164;
	shf.r.wrap.b32 	%r888, %r531, %r530, 1;
	shf.r.wrap.b32 	%r889, %r530, %r531, 1;
	mov.b64 	%rd1166, {%r889, %r888};
	shf.r.wrap.b32 	%r890, %r531, %r530, 8;
	shf.r.wrap.b32 	%r891, %r530, %r531, 8;
	mov.b64 	%rd1167, {%r891, %r890};
	xor.b64  	%rd1168, %rd1166, %rd1167;
	shr.u64 	%rd1169, %rd618, 7;
	xor.b64  	%rd1170, %rd1168, %rd1169;
	add.s64 	%rd1171, %rd1165, %rd581;
	add.s64 	%rd1172, %rd1171, %rd914;
	add.s64 	%rd1173, %rd1172, %rd1170;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r892,%dummy}, %rd1159;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r893}, %rd1159;
	}
	shf.r.wrap.b32 	%r894, %r893, %r892, 14;
	shf.r.wrap.b32 	%r895, %r892, %r893, 14;
	mov.b64 	%rd1174, {%r895, %r894};
	shf.r.wrap.b32 	%r896, %r893, %r892, 18;
	shf.r.wrap.b32 	%r897, %r892, %r893, 18;
	mov.b64 	%rd1175, {%r897, %r896};
	xor.b64  	%rd1176, %rd1174, %rd1175;
	shf.l.wrap.b32 	%r898, %r892, %r893, 23;
	shf.l.wrap.b32 	%r899, %r893, %r892, 23;
	mov.b64 	%rd1177, {%r899, %r898};
	xor.b64  	%rd1178, %rd1176, %rd1177;
	xor.b64  	%rd1179, %rd1085, %rd1122;
	and.b64  	%rd1180, %rd1179, %rd1159;
	xor.b64  	%rd1181, %rd1180, %rd1085;
	add.s64 	%rd1182, %rd1178, %rd1048;
	add.s64 	%rd1183, %rd1182, %rd1181;
	add.s64 	%rd1184, %rd1183, %rd1173;
	add.s64 	%rd1185, %rd1184, 5996557281743188959;
	and.b64  	%rd1186, %rd1160, %rd1123;
	or.b64  	%rd1187, %rd1160, %rd1123;
	and.b64  	%rd1188, %rd1187, %rd1086;
	or.b64  	%rd1189, %rd1188, %rd1186;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r900,%dummy}, %rd1160;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r901}, %rd1160;
	}
	shf.r.wrap.b32 	%r902, %r901, %r900, 28;
	shf.r.wrap.b32 	%r903, %r900, %r901, 28;
	mov.b64 	%rd1190, {%r903, %r902};
	shf.l.wrap.b32 	%r904, %r900, %r901, 30;
	shf.l.wrap.b32 	%r905, %r901, %r900, 30;
	mov.b64 	%rd1191, {%r905, %r904};
	xor.b64  	%rd1192, %rd1190, %rd1191;
	shf.l.wrap.b32 	%r906, %r900, %r901, 25;
	shf.l.wrap.b32 	%r907, %r901, %r900, 25;
	mov.b64 	%rd1193, {%r907, %r906};
	xor.b64  	%rd1194, %rd1192, %rd1193;
	add.s64 	%rd1195, %rd1189, %rd1194;
	add.s64 	%rd1196, %rd1185, %rd1049;
	add.s64 	%rd1197, %rd1195, %rd1185;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r908,%dummy}, %rd1136;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r909}, %rd1136;
	}
	shf.r.wrap.b32 	%r910, %r909, %r908, 19;
	shf.r.wrap.b32 	%r911, %r908, %r909, 19;
	mov.b64 	%rd1198, {%r911, %r910};
	shf.l.wrap.b32 	%r912, %r908, %r909, 3;
	shf.l.wrap.b32 	%r913, %r909, %r908, 3;
	mov.b64 	%rd1199, {%r913, %r912};
	xor.b64  	%rd1200, %rd1198, %rd1199;
	shr.u64 	%rd1201, %rd1136, 6;
	xor.b64  	%rd1202, %rd1200, %rd1201;
	shf.r.wrap.b32 	%r914, %r559, %r558, 1;
	shf.r.wrap.b32 	%r915, %r558, %r559, 1;
	mov.b64 	%rd1203, {%r915, %r914};
	shf.r.wrap.b32 	%r916, %r559, %r558, 8;
	shf.r.wrap.b32 	%r917, %r558, %r559, 8;
	mov.b64 	%rd1204, {%r917, %r916};
	xor.b64  	%rd1205, %rd1203, %rd1204;
	shr.u64 	%rd1206, %rd655, 7;
	xor.b64  	%rd1207, %rd1205, %rd1206;
	add.s64 	%rd1208, %rd1202, %rd618;
	add.s64 	%rd1209, %rd1208, %rd951;
	add.s64 	%rd1210, %rd1209, %rd1207;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r918,%dummy}, %rd1196;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r919}, %rd1196;
	}
	shf.r.wrap.b32 	%r920, %r919, %r918, 14;
	shf.r.wrap.b32 	%r921, %r918, %r919, 14;
	mov.b64 	%rd1211, {%r921, %r920};
	shf.r.wrap.b32 	%r922, %r919, %r918, 18;
	shf.r.wrap.b32 	%r923, %r918, %r919, 18;
	mov.b64 	%rd1212, {%r923, %r922};
	xor.b64  	%rd1213, %rd1211, %rd1212;
	shf.l.wrap.b32 	%r924, %r918, %r919, 23;
	shf.l.wrap.b32 	%r925, %r919, %r918, 23;
	mov.b64 	%rd1214, {%r925, %r924};
	xor.b64  	%rd1215, %rd1213, %rd1214;
	xor.b64  	%rd1216, %rd1122, %rd1159;
	and.b64  	%rd1217, %rd1216, %rd1196;
	xor.b64  	%rd1218, %rd1217, %rd1122;
	add.s64 	%rd1219, %rd1215, %rd1085;
	add.s64 	%rd1220, %rd1219, %rd1218;
	add.s64 	%rd1221, %rd1220, %rd1210;
	add.s64 	%rd1222, %rd1221, 7280758554555802590;
	and.b64  	%rd1223, %rd1197, %rd1160;
	or.b64  	%rd1224, %rd1197, %rd1160;
	and.b64  	%rd1225, %rd1224, %rd1123;
	or.b64  	%rd1226, %rd1225, %rd1223;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r926,%dummy}, %rd1197;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r927}, %rd1197;
	}
	shf.r.wrap.b32 	%r928, %r927, %r926, 28;
	shf.r.wrap.b32 	%r929, %r926, %r927, 28;
	mov.b64 	%rd1227, {%r929, %r928};
	shf.l.wrap.b32 	%r930, %r926, %r927, 30;
	shf.l.wrap.b32 	%r931, %r927, %r926, 30;
	mov.b64 	%rd1228, {%r931, %r930};
	xor.b64  	%rd1229, %rd1227, %rd1228;
	shf.l.wrap.b32 	%r932, %r926, %r927, 25;
	shf.l.wrap.b32 	%r933, %r927, %r926, 25;
	mov.b64 	%rd1230, {%r933, %r932};
	xor.b64  	%rd1231, %rd1229, %rd1230;
	add.s64 	%rd1232, %rd1226, %rd1231;
	add.s64 	%rd1233, %rd1222, %rd1086;
	add.s64 	%rd1234, %rd1232, %rd1222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r934,%dummy}, %rd1173;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r935}, %rd1173;
	}
	shf.r.wrap.b32 	%r936, %r935, %r934, 19;
	shf.r.wrap.b32 	%r937, %r934, %r935, 19;
	mov.b64 	%rd1235, {%r937, %r936};
	shf.l.wrap.b32 	%r938, %r934, %r935, 3;
	shf.l.wrap.b32 	%r939, %r935, %r934, 3;
	mov.b64 	%rd1236, {%r939, %r938};
	xor.b64  	%rd1237, %rd1235, %rd1236;
	shr.u64 	%rd1238, %rd1173, 6;
	xor.b64  	%rd1239, %rd1237, %rd1238;
	shf.r.wrap.b32 	%r940, %r587, %r586, 1;
	shf.r.wrap.b32 	%r941, %r586, %r587, 1;
	mov.b64 	%rd1240, {%r941, %r940};
	shf.r.wrap.b32 	%r942, %r587, %r586, 8;
	shf.r.wrap.b32 	%r943, %r586, %r587, 8;
	mov.b64 	%rd1241, {%r943, %r942};
	xor.b64  	%rd1242, %rd1240, %rd1241;
	shr.u64 	%rd1243, %rd692, 7;
	xor.b64  	%rd1244, %rd1242, %rd1243;
	add.s64 	%rd1245, %rd1239, %rd655;
	add.s64 	%rd1246, %rd1245, %rd988;
	add.s64 	%rd1247, %rd1246, %rd1244;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r944,%dummy}, %rd1233;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r945}, %rd1233;
	}
	shf.r.wrap.b32 	%r946, %r945, %r944, 14;
	shf.r.wrap.b32 	%r947, %r944, %r945, 14;
	mov.b64 	%rd1248, {%r947, %r946};
	shf.r.wrap.b32 	%r948, %r945, %r944, 18;
	shf.r.wrap.b32 	%r949, %r944, %r945, 18;
	mov.b64 	%rd1249, {%r949, %r948};
	xor.b64  	%rd1250, %rd1248, %rd1249;
	shf.l.wrap.b32 	%r950, %r944, %r945, 23;
	shf.l.wrap.b32 	%r951, %r945, %r944, 23;
	mov.b64 	%rd1251, {%r951, %r950};
	xor.b64  	%rd1252, %rd1250, %rd1251;
	xor.b64  	%rd1253, %rd1159, %rd1196;
	and.b64  	%rd1254, %rd1253, %rd1233;
	xor.b64  	%rd1255, %rd1254, %rd1159;
	add.s64 	%rd1256, %rd1252, %rd1122;
	add.s64 	%rd1257, %rd1256, %rd1255;
	add.s64 	%rd1258, %rd1257, %rd1247;
	add.s64 	%rd1259, %rd1258, 8532644243296465576;
	and.b64  	%rd1260, %rd1234, %rd1197;
	or.b64  	%rd1261, %rd1234, %rd1197;
	and.b64  	%rd1262, %rd1261, %rd1160;
	or.b64  	%rd1263, %rd1262, %rd1260;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r952,%dummy}, %rd1234;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r953}, %rd1234;
	}
	shf.r.wrap.b32 	%r954, %r953, %r952, 28;
	shf.r.wrap.b32 	%r955, %r952, %r953, 28;
	mov.b64 	%rd1264, {%r955, %r954};
	shf.l.wrap.b32 	%r956, %r952, %r953, 30;
	shf.l.wrap.b32 	%r957, %r953, %r952, 30;
	mov.b64 	%rd1265, {%r957, %r956};
	xor.b64  	%rd1266, %rd1264, %rd1265;
	shf.l.wrap.b32 	%r958, %r952, %r953, 25;
	shf.l.wrap.b32 	%r959, %r953, %r952, 25;
	mov.b64 	%rd1267, {%r959, %r958};
	xor.b64  	%rd1268, %rd1266, %rd1267;
	add.s64 	%rd1269, %rd1263, %rd1268;
	add.s64 	%rd1270, %rd1259, %rd1123;
	add.s64 	%rd1271, %rd1269, %rd1259;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r960,%dummy}, %rd1210;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r961}, %rd1210;
	}
	shf.r.wrap.b32 	%r962, %r961, %r960, 19;
	shf.r.wrap.b32 	%r963, %r960, %r961, 19;
	mov.b64 	%rd1272, {%r963, %r962};
	shf.l.wrap.b32 	%r964, %r960, %r961, 3;
	shf.l.wrap.b32 	%r965, %r961, %r960, 3;
	mov.b64 	%rd1273, {%r965, %r964};
	xor.b64  	%rd1274, %rd1272, %rd1273;
	shr.u64 	%rd1275, %rd1210, 6;
	xor.b64  	%rd1276, %rd1274, %rd1275;
	shf.r.wrap.b32 	%r966, %r615, %r614, 1;
	shf.r.wrap.b32 	%r967, %r614, %r615, 1;
	mov.b64 	%rd1277, {%r967, %r966};
	shf.r.wrap.b32 	%r968, %r615, %r614, 8;
	shf.r.wrap.b32 	%r969, %r614, %r615, 8;
	mov.b64 	%rd1278, {%r969, %r968};
	xor.b64  	%rd1279, %rd1277, %rd1278;
	shr.u64 	%rd1280, %rd729, 7;
	xor.b64  	%rd1281, %rd1279, %rd1280;
	add.s64 	%rd1282, %rd1276, %rd692;
	add.s64 	%rd1283, %rd1282, %rd1025;
	add.s64 	%rd1284, %rd1283, %rd1281;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r970,%dummy}, %rd1270;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r971}, %rd1270;
	}
	shf.r.wrap.b32 	%r972, %r971, %r970, 14;
	shf.r.wrap.b32 	%r973, %r970, %r971, 14;
	mov.b64 	%rd1285, {%r973, %r972};
	shf.r.wrap.b32 	%r974, %r971, %r970, 18;
	shf.r.wrap.b32 	%r975, %r970, %r971, 18;
	mov.b64 	%rd1286, {%r975, %r974};
	xor.b64  	%rd1287, %rd1285, %rd1286;
	shf.l.wrap.b32 	%r976, %r970, %r971, 23;
	shf.l.wrap.b32 	%r977, %r971, %r970, 23;
	mov.b64 	%rd1288, {%r977, %r976};
	xor.b64  	%rd1289, %rd1287, %rd1288;
	xor.b64  	%rd1290, %rd1196, %rd1233;
	and.b64  	%rd1291, %rd1290, %rd1270;
	xor.b64  	%rd1292, %rd1291, %rd1196;
	add.s64 	%rd1293, %rd1289, %rd1159;
	add.s64 	%rd1294, %rd1293, %rd1292;
	add.s64 	%rd1295, %rd1294, %rd1284;
	add.s64 	%rd1296, %rd1295, -9096487096722542874;
	and.b64  	%rd1297, %rd1271, %rd1234;
	or.b64  	%rd1298, %rd1271, %rd1234;
	and.b64  	%rd1299, %rd1298, %rd1197;
	or.b64  	%rd1300, %rd1299, %rd1297;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r978,%dummy}, %rd1271;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r979}, %rd1271;
	}
	shf.r.wrap.b32 	%r980, %r979, %r978, 28;
	shf.r.wrap.b32 	%r981, %r978, %r979, 28;
	mov.b64 	%rd1301, {%r981, %r980};
	shf.l.wrap.b32 	%r982, %r978, %r979, 30;
	shf.l.wrap.b32 	%r983, %r979, %r978, 30;
	mov.b64 	%rd1302, {%r983, %r982};
	xor.b64  	%rd1303, %rd1301, %rd1302;
	shf.l.wrap.b32 	%r984, %r978, %r979, 25;
	shf.l.wrap.b32 	%r985, %r979, %r978, 25;
	mov.b64 	%rd1304, {%r985, %r984};
	xor.b64  	%rd1305, %rd1303, %rd1304;
	add.s64 	%rd1306, %rd1300, %rd1305;
	add.s64 	%rd1307, %rd1296, %rd1160;
	add.s64 	%rd1308, %rd1306, %rd1296;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r986,%dummy}, %rd1247;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r987}, %rd1247;
	}
	shf.r.wrap.b32 	%r988, %r987, %r986, 19;
	shf.r.wrap.b32 	%r989, %r986, %r987, 19;
	mov.b64 	%rd1309, {%r989, %r988};
	shf.l.wrap.b32 	%r990, %r986, %r987, 3;
	shf.l.wrap.b32 	%r991, %r987, %r986, 3;
	mov.b64 	%rd1310, {%r991, %r990};
	xor.b64  	%rd1311, %rd1309, %rd1310;
	shr.u64 	%rd1312, %rd1247, 6;
	xor.b64  	%rd1313, %rd1311, %rd1312;
	shf.r.wrap.b32 	%r992, %r643, %r642, 1;
	shf.r.wrap.b32 	%r993, %r642, %r643, 1;
	mov.b64 	%rd1314, {%r993, %r992};
	shf.r.wrap.b32 	%r994, %r643, %r642, 8;
	shf.r.wrap.b32 	%r995, %r642, %r643, 8;
	mov.b64 	%rd1315, {%r995, %r994};
	xor.b64  	%rd1316, %rd1314, %rd1315;
	shr.u64 	%rd1317, %rd766, 7;
	xor.b64  	%rd1318, %rd1316, %rd1317;
	add.s64 	%rd1319, %rd1313, %rd729;
	add.s64 	%rd1320, %rd1319, %rd1062;
	add.s64 	%rd1321, %rd1320, %rd1318;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r996,%dummy}, %rd1307;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r997}, %rd1307;
	}
	shf.r.wrap.b32 	%r998, %r997, %r996, 14;
	shf.r.wrap.b32 	%r999, %r996, %r997, 14;
	mov.b64 	%rd1322, {%r999, %r998};
	shf.r.wrap.b32 	%r1000, %r997, %r996, 18;
	shf.r.wrap.b32 	%r1001, %r996, %r997, 18;
	mov.b64 	%rd1323, {%r1001, %r1000};
	xor.b64  	%rd1324, %rd1322, %rd1323;
	shf.l.wrap.b32 	%r1002, %r996, %r997, 23;
	shf.l.wrap.b32 	%r1003, %r997, %r996, 23;
	mov.b64 	%rd1325, {%r1003, %r1002};
	xor.b64  	%rd1326, %rd1324, %rd1325;
	xor.b64  	%rd1327, %rd1233, %rd1270;
	and.b64  	%rd1328, %rd1327, %rd1307;
	xor.b64  	%rd1329, %rd1328, %rd1233;
	add.s64 	%rd1330, %rd1326, %rd1196;
	add.s64 	%rd1331, %rd1330, %rd1329;
	add.s64 	%rd1332, %rd1331, %rd1321;
	add.s64 	%rd1333, %rd1332, -7894198246740708037;
	and.b64  	%rd1334, %rd1308, %rd1271;
	or.b64  	%rd1335, %rd1308, %rd1271;
	and.b64  	%rd1336, %rd1335, %rd1234;
	or.b64  	%rd1337, %rd1336, %rd1334;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1004,%dummy}, %rd1308;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1005}, %rd1308;
	}
	shf.r.wrap.b32 	%r1006, %r1005, %r1004, 28;
	shf.r.wrap.b32 	%r1007, %r1004, %r1005, 28;
	mov.b64 	%rd1338, {%r1007, %r1006};
	shf.l.wrap.b32 	%r1008, %r1004, %r1005, 30;
	shf.l.wrap.b32 	%r1009, %r1005, %r1004, 30;
	mov.b64 	%rd1339, {%r1009, %r1008};
	xor.b64  	%rd1340, %rd1338, %rd1339;
	shf.l.wrap.b32 	%r1010, %r1004, %r1005, 25;
	shf.l.wrap.b32 	%r1011, %r1005, %r1004, 25;
	mov.b64 	%rd1341, {%r1011, %r1010};
	xor.b64  	%rd1342, %rd1340, %rd1341;
	add.s64 	%rd1343, %rd1337, %rd1342;
	add.s64 	%rd1344, %rd1333, %rd1197;
	add.s64 	%rd1345, %rd1343, %rd1333;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1012,%dummy}, %rd1284;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1013}, %rd1284;
	}
	shf.r.wrap.b32 	%r1014, %r1013, %r1012, 19;
	shf.r.wrap.b32 	%r1015, %r1012, %r1013, 19;
	mov.b64 	%rd1346, {%r1015, %r1014};
	shf.l.wrap.b32 	%r1016, %r1012, %r1013, 3;
	shf.l.wrap.b32 	%r1017, %r1013, %r1012, 3;
	mov.b64 	%rd1347, {%r1017, %r1016};
	xor.b64  	%rd1348, %rd1346, %rd1347;
	shr.u64 	%rd1349, %rd1284, 6;
	xor.b64  	%rd1350, %rd1348, %rd1349;
	shf.r.wrap.b32 	%r1018, %r671, %r670, 1;
	shf.r.wrap.b32 	%r1019, %r670, %r671, 1;
	mov.b64 	%rd1351, {%r1019, %r1018};
	shf.r.wrap.b32 	%r1020, %r671, %r670, 8;
	shf.r.wrap.b32 	%r1021, %r670, %r671, 8;
	mov.b64 	%rd1352, {%r1021, %r1020};
	xor.b64  	%rd1353, %rd1351, %rd1352;
	shr.u64 	%rd1354, %rd803, 7;
	xor.b64  	%rd1355, %rd1353, %rd1354;
	add.s64 	%rd1356, %rd1350, %rd766;
	add.s64 	%rd1357, %rd1356, %rd1099;
	add.s64 	%rd1358, %rd1357, %rd1355;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1022,%dummy}, %rd1344;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1023}, %rd1344;
	}
	shf.r.wrap.b32 	%r1024, %r1023, %r1022, 14;
	shf.r.wrap.b32 	%r1025, %r1022, %r1023, 14;
	mov.b64 	%rd1359, {%r1025, %r1024};
	shf.r.wrap.b32 	%r1026, %r1023, %r1022, 18;
	shf.r.wrap.b32 	%r1027, %r1022, %r1023, 18;
	mov.b64 	%rd1360, {%r1027, %r1026};
	xor.b64  	%rd1361, %rd1359, %rd1360;
	shf.l.wrap.b32 	%r1028, %r1022, %r1023, 23;
	shf.l.wrap.b32 	%r1029, %r1023, %r1022, 23;
	mov.b64 	%rd1362, {%r1029, %r1028};
	xor.b64  	%rd1363, %rd1361, %rd1362;
	xor.b64  	%rd1364, %rd1270, %rd1307;
	and.b64  	%rd1365, %rd1364, %rd1344;
	xor.b64  	%rd1366, %rd1365, %rd1270;
	add.s64 	%rd1367, %rd1363, %rd1233;
	add.s64 	%rd1368, %rd1367, %rd1366;
	add.s64 	%rd1369, %rd1368, %rd1358;
	add.s64 	%rd1370, %rd1369, -6719396339535248540;
	and.b64  	%rd1371, %rd1345, %rd1308;
	or.b64  	%rd1372, %rd1345, %rd1308;
	and.b64  	%rd1373, %rd1372, %rd1271;
	or.b64  	%rd1374, %rd1373, %rd1371;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1030,%dummy}, %rd1345;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1031}, %rd1345;
	}
	shf.r.wrap.b32 	%r1032, %r1031, %r1030, 28;
	shf.r.wrap.b32 	%r1033, %r1030, %r1031, 28;
	mov.b64 	%rd1375, {%r1033, %r1032};
	shf.l.wrap.b32 	%r1034, %r1030, %r1031, 30;
	shf.l.wrap.b32 	%r1035, %r1031, %r1030, 30;
	mov.b64 	%rd1376, {%r1035, %r1034};
	xor.b64  	%rd1377, %rd1375, %rd1376;
	shf.l.wrap.b32 	%r1036, %r1030, %r1031, 25;
	shf.l.wrap.b32 	%r1037, %r1031, %r1030, 25;
	mov.b64 	%rd1378, {%r1037, %r1036};
	xor.b64  	%rd1379, %rd1377, %rd1378;
	add.s64 	%rd1380, %rd1374, %rd1379;
	add.s64 	%rd1381, %rd1370, %rd1234;
	add.s64 	%rd1382, %rd1380, %rd1370;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1038,%dummy}, %rd1321;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1039}, %rd1321;
	}
	shf.r.wrap.b32 	%r1040, %r1039, %r1038, 19;
	shf.r.wrap.b32 	%r1041, %r1038, %r1039, 19;
	mov.b64 	%rd1383, {%r1041, %r1040};
	shf.l.wrap.b32 	%r1042, %r1038, %r1039, 3;
	shf.l.wrap.b32 	%r1043, %r1039, %r1038, 3;
	mov.b64 	%rd1384, {%r1043, %r1042};
	xor.b64  	%rd1385, %rd1383, %rd1384;
	shr.u64 	%rd1386, %rd1321, 6;
	xor.b64  	%rd1387, %rd1385, %rd1386;
	shf.r.wrap.b32 	%r1044, %r699, %r698, 1;
	shf.r.wrap.b32 	%r1045, %r698, %r699, 1;
	mov.b64 	%rd1388, {%r1045, %r1044};
	shf.r.wrap.b32 	%r1046, %r699, %r698, 8;
	shf.r.wrap.b32 	%r1047, %r698, %r699, 8;
	mov.b64 	%rd1389, {%r1047, %r1046};
	xor.b64  	%rd1390, %rd1388, %rd1389;
	shr.u64 	%rd1391, %rd840, 7;
	xor.b64  	%rd1392, %rd1390, %rd1391;
	add.s64 	%rd1393, %rd1387, %rd803;
	add.s64 	%rd1394, %rd1393, %rd1136;
	add.s64 	%rd1395, %rd1394, %rd1392;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1048,%dummy}, %rd1381;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1049}, %rd1381;
	}
	shf.r.wrap.b32 	%r1050, %r1049, %r1048, 14;
	shf.r.wrap.b32 	%r1051, %r1048, %r1049, 14;
	mov.b64 	%rd1396, {%r1051, %r1050};
	shf.r.wrap.b32 	%r1052, %r1049, %r1048, 18;
	shf.r.wrap.b32 	%r1053, %r1048, %r1049, 18;
	mov.b64 	%rd1397, {%r1053, %r1052};
	xor.b64  	%rd1398, %rd1396, %rd1397;
	shf.l.wrap.b32 	%r1054, %r1048, %r1049, 23;
	shf.l.wrap.b32 	%r1055, %r1049, %r1048, 23;
	mov.b64 	%rd1399, {%r1055, %r1054};
	xor.b64  	%rd1400, %rd1398, %rd1399;
	xor.b64  	%rd1401, %rd1307, %rd1344;
	and.b64  	%rd1402, %rd1401, %rd1381;
	xor.b64  	%rd1403, %rd1402, %rd1307;
	add.s64 	%rd1404, %rd1400, %rd1270;
	add.s64 	%rd1405, %rd1404, %rd1403;
	add.s64 	%rd1406, %rd1405, %rd1395;
	add.s64 	%rd1407, %rd1406, -6333637450476146687;
	and.b64  	%rd1408, %rd1382, %rd1345;
	or.b64  	%rd1409, %rd1382, %rd1345;
	and.b64  	%rd1410, %rd1409, %rd1308;
	or.b64  	%rd1411, %rd1410, %rd1408;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1056,%dummy}, %rd1382;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1057}, %rd1382;
	}
	shf.r.wrap.b32 	%r1058, %r1057, %r1056, 28;
	shf.r.wrap.b32 	%r1059, %r1056, %r1057, 28;
	mov.b64 	%rd1412, {%r1059, %r1058};
	shf.l.wrap.b32 	%r1060, %r1056, %r1057, 30;
	shf.l.wrap.b32 	%r1061, %r1057, %r1056, 30;
	mov.b64 	%rd1413, {%r1061, %r1060};
	xor.b64  	%rd1414, %rd1412, %rd1413;
	shf.l.wrap.b32 	%r1062, %r1056, %r1057, 25;
	shf.l.wrap.b32 	%r1063, %r1057, %r1056, 25;
	mov.b64 	%rd1415, {%r1063, %r1062};
	xor.b64  	%rd1416, %rd1414, %rd1415;
	add.s64 	%rd1417, %rd1411, %rd1416;
	add.s64 	%rd1418, %rd1407, %rd1271;
	add.s64 	%rd1419, %rd1417, %rd1407;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1064,%dummy}, %rd1358;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1065}, %rd1358;
	}
	shf.r.wrap.b32 	%r1066, %r1065, %r1064, 19;
	shf.r.wrap.b32 	%r1067, %r1064, %r1065, 19;
	mov.b64 	%rd1420, {%r1067, %r1066};
	shf.l.wrap.b32 	%r1068, %r1064, %r1065, 3;
	shf.l.wrap.b32 	%r1069, %r1065, %r1064, 3;
	mov.b64 	%rd1421, {%r1069, %r1068};
	xor.b64  	%rd1422, %rd1420, %rd1421;
	shr.u64 	%rd1423, %rd1358, 6;
	xor.b64  	%rd1424, %rd1422, %rd1423;
	shf.r.wrap.b32 	%r1070, %r727, %r726, 1;
	shf.r.wrap.b32 	%r1071, %r726, %r727, 1;
	mov.b64 	%rd1425, {%r1071, %r1070};
	shf.r.wrap.b32 	%r1072, %r727, %r726, 8;
	shf.r.wrap.b32 	%r1073, %r726, %r727, 8;
	mov.b64 	%rd1426, {%r1073, %r1072};
	xor.b64  	%rd1427, %rd1425, %rd1426;
	shr.u64 	%rd1428, %rd877, 7;
	xor.b64  	%rd1429, %rd1427, %rd1428;
	add.s64 	%rd1430, %rd1424, %rd840;
	add.s64 	%rd1431, %rd1430, %rd1173;
	add.s64 	%rd1432, %rd1431, %rd1429;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1074,%dummy}, %rd1418;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1075}, %rd1418;
	}
	shf.r.wrap.b32 	%r1076, %r1075, %r1074, 14;
	shf.r.wrap.b32 	%r1077, %r1074, %r1075, 14;
	mov.b64 	%rd1433, {%r1077, %r1076};
	shf.r.wrap.b32 	%r1078, %r1075, %r1074, 18;
	shf.r.wrap.b32 	%r1079, %r1074, %r1075, 18;
	mov.b64 	%rd1434, {%r1079, %r1078};
	xor.b64  	%rd1435, %rd1433, %rd1434;
	shf.l.wrap.b32 	%r1080, %r1074, %r1075, 23;
	shf.l.wrap.b32 	%r1081, %r1075, %r1074, 23;
	mov.b64 	%rd1436, {%r1081, %r1080};
	xor.b64  	%rd1437, %rd1435, %rd1436;
	xor.b64  	%rd1438, %rd1344, %rd1381;
	and.b64  	%rd1439, %rd1438, %rd1418;
	xor.b64  	%rd1440, %rd1439, %rd1344;
	add.s64 	%rd1441, %rd1437, %rd1307;
	add.s64 	%rd1442, %rd1441, %rd1440;
	add.s64 	%rd1443, %rd1442, %rd1432;
	add.s64 	%rd1444, %rd1443, -4446306890439682159;
	and.b64  	%rd1445, %rd1419, %rd1382;
	or.b64  	%rd1446, %rd1419, %rd1382;
	and.b64  	%rd1447, %rd1446, %rd1345;
	or.b64  	%rd1448, %rd1447, %rd1445;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1082,%dummy}, %rd1419;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1083}, %rd1419;
	}
	shf.r.wrap.b32 	%r1084, %r1083, %r1082, 28;
	shf.r.wrap.b32 	%r1085, %r1082, %r1083, 28;
	mov.b64 	%rd1449, {%r1085, %r1084};
	shf.l.wrap.b32 	%r1086, %r1082, %r1083, 30;
	shf.l.wrap.b32 	%r1087, %r1083, %r1082, 30;
	mov.b64 	%rd1450, {%r1087, %r1086};
	xor.b64  	%rd1451, %rd1449, %rd1450;
	shf.l.wrap.b32 	%r1088, %r1082, %r1083, 25;
	shf.l.wrap.b32 	%r1089, %r1083, %r1082, 25;
	mov.b64 	%rd1452, {%r1089, %r1088};
	xor.b64  	%rd1453, %rd1451, %rd1452;
	add.s64 	%rd1454, %rd1448, %rd1453;
	add.s64 	%rd1455, %rd1444, %rd1308;
	add.s64 	%rd1456, %rd1454, %rd1444;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1090,%dummy}, %rd1395;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1091}, %rd1395;
	}
	shf.r.wrap.b32 	%r1092, %r1091, %r1090, 19;
	shf.r.wrap.b32 	%r1093, %r1090, %r1091, 19;
	mov.b64 	%rd1457, {%r1093, %r1092};
	shf.l.wrap.b32 	%r1094, %r1090, %r1091, 3;
	shf.l.wrap.b32 	%r1095, %r1091, %r1090, 3;
	mov.b64 	%rd1458, {%r1095, %r1094};
	xor.b64  	%rd1459, %rd1457, %rd1458;
	shr.u64 	%rd1460, %rd1395, 6;
	xor.b64  	%rd1461, %rd1459, %rd1460;
	shf.r.wrap.b32 	%r1096, %r753, %r752, 1;
	shf.r.wrap.b32 	%r1097, %r752, %r753, 1;
	mov.b64 	%rd1462, {%r1097, %r1096};
	shf.r.wrap.b32 	%r1098, %r753, %r752, 8;
	shf.r.wrap.b32 	%r1099, %r752, %r753, 8;
	mov.b64 	%rd1463, {%r1099, %r1098};
	xor.b64  	%rd1464, %rd1462, %rd1463;
	shr.u64 	%rd1465, %rd914, 7;
	xor.b64  	%rd1466, %rd1464, %rd1465;
	add.s64 	%rd1467, %rd1461, %rd877;
	add.s64 	%rd1468, %rd1467, %rd1210;
	add.s64 	%rd1469, %rd1468, %rd1466;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1100,%dummy}, %rd1455;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1101}, %rd1455;
	}
	shf.r.wrap.b32 	%r1102, %r1101, %r1100, 14;
	shf.r.wrap.b32 	%r1103, %r1100, %r1101, 14;
	mov.b64 	%rd1470, {%r1103, %r1102};
	shf.r.wrap.b32 	%r1104, %r1101, %r1100, 18;
	shf.r.wrap.b32 	%r1105, %r1100, %r1101, 18;
	mov.b64 	%rd1471, {%r1105, %r1104};
	xor.b64  	%rd1472, %rd1470, %rd1471;
	shf.l.wrap.b32 	%r1106, %r1100, %r1101, 23;
	shf.l.wrap.b32 	%r1107, %r1101, %r1100, 23;
	mov.b64 	%rd1473, {%r1107, %r1106};
	xor.b64  	%rd1474, %rd1472, %rd1473;
	xor.b64  	%rd1475, %rd1381, %rd1418;
	and.b64  	%rd1476, %rd1475, %rd1455;
	xor.b64  	%rd1477, %rd1476, %rd1381;
	add.s64 	%rd1478, %rd1474, %rd1344;
	add.s64 	%rd1479, %rd1478, %rd1477;
	add.s64 	%rd1480, %rd1479, %rd1469;
	add.s64 	%rd1481, %rd1480, -4076793802049405392;
	and.b64  	%rd1482, %rd1456, %rd1419;
	or.b64  	%rd1483, %rd1456, %rd1419;
	and.b64  	%rd1484, %rd1483, %rd1382;
	or.b64  	%rd1485, %rd1484, %rd1482;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1108,%dummy}, %rd1456;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1109}, %rd1456;
	}
	shf.r.wrap.b32 	%r1110, %r1109, %r1108, 28;
	shf.r.wrap.b32 	%r1111, %r1108, %r1109, 28;
	mov.b64 	%rd1486, {%r1111, %r1110};
	shf.l.wrap.b32 	%r1112, %r1108, %r1109, 30;
	shf.l.wrap.b32 	%r1113, %r1109, %r1108, 30;
	mov.b64 	%rd1487, {%r1113, %r1112};
	xor.b64  	%rd1488, %rd1486, %rd1487;
	shf.l.wrap.b32 	%r1114, %r1108, %r1109, 25;
	shf.l.wrap.b32 	%r1115, %r1109, %r1108, 25;
	mov.b64 	%rd1489, {%r1115, %r1114};
	xor.b64  	%rd1490, %rd1488, %rd1489;
	add.s64 	%rd1491, %rd1485, %rd1490;
	add.s64 	%rd1492, %rd1481, %rd1345;
	add.s64 	%rd1493, %rd1491, %rd1481;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1116,%dummy}, %rd1432;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1117}, %rd1432;
	}
	shf.r.wrap.b32 	%r1118, %r1117, %r1116, 19;
	shf.r.wrap.b32 	%r1119, %r1116, %r1117, 19;
	mov.b64 	%rd1494, {%r1119, %r1118};
	shf.l.wrap.b32 	%r1120, %r1116, %r1117, 3;
	shf.l.wrap.b32 	%r1121, %r1117, %r1116, 3;
	mov.b64 	%rd1495, {%r1121, %r1120};
	xor.b64  	%rd1496, %rd1494, %rd1495;
	shr.u64 	%rd1497, %rd1432, 6;
	xor.b64  	%rd1498, %rd1496, %rd1497;
	shf.r.wrap.b32 	%r1122, %r779, %r778, 1;
	shf.r.wrap.b32 	%r1123, %r778, %r779, 1;
	mov.b64 	%rd1499, {%r1123, %r1122};
	shf.r.wrap.b32 	%r1124, %r779, %r778, 8;
	shf.r.wrap.b32 	%r1125, %r778, %r779, 8;
	mov.b64 	%rd1500, {%r1125, %r1124};
	xor.b64  	%rd1501, %rd1499, %rd1500;
	shr.u64 	%rd1502, %rd951, 7;
	xor.b64  	%rd1503, %rd1501, %rd1502;
	add.s64 	%rd1504, %rd1498, %rd914;
	add.s64 	%rd1505, %rd1504, %rd1247;
	add.s64 	%rd1506, %rd1505, %rd1503;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1126,%dummy}, %rd1492;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1127}, %rd1492;
	}
	shf.r.wrap.b32 	%r1128, %r1127, %r1126, 14;
	shf.r.wrap.b32 	%r1129, %r1126, %r1127, 14;
	mov.b64 	%rd1507, {%r1129, %r1128};
	shf.r.wrap.b32 	%r1130, %r1127, %r1126, 18;
	shf.r.wrap.b32 	%r1131, %r1126, %r1127, 18;
	mov.b64 	%rd1508, {%r1131, %r1130};
	xor.b64  	%rd1509, %rd1507, %rd1508;
	shf.l.wrap.b32 	%r1132, %r1126, %r1127, 23;
	shf.l.wrap.b32 	%r1133, %r1127, %r1126, 23;
	mov.b64 	%rd1510, {%r1133, %r1132};
	xor.b64  	%rd1511, %rd1509, %rd1510;
	xor.b64  	%rd1512, %rd1418, %rd1455;
	and.b64  	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1418;
	add.s64 	%rd1515, %rd1511, %rd1381;
	add.s64 	%rd1516, %rd1515, %rd1514;
	add.s64 	%rd1517, %rd1516, %rd1506;
	add.s64 	%rd1518, %rd1517, -3345356375505022440;
	and.b64  	%rd1519, %rd1493, %rd1456;
	or.b64  	%rd1520, %rd1493, %rd1456;
	and.b64  	%rd1521, %rd1520, %rd1419;
	or.b64  	%rd1522, %rd1521, %rd1519;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1134,%dummy}, %rd1493;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1135}, %rd1493;
	}
	shf.r.wrap.b32 	%r1136, %r1135, %r1134, 28;
	shf.r.wrap.b32 	%r1137, %r1134, %r1135, 28;
	mov.b64 	%rd1523, {%r1137, %r1136};
	shf.l.wrap.b32 	%r1138, %r1134, %r1135, 30;
	shf.l.wrap.b32 	%r1139, %r1135, %r1134, 30;
	mov.b64 	%rd1524, {%r1139, %r1138};
	xor.b64  	%rd1525, %rd1523, %rd1524;
	shf.l.wrap.b32 	%r1140, %r1134, %r1135, 25;
	shf.l.wrap.b32 	%r1141, %r1135, %r1134, 25;
	mov.b64 	%rd1526, {%r1141, %r1140};
	xor.b64  	%rd1527, %rd1525, %rd1526;
	add.s64 	%rd1528, %rd1522, %rd1527;
	add.s64 	%rd1529, %rd1518, %rd1382;
	add.s64 	%rd1530, %rd1528, %rd1518;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1142,%dummy}, %rd1469;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1143}, %rd1469;
	}
	shf.r.wrap.b32 	%r1144, %r1143, %r1142, 19;
	shf.r.wrap.b32 	%r1145, %r1142, %r1143, 19;
	mov.b64 	%rd1531, {%r1145, %r1144};
	shf.l.wrap.b32 	%r1146, %r1142, %r1143, 3;
	shf.l.wrap.b32 	%r1147, %r1143, %r1142, 3;
	mov.b64 	%rd1532, {%r1147, %r1146};
	xor.b64  	%rd1533, %rd1531, %rd1532;
	shr.u64 	%rd1534, %rd1469, 6;
	xor.b64  	%rd1535, %rd1533, %rd1534;
	shf.r.wrap.b32 	%r1148, %r805, %r804, 1;
	shf.r.wrap.b32 	%r1149, %r804, %r805, 1;
	mov.b64 	%rd1536, {%r1149, %r1148};
	shf.r.wrap.b32 	%r1150, %r805, %r804, 8;
	shf.r.wrap.b32 	%r1151, %r804, %r805, 8;
	mov.b64 	%rd1537, {%r1151, %r1150};
	xor.b64  	%rd1538, %rd1536, %rd1537;
	shr.u64 	%rd1539, %rd988, 7;
	xor.b64  	%rd1540, %rd1538, %rd1539;
	add.s64 	%rd1541, %rd1535, %rd951;
	add.s64 	%rd1542, %rd1541, %rd1284;
	add.s64 	%rd1543, %rd1542, %rd1540;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1152,%dummy}, %rd1529;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1153}, %rd1529;
	}
	shf.r.wrap.b32 	%r1154, %r1153, %r1152, 14;
	shf.r.wrap.b32 	%r1155, %r1152, %r1153, 14;
	mov.b64 	%rd1544, {%r1155, %r1154};
	shf.r.wrap.b32 	%r1156, %r1153, %r1152, 18;
	shf.r.wrap.b32 	%r1157, %r1152, %r1153, 18;
	mov.b64 	%rd1545, {%r1157, %r1156};
	xor.b64  	%rd1546, %rd1544, %rd1545;
	shf.l.wrap.b32 	%r1158, %r1152, %r1153, 23;
	shf.l.wrap.b32 	%r1159, %r1153, %r1152, 23;
	mov.b64 	%rd1547, {%r1159, %r1158};
	xor.b64  	%rd1548, %rd1546, %rd1547;
	xor.b64  	%rd1549, %rd1455, %rd1492;
	and.b64  	%rd1550, %rd1549, %rd1529;
	xor.b64  	%rd1551, %rd1550, %rd1455;
	add.s64 	%rd1552, %rd1548, %rd1418;
	add.s64 	%rd1553, %rd1552, %rd1551;
	add.s64 	%rd1554, %rd1553, %rd1543;
	add.s64 	%rd1555, %rd1554, -2983346525034927856;
	and.b64  	%rd1556, %rd1530, %rd1493;
	or.b64  	%rd1557, %rd1530, %rd1493;
	and.b64  	%rd1558, %rd1557, %rd1456;
	or.b64  	%rd1559, %rd1558, %rd1556;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1160,%dummy}, %rd1530;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1161}, %rd1530;
	}
	shf.r.wrap.b32 	%r1162, %r1161, %r1160, 28;
	shf.r.wrap.b32 	%r1163, %r1160, %r1161, 28;
	mov.b64 	%rd1560, {%r1163, %r1162};
	shf.l.wrap.b32 	%r1164, %r1160, %r1161, 30;
	shf.l.wrap.b32 	%r1165, %r1161, %r1160, 30;
	mov.b64 	%rd1561, {%r1165, %r1164};
	xor.b64  	%rd1562, %rd1560, %rd1561;
	shf.l.wrap.b32 	%r1166, %r1160, %r1161, 25;
	shf.l.wrap.b32 	%r1167, %r1161, %r1160, 25;
	mov.b64 	%rd1563, {%r1167, %r1166};
	xor.b64  	%rd1564, %rd1562, %rd1563;
	add.s64 	%rd1565, %rd1559, %rd1564;
	add.s64 	%rd1566, %rd1555, %rd1419;
	add.s64 	%rd1567, %rd1565, %rd1555;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1168,%dummy}, %rd1506;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1169}, %rd1506;
	}
	shf.r.wrap.b32 	%r1170, %r1169, %r1168, 19;
	shf.r.wrap.b32 	%r1171, %r1168, %r1169, 19;
	mov.b64 	%rd1568, {%r1171, %r1170};
	shf.l.wrap.b32 	%r1172, %r1168, %r1169, 3;
	shf.l.wrap.b32 	%r1173, %r1169, %r1168, 3;
	mov.b64 	%rd1569, {%r1173, %r1172};
	xor.b64  	%rd1570, %rd1568, %rd1569;
	shr.u64 	%rd1571, %rd1506, 6;
	xor.b64  	%rd1572, %rd1570, %rd1571;
	shf.r.wrap.b32 	%r1174, %r831, %r830, 1;
	shf.r.wrap.b32 	%r1175, %r830, %r831, 1;
	mov.b64 	%rd1573, {%r1175, %r1174};
	shf.r.wrap.b32 	%r1176, %r831, %r830, 8;
	shf.r.wrap.b32 	%r1177, %r830, %r831, 8;
	mov.b64 	%rd1574, {%r1177, %r1176};
	xor.b64  	%rd1575, %rd1573, %rd1574;
	shr.u64 	%rd1576, %rd1025, 7;
	xor.b64  	%rd1577, %rd1575, %rd1576;
	add.s64 	%rd1578, %rd1572, %rd988;
	add.s64 	%rd1579, %rd1578, %rd1321;
	add.s64 	%rd1580, %rd1579, %rd1577;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1178,%dummy}, %rd1566;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1179}, %rd1566;
	}
	shf.r.wrap.b32 	%r1180, %r1179, %r1178, 14;
	shf.r.wrap.b32 	%r1181, %r1178, %r1179, 14;
	mov.b64 	%rd1581, {%r1181, %r1180};
	shf.r.wrap.b32 	%r1182, %r1179, %r1178, 18;
	shf.r.wrap.b32 	%r1183, %r1178, %r1179, 18;
	mov.b64 	%rd1582, {%r1183, %r1182};
	xor.b64  	%rd1583, %rd1581, %rd1582;
	shf.l.wrap.b32 	%r1184, %r1178, %r1179, 23;
	shf.l.wrap.b32 	%r1185, %r1179, %r1178, 23;
	mov.b64 	%rd1584, {%r1185, %r1184};
	xor.b64  	%rd1585, %rd1583, %rd1584;
	xor.b64  	%rd1586, %rd1492, %rd1529;
	and.b64  	%rd1587, %rd1586, %rd1566;
	xor.b64  	%rd1588, %rd1587, %rd1492;
	add.s64 	%rd1589, %rd1585, %rd1455;
	add.s64 	%rd1590, %rd1589, %rd1588;
	add.s64 	%rd1591, %rd1590, %rd1580;
	add.s64 	%rd1592, %rd1591, -860691631967231958;
	and.b64  	%rd1593, %rd1567, %rd1530;
	or.b64  	%rd1594, %rd1567, %rd1530;
	and.b64  	%rd1595, %rd1594, %rd1493;
	or.b64  	%rd1596, %rd1595, %rd1593;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1186,%dummy}, %rd1567;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1187}, %rd1567;
	}
	shf.r.wrap.b32 	%r1188, %r1187, %r1186, 28;
	shf.r.wrap.b32 	%r1189, %r1186, %r1187, 28;
	mov.b64 	%rd1597, {%r1189, %r1188};
	shf.l.wrap.b32 	%r1190, %r1186, %r1187, 30;
	shf.l.wrap.b32 	%r1191, %r1187, %r1186, 30;
	mov.b64 	%rd1598, {%r1191, %r1190};
	xor.b64  	%rd1599, %rd1597, %rd1598;
	shf.l.wrap.b32 	%r1192, %r1186, %r1187, 25;
	shf.l.wrap.b32 	%r1193, %r1187, %r1186, 25;
	mov.b64 	%rd1600, {%r1193, %r1192};
	xor.b64  	%rd1601, %rd1599, %rd1600;
	add.s64 	%rd1602, %rd1596, %rd1601;
	add.s64 	%rd1603, %rd1592, %rd1456;
	add.s64 	%rd1604, %rd1602, %rd1592;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1194,%dummy}, %rd1543;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1195}, %rd1543;
	}
	shf.r.wrap.b32 	%r1196, %r1195, %r1194, 19;
	shf.r.wrap.b32 	%r1197, %r1194, %r1195, 19;
	mov.b64 	%rd1605, {%r1197, %r1196};
	shf.l.wrap.b32 	%r1198, %r1194, %r1195, 3;
	shf.l.wrap.b32 	%r1199, %r1195, %r1194, 3;
	mov.b64 	%rd1606, {%r1199, %r1198};
	xor.b64  	%rd1607, %rd1605, %rd1606;
	shr.u64 	%rd1608, %rd1543, 6;
	xor.b64  	%rd1609, %rd1607, %rd1608;
	shf.r.wrap.b32 	%r1200, %r857, %r856, 1;
	shf.r.wrap.b32 	%r1201, %r856, %r857, 1;
	mov.b64 	%rd1610, {%r1201, %r1200};
	shf.r.wrap.b32 	%r1202, %r857, %r856, 8;
	shf.r.wrap.b32 	%r1203, %r856, %r857, 8;
	mov.b64 	%rd1611, {%r1203, %r1202};
	xor.b64  	%rd1612, %rd1610, %rd1611;
	shr.u64 	%rd1613, %rd1062, 7;
	xor.b64  	%rd1614, %rd1612, %rd1613;
	add.s64 	%rd1615, %rd1609, %rd1025;
	add.s64 	%rd1616, %rd1615, %rd1358;
	add.s64 	%rd1617, %rd1616, %rd1614;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1204,%dummy}, %rd1603;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1205}, %rd1603;
	}
	shf.r.wrap.b32 	%r1206, %r1205, %r1204, 14;
	shf.r.wrap.b32 	%r1207, %r1204, %r1205, 14;
	mov.b64 	%rd1618, {%r1207, %r1206};
	shf.r.wrap.b32 	%r1208, %r1205, %r1204, 18;
	shf.r.wrap.b32 	%r1209, %r1204, %r1205, 18;
	mov.b64 	%rd1619, {%r1209, %r1208};
	xor.b64  	%rd1620, %rd1618, %rd1619;
	shf.l.wrap.b32 	%r1210, %r1204, %r1205, 23;
	shf.l.wrap.b32 	%r1211, %r1205, %r1204, 23;
	mov.b64 	%rd1621, {%r1211, %r1210};
	xor.b64  	%rd1622, %rd1620, %rd1621;
	xor.b64  	%rd1623, %rd1529, %rd1566;
	and.b64  	%rd1624, %rd1623, %rd1603;
	xor.b64  	%rd1625, %rd1624, %rd1529;
	add.s64 	%rd1626, %rd1622, %rd1492;
	add.s64 	%rd1627, %rd1626, %rd1625;
	add.s64 	%rd1628, %rd1627, %rd1617;
	add.s64 	%rd1629, %rd1628, 1182934255886127544;
	and.b64  	%rd1630, %rd1604, %rd1567;
	or.b64  	%rd1631, %rd1604, %rd1567;
	and.b64  	%rd1632, %rd1631, %rd1530;
	or.b64  	%rd1633, %rd1632, %rd1630;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1212,%dummy}, %rd1604;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1213}, %rd1604;
	}
	shf.r.wrap.b32 	%r1214, %r1213, %r1212, 28;
	shf.r.wrap.b32 	%r1215, %r1212, %r1213, 28;
	mov.b64 	%rd1634, {%r1215, %r1214};
	shf.l.wrap.b32 	%r1216, %r1212, %r1213, 30;
	shf.l.wrap.b32 	%r1217, %r1213, %r1212, 30;
	mov.b64 	%rd1635, {%r1217, %r1216};
	xor.b64  	%rd1636, %rd1634, %rd1635;
	shf.l.wrap.b32 	%r1218, %r1212, %r1213, 25;
	shf.l.wrap.b32 	%r1219, %r1213, %r1212, 25;
	mov.b64 	%rd1637, {%r1219, %r1218};
	xor.b64  	%rd1638, %rd1636, %rd1637;
	add.s64 	%rd1639, %rd1633, %rd1638;
	add.s64 	%rd1640, %rd1629, %rd1493;
	add.s64 	%rd1641, %rd1639, %rd1629;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1220,%dummy}, %rd1580;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1221}, %rd1580;
	}
	shf.r.wrap.b32 	%r1222, %r1221, %r1220, 19;
	shf.r.wrap.b32 	%r1223, %r1220, %r1221, 19;
	mov.b64 	%rd1642, {%r1223, %r1222};
	shf.l.wrap.b32 	%r1224, %r1220, %r1221, 3;
	shf.l.wrap.b32 	%r1225, %r1221, %r1220, 3;
	mov.b64 	%rd1643, {%r1225, %r1224};
	xor.b64  	%rd1644, %rd1642, %rd1643;
	shr.u64 	%rd1645, %rd1580, 6;
	xor.b64  	%rd1646, %rd1644, %rd1645;
	shf.r.wrap.b32 	%r1226, %r883, %r882, 1;
	shf.r.wrap.b32 	%r1227, %r882, %r883, 1;
	mov.b64 	%rd1647, {%r1227, %r1226};
	shf.r.wrap.b32 	%r1228, %r883, %r882, 8;
	shf.r.wrap.b32 	%r1229, %r882, %r883, 8;
	mov.b64 	%rd1648, {%r1229, %r1228};
	xor.b64  	%rd1649, %rd1647, %rd1648;
	shr.u64 	%rd1650, %rd1099, 7;
	xor.b64  	%rd1651, %rd1649, %rd1650;
	add.s64 	%rd1652, %rd1646, %rd1062;
	add.s64 	%rd1653, %rd1652, %rd1395;
	add.s64 	%rd1654, %rd1653, %rd1651;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1230,%dummy}, %rd1640;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1231}, %rd1640;
	}
	shf.r.wrap.b32 	%r1232, %r1231, %r1230, 14;
	shf.r.wrap.b32 	%r1233, %r1230, %r1231, 14;
	mov.b64 	%rd1655, {%r1233, %r1232};
	shf.r.wrap.b32 	%r1234, %r1231, %r1230, 18;
	shf.r.wrap.b32 	%r1235, %r1230, %r1231, 18;
	mov.b64 	%rd1656, {%r1235, %r1234};
	xor.b64  	%rd1657, %rd1655, %rd1656;
	shf.l.wrap.b32 	%r1236, %r1230, %r1231, 23;
	shf.l.wrap.b32 	%r1237, %r1231, %r1230, 23;
	mov.b64 	%rd1658, {%r1237, %r1236};
	xor.b64  	%rd1659, %rd1657, %rd1658;
	xor.b64  	%rd1660, %rd1566, %rd1603;
	and.b64  	%rd1661, %rd1660, %rd1640;
	xor.b64  	%rd1662, %rd1661, %rd1566;
	add.s64 	%rd1663, %rd1659, %rd1529;
	add.s64 	%rd1664, %rd1663, %rd1662;
	add.s64 	%rd1665, %rd1664, %rd1654;
	add.s64 	%rd1666, %rd1665, 1847814050463011016;
	and.b64  	%rd1667, %rd1641, %rd1604;
	or.b64  	%rd1668, %rd1641, %rd1604;
	and.b64  	%rd1669, %rd1668, %rd1567;
	or.b64  	%rd1670, %rd1669, %rd1667;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1238,%dummy}, %rd1641;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1239}, %rd1641;
	}
	shf.r.wrap.b32 	%r1240, %r1239, %r1238, 28;
	shf.r.wrap.b32 	%r1241, %r1238, %r1239, 28;
	mov.b64 	%rd1671, {%r1241, %r1240};
	shf.l.wrap.b32 	%r1242, %r1238, %r1239, 30;
	shf.l.wrap.b32 	%r1243, %r1239, %r1238, 30;
	mov.b64 	%rd1672, {%r1243, %r1242};
	xor.b64  	%rd1673, %rd1671, %rd1672;
	shf.l.wrap.b32 	%r1244, %r1238, %r1239, 25;
	shf.l.wrap.b32 	%r1245, %r1239, %r1238, 25;
	mov.b64 	%rd1674, {%r1245, %r1244};
	xor.b64  	%rd1675, %rd1673, %rd1674;
	add.s64 	%rd1676, %rd1670, %rd1675;
	add.s64 	%rd1677, %rd1666, %rd1530;
	add.s64 	%rd1678, %rd1676, %rd1666;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1246,%dummy}, %rd1617;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1247}, %rd1617;
	}
	shf.r.wrap.b32 	%r1248, %r1247, %r1246, 19;
	shf.r.wrap.b32 	%r1249, %r1246, %r1247, 19;
	mov.b64 	%rd1679, {%r1249, %r1248};
	shf.l.wrap.b32 	%r1250, %r1246, %r1247, 3;
	shf.l.wrap.b32 	%r1251, %r1247, %r1246, 3;
	mov.b64 	%rd1680, {%r1251, %r1250};
	xor.b64  	%rd1681, %rd1679, %rd1680;
	shr.u64 	%rd1682, %rd1617, 6;
	xor.b64  	%rd1683, %rd1681, %rd1682;
	shf.r.wrap.b32 	%r1252, %r909, %r908, 1;
	shf.r.wrap.b32 	%r1253, %r908, %r909, 1;
	mov.b64 	%rd1684, {%r1253, %r1252};
	shf.r.wrap.b32 	%r1254, %r909, %r908, 8;
	shf.r.wrap.b32 	%r1255, %r908, %r909, 8;
	mov.b64 	%rd1685, {%r1255, %r1254};
	xor.b64  	%rd1686, %rd1684, %rd1685;
	shr.u64 	%rd1687, %rd1136, 7;
	xor.b64  	%rd1688, %rd1686, %rd1687;
	add.s64 	%rd1689, %rd1683, %rd1099;
	add.s64 	%rd1690, %rd1689, %rd1432;
	add.s64 	%rd1691, %rd1690, %rd1688;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1256,%dummy}, %rd1677;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1257}, %rd1677;
	}
	shf.r.wrap.b32 	%r1258, %r1257, %r1256, 14;
	shf.r.wrap.b32 	%r1259, %r1256, %r1257, 14;
	mov.b64 	%rd1692, {%r1259, %r1258};
	shf.r.wrap.b32 	%r1260, %r1257, %r1256, 18;
	shf.r.wrap.b32 	%r1261, %r1256, %r1257, 18;
	mov.b64 	%rd1693, {%r1261, %r1260};
	xor.b64  	%rd1694, %rd1692, %rd1693;
	shf.l.wrap.b32 	%r1262, %r1256, %r1257, 23;
	shf.l.wrap.b32 	%r1263, %r1257, %r1256, 23;
	mov.b64 	%rd1695, {%r1263, %r1262};
	xor.b64  	%rd1696, %rd1694, %rd1695;
	xor.b64  	%rd1697, %rd1603, %rd1640;
	and.b64  	%rd1698, %rd1697, %rd1677;
	xor.b64  	%rd1699, %rd1698, %rd1603;
	add.s64 	%rd1700, %rd1696, %rd1566;
	add.s64 	%rd1701, %rd1700, %rd1699;
	add.s64 	%rd1702, %rd1701, %rd1691;
	add.s64 	%rd1703, %rd1702, 2177327727835720531;
	and.b64  	%rd1704, %rd1678, %rd1641;
	or.b64  	%rd1705, %rd1678, %rd1641;
	and.b64  	%rd1706, %rd1705, %rd1604;
	or.b64  	%rd1707, %rd1706, %rd1704;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1264,%dummy}, %rd1678;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1265}, %rd1678;
	}
	shf.r.wrap.b32 	%r1266, %r1265, %r1264, 28;
	shf.r.wrap.b32 	%r1267, %r1264, %r1265, 28;
	mov.b64 	%rd1708, {%r1267, %r1266};
	shf.l.wrap.b32 	%r1268, %r1264, %r1265, 30;
	shf.l.wrap.b32 	%r1269, %r1265, %r1264, 30;
	mov.b64 	%rd1709, {%r1269, %r1268};
	xor.b64  	%rd1710, %rd1708, %rd1709;
	shf.l.wrap.b32 	%r1270, %r1264, %r1265, 25;
	shf.l.wrap.b32 	%r1271, %r1265, %r1264, 25;
	mov.b64 	%rd1711, {%r1271, %r1270};
	xor.b64  	%rd1712, %rd1710, %rd1711;
	add.s64 	%rd1713, %rd1707, %rd1712;
	add.s64 	%rd1714, %rd1703, %rd1567;
	add.s64 	%rd1715, %rd1713, %rd1703;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1272,%dummy}, %rd1654;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1273}, %rd1654;
	}
	shf.r.wrap.b32 	%r1274, %r1273, %r1272, 19;
	shf.r.wrap.b32 	%r1275, %r1272, %r1273, 19;
	mov.b64 	%rd1716, {%r1275, %r1274};
	shf.l.wrap.b32 	%r1276, %r1272, %r1273, 3;
	shf.l.wrap.b32 	%r1277, %r1273, %r1272, 3;
	mov.b64 	%rd1717, {%r1277, %r1276};
	xor.b64  	%rd1718, %rd1716, %rd1717;
	shr.u64 	%rd1719, %rd1654, 6;
	xor.b64  	%rd1720, %rd1718, %rd1719;
	shf.r.wrap.b32 	%r1278, %r935, %r934, 1;
	shf.r.wrap.b32 	%r1279, %r934, %r935, 1;
	mov.b64 	%rd1721, {%r1279, %r1278};
	shf.r.wrap.b32 	%r1280, %r935, %r934, 8;
	shf.r.wrap.b32 	%r1281, %r934, %r935, 8;
	mov.b64 	%rd1722, {%r1281, %r1280};
	xor.b64  	%rd1723, %rd1721, %rd1722;
	shr.u64 	%rd1724, %rd1173, 7;
	xor.b64  	%rd1725, %rd1723, %rd1724;
	add.s64 	%rd1726, %rd1720, %rd1136;
	add.s64 	%rd1727, %rd1726, %rd1469;
	add.s64 	%rd1728, %rd1727, %rd1725;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1282,%dummy}, %rd1714;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1283}, %rd1714;
	}
	shf.r.wrap.b32 	%r1284, %r1283, %r1282, 14;
	shf.r.wrap.b32 	%r1285, %r1282, %r1283, 14;
	mov.b64 	%rd1729, {%r1285, %r1284};
	shf.r.wrap.b32 	%r1286, %r1283, %r1282, 18;
	shf.r.wrap.b32 	%r1287, %r1282, %r1283, 18;
	mov.b64 	%rd1730, {%r1287, %r1286};
	xor.b64  	%rd1731, %rd1729, %rd1730;
	shf.l.wrap.b32 	%r1288, %r1282, %r1283, 23;
	shf.l.wrap.b32 	%r1289, %r1283, %r1282, 23;
	mov.b64 	%rd1732, {%r1289, %r1288};
	xor.b64  	%rd1733, %rd1731, %rd1732;
	xor.b64  	%rd1734, %rd1640, %rd1677;
	and.b64  	%rd1735, %rd1734, %rd1714;
	xor.b64  	%rd1736, %rd1735, %rd1640;
	add.s64 	%rd1737, %rd1733, %rd1603;
	add.s64 	%rd1738, %rd1737, %rd1736;
	add.s64 	%rd1739, %rd1738, %rd1728;
	add.s64 	%rd1740, %rd1739, 2830643537854262169;
	and.b64  	%rd1741, %rd1715, %rd1678;
	or.b64  	%rd1742, %rd1715, %rd1678;
	and.b64  	%rd1743, %rd1742, %rd1641;
	or.b64  	%rd1744, %rd1743, %rd1741;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1290,%dummy}, %rd1715;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1291}, %rd1715;
	}
	shf.r.wrap.b32 	%r1292, %r1291, %r1290, 28;
	shf.r.wrap.b32 	%r1293, %r1290, %r1291, 28;
	mov.b64 	%rd1745, {%r1293, %r1292};
	shf.l.wrap.b32 	%r1294, %r1290, %r1291, 30;
	shf.l.wrap.b32 	%r1295, %r1291, %r1290, 30;
	mov.b64 	%rd1746, {%r1295, %r1294};
	xor.b64  	%rd1747, %rd1745, %rd1746;
	shf.l.wrap.b32 	%r1296, %r1290, %r1291, 25;
	shf.l.wrap.b32 	%r1297, %r1291, %r1290, 25;
	mov.b64 	%rd1748, {%r1297, %r1296};
	xor.b64  	%rd1749, %rd1747, %rd1748;
	add.s64 	%rd1750, %rd1744, %rd1749;
	add.s64 	%rd1751, %rd1740, %rd1604;
	add.s64 	%rd1752, %rd1750, %rd1740;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1298,%dummy}, %rd1691;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1299}, %rd1691;
	}
	shf.r.wrap.b32 	%r1300, %r1299, %r1298, 19;
	shf.r.wrap.b32 	%r1301, %r1298, %r1299, 19;
	mov.b64 	%rd1753, {%r1301, %r1300};
	shf.l.wrap.b32 	%r1302, %r1298, %r1299, 3;
	shf.l.wrap.b32 	%r1303, %r1299, %r1298, 3;
	mov.b64 	%rd1754, {%r1303, %r1302};
	xor.b64  	%rd1755, %rd1753, %rd1754;
	shr.u64 	%rd1756, %rd1691, 6;
	xor.b64  	%rd1757, %rd1755, %rd1756;
	shf.r.wrap.b32 	%r1304, %r961, %r960, 1;
	shf.r.wrap.b32 	%r1305, %r960, %r961, 1;
	mov.b64 	%rd1758, {%r1305, %r1304};
	shf.r.wrap.b32 	%r1306, %r961, %r960, 8;
	shf.r.wrap.b32 	%r1307, %r960, %r961, 8;
	mov.b64 	%rd1759, {%r1307, %r1306};
	xor.b64  	%rd1760, %rd1758, %rd1759;
	shr.u64 	%rd1761, %rd1210, 7;
	xor.b64  	%rd1762, %rd1760, %rd1761;
	add.s64 	%rd1763, %rd1757, %rd1173;
	add.s64 	%rd1764, %rd1763, %rd1506;
	add.s64 	%rd1765, %rd1764, %rd1762;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1308,%dummy}, %rd1751;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1309}, %rd1751;
	}
	shf.r.wrap.b32 	%r1310, %r1309, %r1308, 14;
	shf.r.wrap.b32 	%r1311, %r1308, %r1309, 14;
	mov.b64 	%rd1766, {%r1311, %r1310};
	shf.r.wrap.b32 	%r1312, %r1309, %r1308, 18;
	shf.r.wrap.b32 	%r1313, %r1308, %r1309, 18;
	mov.b64 	%rd1767, {%r1313, %r1312};
	xor.b64  	%rd1768, %rd1766, %rd1767;
	shf.l.wrap.b32 	%r1314, %r1308, %r1309, 23;
	shf.l.wrap.b32 	%r1315, %r1309, %r1308, 23;
	mov.b64 	%rd1769, {%r1315, %r1314};
	xor.b64  	%rd1770, %rd1768, %rd1769;
	xor.b64  	%rd1771, %rd1677, %rd1714;
	and.b64  	%rd1772, %rd1771, %rd1751;
	xor.b64  	%rd1773, %rd1772, %rd1677;
	add.s64 	%rd1774, %rd1770, %rd1640;
	add.s64 	%rd1775, %rd1774, %rd1773;
	add.s64 	%rd1776, %rd1775, %rd1765;
	add.s64 	%rd1777, %rd1776, 3796741975233480872;
	and.b64  	%rd1778, %rd1752, %rd1715;
	or.b64  	%rd1779, %rd1752, %rd1715;
	and.b64  	%rd1780, %rd1779, %rd1678;
	or.b64  	%rd1781, %rd1780, %rd1778;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1316,%dummy}, %rd1752;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1317}, %rd1752;
	}
	shf.r.wrap.b32 	%r1318, %r1317, %r1316, 28;
	shf.r.wrap.b32 	%r1319, %r1316, %r1317, 28;
	mov.b64 	%rd1782, {%r1319, %r1318};
	shf.l.wrap.b32 	%r1320, %r1316, %r1317, 30;
	shf.l.wrap.b32 	%r1321, %r1317, %r1316, 30;
	mov.b64 	%rd1783, {%r1321, %r1320};
	xor.b64  	%rd1784, %rd1782, %rd1783;
	shf.l.wrap.b32 	%r1322, %r1316, %r1317, 25;
	shf.l.wrap.b32 	%r1323, %r1317, %r1316, 25;
	mov.b64 	%rd1785, {%r1323, %r1322};
	xor.b64  	%rd1786, %rd1784, %rd1785;
	add.s64 	%rd1787, %rd1781, %rd1786;
	add.s64 	%rd1788, %rd1777, %rd1641;
	add.s64 	%rd1789, %rd1787, %rd1777;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1324,%dummy}, %rd1728;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1325}, %rd1728;
	}
	shf.r.wrap.b32 	%r1326, %r1325, %r1324, 19;
	shf.r.wrap.b32 	%r1327, %r1324, %r1325, 19;
	mov.b64 	%rd1790, {%r1327, %r1326};
	shf.l.wrap.b32 	%r1328, %r1324, %r1325, 3;
	shf.l.wrap.b32 	%r1329, %r1325, %r1324, 3;
	mov.b64 	%rd1791, {%r1329, %r1328};
	xor.b64  	%rd1792, %rd1790, %rd1791;
	shr.u64 	%rd1793, %rd1728, 6;
	xor.b64  	%rd1794, %rd1792, %rd1793;
	shf.r.wrap.b32 	%r1330, %r987, %r986, 1;
	shf.r.wrap.b32 	%r1331, %r986, %r987, 1;
	mov.b64 	%rd1795, {%r1331, %r1330};
	shf.r.wrap.b32 	%r1332, %r987, %r986, 8;
	shf.r.wrap.b32 	%r1333, %r986, %r987, 8;
	mov.b64 	%rd1796, {%r1333, %r1332};
	xor.b64  	%rd1797, %rd1795, %rd1796;
	shr.u64 	%rd1798, %rd1247, 7;
	xor.b64  	%rd1799, %rd1797, %rd1798;
	add.s64 	%rd1800, %rd1794, %rd1210;
	add.s64 	%rd1801, %rd1800, %rd1543;
	add.s64 	%rd1802, %rd1801, %rd1799;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1334,%dummy}, %rd1788;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1335}, %rd1788;
	}
	shf.r.wrap.b32 	%r1336, %r1335, %r1334, 14;
	shf.r.wrap.b32 	%r1337, %r1334, %r1335, 14;
	mov.b64 	%rd1803, {%r1337, %r1336};
	shf.r.wrap.b32 	%r1338, %r1335, %r1334, 18;
	shf.r.wrap.b32 	%r1339, %r1334, %r1335, 18;
	mov.b64 	%rd1804, {%r1339, %r1338};
	xor.b64  	%rd1805, %rd1803, %rd1804;
	shf.l.wrap.b32 	%r1340, %r1334, %r1335, 23;
	shf.l.wrap.b32 	%r1341, %r1335, %r1334, 23;
	mov.b64 	%rd1806, {%r1341, %r1340};
	xor.b64  	%rd1807, %rd1805, %rd1806;
	xor.b64  	%rd1808, %rd1714, %rd1751;
	and.b64  	%rd1809, %rd1808, %rd1788;
	xor.b64  	%rd1810, %rd1809, %rd1714;
	add.s64 	%rd1811, %rd1807, %rd1677;
	add.s64 	%rd1812, %rd1811, %rd1810;
	add.s64 	%rd1813, %rd1812, %rd1802;
	add.s64 	%rd1814, %rd1813, 4115178125766777443;
	and.b64  	%rd1815, %rd1789, %rd1752;
	or.b64  	%rd1816, %rd1789, %rd1752;
	and.b64  	%rd1817, %rd1816, %rd1715;
	or.b64  	%rd1818, %rd1817, %rd1815;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1342,%dummy}, %rd1789;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1343}, %rd1789;
	}
	shf.r.wrap.b32 	%r1344, %r1343, %r1342, 28;
	shf.r.wrap.b32 	%r1345, %r1342, %r1343, 28;
	mov.b64 	%rd1819, {%r1345, %r1344};
	shf.l.wrap.b32 	%r1346, %r1342, %r1343, 30;
	shf.l.wrap.b32 	%r1347, %r1343, %r1342, 30;
	mov.b64 	%rd1820, {%r1347, %r1346};
	xor.b64  	%rd1821, %rd1819, %rd1820;
	shf.l.wrap.b32 	%r1348, %r1342, %r1343, 25;
	shf.l.wrap.b32 	%r1349, %r1343, %r1342, 25;
	mov.b64 	%rd1822, {%r1349, %r1348};
	xor.b64  	%rd1823, %rd1821, %rd1822;
	add.s64 	%rd1824, %rd1818, %rd1823;
	add.s64 	%rd1825, %rd1814, %rd1678;
	add.s64 	%rd1826, %rd1824, %rd1814;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1350,%dummy}, %rd1765;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1351}, %rd1765;
	}
	shf.r.wrap.b32 	%r1352, %r1351, %r1350, 19;
	shf.r.wrap.b32 	%r1353, %r1350, %r1351, 19;
	mov.b64 	%rd1827, {%r1353, %r1352};
	shf.l.wrap.b32 	%r1354, %r1350, %r1351, 3;
	shf.l.wrap.b32 	%r1355, %r1351, %r1350, 3;
	mov.b64 	%rd1828, {%r1355, %r1354};
	xor.b64  	%rd1829, %rd1827, %rd1828;
	shr.u64 	%rd1830, %rd1765, 6;
	xor.b64  	%rd1831, %rd1829, %rd1830;
	shf.r.wrap.b32 	%r1356, %r1013, %r1012, 1;
	shf.r.wrap.b32 	%r1357, %r1012, %r1013, 1;
	mov.b64 	%rd1832, {%r1357, %r1356};
	shf.r.wrap.b32 	%r1358, %r1013, %r1012, 8;
	shf.r.wrap.b32 	%r1359, %r1012, %r1013, 8;
	mov.b64 	%rd1833, {%r1359, %r1358};
	xor.b64  	%rd1834, %rd1832, %rd1833;
	shr.u64 	%rd1835, %rd1284, 7;
	xor.b64  	%rd1836, %rd1834, %rd1835;
	add.s64 	%rd1837, %rd1831, %rd1247;
	add.s64 	%rd1838, %rd1837, %rd1580;
	add.s64 	%rd1839, %rd1838, %rd1836;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1360,%dummy}, %rd1825;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1361}, %rd1825;
	}
	shf.r.wrap.b32 	%r1362, %r1361, %r1360, 14;
	shf.r.wrap.b32 	%r1363, %r1360, %r1361, 14;
	mov.b64 	%rd1840, {%r1363, %r1362};
	shf.r.wrap.b32 	%r1364, %r1361, %r1360, 18;
	shf.r.wrap.b32 	%r1365, %r1360, %r1361, 18;
	mov.b64 	%rd1841, {%r1365, %r1364};
	xor.b64  	%rd1842, %rd1840, %rd1841;
	shf.l.wrap.b32 	%r1366, %r1360, %r1361, 23;
	shf.l.wrap.b32 	%r1367, %r1361, %r1360, 23;
	mov.b64 	%rd1843, {%r1367, %r1366};
	xor.b64  	%rd1844, %rd1842, %rd1843;
	xor.b64  	%rd1845, %rd1751, %rd1788;
	and.b64  	%rd1846, %rd1845, %rd1825;
	xor.b64  	%rd1847, %rd1846, %rd1751;
	add.s64 	%rd1848, %rd1844, %rd1714;
	add.s64 	%rd1849, %rd1848, %rd1847;
	add.s64 	%rd1850, %rd1849, %rd1839;
	add.s64 	%rd1851, %rd1850, 5681478168544905931;
	and.b64  	%rd1852, %rd1826, %rd1789;
	or.b64  	%rd1853, %rd1826, %rd1789;
	and.b64  	%rd1854, %rd1853, %rd1752;
	or.b64  	%rd1855, %rd1854, %rd1852;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1368,%dummy}, %rd1826;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1369}, %rd1826;
	}
	shf.r.wrap.b32 	%r1370, %r1369, %r1368, 28;
	shf.r.wrap.b32 	%r1371, %r1368, %r1369, 28;
	mov.b64 	%rd1856, {%r1371, %r1370};
	shf.l.wrap.b32 	%r1372, %r1368, %r1369, 30;
	shf.l.wrap.b32 	%r1373, %r1369, %r1368, 30;
	mov.b64 	%rd1857, {%r1373, %r1372};
	xor.b64  	%rd1858, %rd1856, %rd1857;
	shf.l.wrap.b32 	%r1374, %r1368, %r1369, 25;
	shf.l.wrap.b32 	%r1375, %r1369, %r1368, 25;
	mov.b64 	%rd1859, {%r1375, %r1374};
	xor.b64  	%rd1860, %rd1858, %rd1859;
	add.s64 	%rd1861, %rd1855, %rd1860;
	add.s64 	%rd1862, %rd1851, %rd1715;
	add.s64 	%rd1863, %rd1861, %rd1851;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1376,%dummy}, %rd1802;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1377}, %rd1802;
	}
	shf.r.wrap.b32 	%r1378, %r1377, %r1376, 19;
	shf.r.wrap.b32 	%r1379, %r1376, %r1377, 19;
	mov.b64 	%rd1864, {%r1379, %r1378};
	shf.l.wrap.b32 	%r1380, %r1376, %r1377, 3;
	shf.l.wrap.b32 	%r1381, %r1377, %r1376, 3;
	mov.b64 	%rd1865, {%r1381, %r1380};
	xor.b64  	%rd1866, %rd1864, %rd1865;
	shr.u64 	%rd1867, %rd1802, 6;
	xor.b64  	%rd1868, %rd1866, %rd1867;
	shf.r.wrap.b32 	%r1382, %r1039, %r1038, 1;
	shf.r.wrap.b32 	%r1383, %r1038, %r1039, 1;
	mov.b64 	%rd1869, {%r1383, %r1382};
	shf.r.wrap.b32 	%r1384, %r1039, %r1038, 8;
	shf.r.wrap.b32 	%r1385, %r1038, %r1039, 8;
	mov.b64 	%rd1870, {%r1385, %r1384};
	xor.b64  	%rd1871, %rd1869, %rd1870;
	shr.u64 	%rd1872, %rd1321, 7;
	xor.b64  	%rd1873, %rd1871, %rd1872;
	add.s64 	%rd1874, %rd1868, %rd1284;
	add.s64 	%rd1875, %rd1874, %rd1617;
	add.s64 	%rd1876, %rd1875, %rd1873;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1386,%dummy}, %rd1862;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1387}, %rd1862;
	}
	shf.r.wrap.b32 	%r1388, %r1387, %r1386, 14;
	shf.r.wrap.b32 	%r1389, %r1386, %r1387, 14;
	mov.b64 	%rd1877, {%r1389, %r1388};
	shf.r.wrap.b32 	%r1390, %r1387, %r1386, 18;
	shf.r.wrap.b32 	%r1391, %r1386, %r1387, 18;
	mov.b64 	%rd1878, {%r1391, %r1390};
	xor.b64  	%rd1879, %rd1877, %rd1878;
	shf.l.wrap.b32 	%r1392, %r1386, %r1387, 23;
	shf.l.wrap.b32 	%r1393, %r1387, %r1386, 23;
	mov.b64 	%rd1880, {%r1393, %r1392};
	xor.b64  	%rd1881, %rd1879, %rd1880;
	xor.b64  	%rd1882, %rd1788, %rd1825;
	and.b64  	%rd1883, %rd1882, %rd1862;
	xor.b64  	%rd1884, %rd1883, %rd1788;
	add.s64 	%rd1885, %rd1881, %rd1751;
	add.s64 	%rd1886, %rd1885, %rd1884;
	add.s64 	%rd1887, %rd1886, %rd1876;
	add.s64 	%rd1888, %rd1887, 6601373596472566643;
	and.b64  	%rd1889, %rd1863, %rd1826;
	or.b64  	%rd1890, %rd1863, %rd1826;
	and.b64  	%rd1891, %rd1890, %rd1789;
	or.b64  	%rd1892, %rd1891, %rd1889;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1394,%dummy}, %rd1863;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1395}, %rd1863;
	}
	shf.r.wrap.b32 	%r1396, %r1395, %r1394, 28;
	shf.r.wrap.b32 	%r1397, %r1394, %r1395, 28;
	mov.b64 	%rd1893, {%r1397, %r1396};
	shf.l.wrap.b32 	%r1398, %r1394, %r1395, 30;
	shf.l.wrap.b32 	%r1399, %r1395, %r1394, 30;
	mov.b64 	%rd1894, {%r1399, %r1398};
	xor.b64  	%rd1895, %rd1893, %rd1894;
	shf.l.wrap.b32 	%r1400, %r1394, %r1395, 25;
	shf.l.wrap.b32 	%r1401, %r1395, %r1394, 25;
	mov.b64 	%rd1896, {%r1401, %r1400};
	xor.b64  	%rd1897, %rd1895, %rd1896;
	add.s64 	%rd1898, %rd1892, %rd1897;
	add.s64 	%rd1899, %rd1888, %rd1752;
	add.s64 	%rd1900, %rd1898, %rd1888;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1402,%dummy}, %rd1839;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1403}, %rd1839;
	}
	shf.r.wrap.b32 	%r1404, %r1403, %r1402, 19;
	shf.r.wrap.b32 	%r1405, %r1402, %r1403, 19;
	mov.b64 	%rd1901, {%r1405, %r1404};
	shf.l.wrap.b32 	%r1406, %r1402, %r1403, 3;
	shf.l.wrap.b32 	%r1407, %r1403, %r1402, 3;
	mov.b64 	%rd1902, {%r1407, %r1406};
	xor.b64  	%rd1903, %rd1901, %rd1902;
	shr.u64 	%rd1904, %rd1839, 6;
	xor.b64  	%rd1905, %rd1903, %rd1904;
	shf.r.wrap.b32 	%r1408, %r1065, %r1064, 1;
	shf.r.wrap.b32 	%r1409, %r1064, %r1065, 1;
	mov.b64 	%rd1906, {%r1409, %r1408};
	shf.r.wrap.b32 	%r1410, %r1065, %r1064, 8;
	shf.r.wrap.b32 	%r1411, %r1064, %r1065, 8;
	mov.b64 	%rd1907, {%r1411, %r1410};
	xor.b64  	%rd1908, %rd1906, %rd1907;
	shr.u64 	%rd1909, %rd1358, 7;
	xor.b64  	%rd1910, %rd1908, %rd1909;
	add.s64 	%rd1911, %rd1905, %rd1321;
	add.s64 	%rd1912, %rd1911, %rd1654;
	add.s64 	%rd1913, %rd1912, %rd1910;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1412,%dummy}, %rd1899;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1413}, %rd1899;
	}
	shf.r.wrap.b32 	%r1414, %r1413, %r1412, 14;
	shf.r.wrap.b32 	%r1415, %r1412, %r1413, 14;
	mov.b64 	%rd1914, {%r1415, %r1414};
	shf.r.wrap.b32 	%r1416, %r1413, %r1412, 18;
	shf.r.wrap.b32 	%r1417, %r1412, %r1413, 18;
	mov.b64 	%rd1915, {%r1417, %r1416};
	xor.b64  	%rd1916, %rd1914, %rd1915;
	shf.l.wrap.b32 	%r1418, %r1412, %r1413, 23;
	shf.l.wrap.b32 	%r1419, %r1413, %r1412, 23;
	mov.b64 	%rd1917, {%r1419, %r1418};
	xor.b64  	%rd1918, %rd1916, %rd1917;
	xor.b64  	%rd1919, %rd1825, %rd1862;
	and.b64  	%rd1920, %rd1919, %rd1899;
	xor.b64  	%rd1921, %rd1920, %rd1825;
	add.s64 	%rd1922, %rd1918, %rd1788;
	add.s64 	%rd1923, %rd1922, %rd1921;
	add.s64 	%rd1924, %rd1923, %rd1913;
	add.s64 	%rd1925, %rd1924, 7507060721942968483;
	and.b64  	%rd1926, %rd1900, %rd1863;
	or.b64  	%rd1927, %rd1900, %rd1863;
	and.b64  	%rd1928, %rd1927, %rd1826;
	or.b64  	%rd1929, %rd1928, %rd1926;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1420,%dummy}, %rd1900;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1421}, %rd1900;
	}
	shf.r.wrap.b32 	%r1422, %r1421, %r1420, 28;
	shf.r.wrap.b32 	%r1423, %r1420, %r1421, 28;
	mov.b64 	%rd1930, {%r1423, %r1422};
	shf.l.wrap.b32 	%r1424, %r1420, %r1421, 30;
	shf.l.wrap.b32 	%r1425, %r1421, %r1420, 30;
	mov.b64 	%rd1931, {%r1425, %r1424};
	xor.b64  	%rd1932, %rd1930, %rd1931;
	shf.l.wrap.b32 	%r1426, %r1420, %r1421, 25;
	shf.l.wrap.b32 	%r1427, %r1421, %r1420, 25;
	mov.b64 	%rd1933, {%r1427, %r1426};
	xor.b64  	%rd1934, %rd1932, %rd1933;
	add.s64 	%rd1935, %rd1929, %rd1934;
	add.s64 	%rd1936, %rd1925, %rd1789;
	add.s64 	%rd1937, %rd1935, %rd1925;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1428,%dummy}, %rd1876;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1429}, %rd1876;
	}
	shf.r.wrap.b32 	%r1430, %r1429, %r1428, 19;
	shf.r.wrap.b32 	%r1431, %r1428, %r1429, 19;
	mov.b64 	%rd1938, {%r1431, %r1430};
	shf.l.wrap.b32 	%r1432, %r1428, %r1429, 3;
	shf.l.wrap.b32 	%r1433, %r1429, %r1428, 3;
	mov.b64 	%rd1939, {%r1433, %r1432};
	xor.b64  	%rd1940, %rd1938, %rd1939;
	shr.u64 	%rd1941, %rd1876, 6;
	xor.b64  	%rd1942, %rd1940, %rd1941;
	shf.r.wrap.b32 	%r1434, %r1091, %r1090, 1;
	shf.r.wrap.b32 	%r1435, %r1090, %r1091, 1;
	mov.b64 	%rd1943, {%r1435, %r1434};
	shf.r.wrap.b32 	%r1436, %r1091, %r1090, 8;
	shf.r.wrap.b32 	%r1437, %r1090, %r1091, 8;
	mov.b64 	%rd1944, {%r1437, %r1436};
	xor.b64  	%rd1945, %rd1943, %rd1944;
	shr.u64 	%rd1946, %rd1395, 7;
	xor.b64  	%rd1947, %rd1945, %rd1946;
	add.s64 	%rd1948, %rd1942, %rd1358;
	add.s64 	%rd1949, %rd1948, %rd1691;
	add.s64 	%rd1950, %rd1949, %rd1947;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1438,%dummy}, %rd1936;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1439}, %rd1936;
	}
	shf.r.wrap.b32 	%r1440, %r1439, %r1438, 14;
	shf.r.wrap.b32 	%r1441, %r1438, %r1439, 14;
	mov.b64 	%rd1951, {%r1441, %r1440};
	shf.r.wrap.b32 	%r1442, %r1439, %r1438, 18;
	shf.r.wrap.b32 	%r1443, %r1438, %r1439, 18;
	mov.b64 	%rd1952, {%r1443, %r1442};
	xor.b64  	%rd1953, %rd1951, %rd1952;
	shf.l.wrap.b32 	%r1444, %r1438, %r1439, 23;
	shf.l.wrap.b32 	%r1445, %r1439, %r1438, 23;
	mov.b64 	%rd1954, {%r1445, %r1444};
	xor.b64  	%rd1955, %rd1953, %rd1954;
	xor.b64  	%rd1956, %rd1862, %rd1899;
	and.b64  	%rd1957, %rd1956, %rd1936;
	xor.b64  	%rd1958, %rd1957, %rd1862;
	add.s64 	%rd1959, %rd1955, %rd1825;
	add.s64 	%rd1960, %rd1959, %rd1958;
	add.s64 	%rd1961, %rd1960, %rd1950;
	add.s64 	%rd1962, %rd1961, 8399075790359081724;
	and.b64  	%rd1963, %rd1937, %rd1900;
	or.b64  	%rd1964, %rd1937, %rd1900;
	and.b64  	%rd1965, %rd1964, %rd1863;
	or.b64  	%rd1966, %rd1965, %rd1963;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1446,%dummy}, %rd1937;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1447}, %rd1937;
	}
	shf.r.wrap.b32 	%r1448, %r1447, %r1446, 28;
	shf.r.wrap.b32 	%r1449, %r1446, %r1447, 28;
	mov.b64 	%rd1967, {%r1449, %r1448};
	shf.l.wrap.b32 	%r1450, %r1446, %r1447, 30;
	shf.l.wrap.b32 	%r1451, %r1447, %r1446, 30;
	mov.b64 	%rd1968, {%r1451, %r1450};
	xor.b64  	%rd1969, %rd1967, %rd1968;
	shf.l.wrap.b32 	%r1452, %r1446, %r1447, 25;
	shf.l.wrap.b32 	%r1453, %r1447, %r1446, 25;
	mov.b64 	%rd1970, {%r1453, %r1452};
	xor.b64  	%rd1971, %rd1969, %rd1970;
	add.s64 	%rd1972, %rd1966, %rd1971;
	add.s64 	%rd1973, %rd1962, %rd1826;
	add.s64 	%rd1974, %rd1972, %rd1962;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1454,%dummy}, %rd1913;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1455}, %rd1913;
	}
	shf.r.wrap.b32 	%r1456, %r1455, %r1454, 19;
	shf.r.wrap.b32 	%r1457, %r1454, %r1455, 19;
	mov.b64 	%rd1975, {%r1457, %r1456};
	shf.l.wrap.b32 	%r1458, %r1454, %r1455, 3;
	shf.l.wrap.b32 	%r1459, %r1455, %r1454, 3;
	mov.b64 	%rd1976, {%r1459, %r1458};
	xor.b64  	%rd1977, %rd1975, %rd1976;
	shr.u64 	%rd1978, %rd1913, 6;
	xor.b64  	%rd1979, %rd1977, %rd1978;
	shf.r.wrap.b32 	%r1460, %r1117, %r1116, 1;
	shf.r.wrap.b32 	%r1461, %r1116, %r1117, 1;
	mov.b64 	%rd1980, {%r1461, %r1460};
	shf.r.wrap.b32 	%r1462, %r1117, %r1116, 8;
	shf.r.wrap.b32 	%r1463, %r1116, %r1117, 8;
	mov.b64 	%rd1981, {%r1463, %r1462};
	xor.b64  	%rd1982, %rd1980, %rd1981;
	shr.u64 	%rd1983, %rd1432, 7;
	xor.b64  	%rd1984, %rd1982, %rd1983;
	add.s64 	%rd1985, %rd1979, %rd1395;
	add.s64 	%rd1986, %rd1985, %rd1728;
	add.s64 	%rd1987, %rd1986, %rd1984;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1464,%dummy}, %rd1973;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1465}, %rd1973;
	}
	shf.r.wrap.b32 	%r1466, %r1465, %r1464, 14;
	shf.r.wrap.b32 	%r1467, %r1464, %r1465, 14;
	mov.b64 	%rd1988, {%r1467, %r1466};
	shf.r.wrap.b32 	%r1468, %r1465, %r1464, 18;
	shf.r.wrap.b32 	%r1469, %r1464, %r1465, 18;
	mov.b64 	%rd1989, {%r1469, %r1468};
	xor.b64  	%rd1990, %rd1988, %rd1989;
	shf.l.wrap.b32 	%r1470, %r1464, %r1465, 23;
	shf.l.wrap.b32 	%r1471, %r1465, %r1464, 23;
	mov.b64 	%rd1991, {%r1471, %r1470};
	xor.b64  	%rd1992, %rd1990, %rd1991;
	xor.b64  	%rd1993, %rd1899, %rd1936;
	and.b64  	%rd1994, %rd1993, %rd1973;
	xor.b64  	%rd1995, %rd1994, %rd1899;
	add.s64 	%rd1996, %rd1992, %rd1862;
	add.s64 	%rd1997, %rd1996, %rd1995;
	add.s64 	%rd1998, %rd1997, %rd1987;
	add.s64 	%rd1999, %rd1998, 8693463985226723168;
	and.b64  	%rd2000, %rd1974, %rd1937;
	or.b64  	%rd2001, %rd1974, %rd1937;
	and.b64  	%rd2002, %rd2001, %rd1900;
	or.b64  	%rd2003, %rd2002, %rd2000;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1472,%dummy}, %rd1974;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1473}, %rd1974;
	}
	shf.r.wrap.b32 	%r1474, %r1473, %r1472, 28;
	shf.r.wrap.b32 	%r1475, %r1472, %r1473, 28;
	mov.b64 	%rd2004, {%r1475, %r1474};
	shf.l.wrap.b32 	%r1476, %r1472, %r1473, 30;
	shf.l.wrap.b32 	%r1477, %r1473, %r1472, 30;
	mov.b64 	%rd2005, {%r1477, %r1476};
	xor.b64  	%rd2006, %rd2004, %rd2005;
	shf.l.wrap.b32 	%r1478, %r1472, %r1473, 25;
	shf.l.wrap.b32 	%r1479, %r1473, %r1472, 25;
	mov.b64 	%rd2007, {%r1479, %r1478};
	xor.b64  	%rd2008, %rd2006, %rd2007;
	add.s64 	%rd2009, %rd2003, %rd2008;
	add.s64 	%rd2010, %rd1999, %rd1863;
	add.s64 	%rd2011, %rd2009, %rd1999;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1480,%dummy}, %rd1950;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1481}, %rd1950;
	}
	shf.r.wrap.b32 	%r1482, %r1481, %r1480, 19;
	shf.r.wrap.b32 	%r1483, %r1480, %r1481, 19;
	mov.b64 	%rd2012, {%r1483, %r1482};
	shf.l.wrap.b32 	%r1484, %r1480, %r1481, 3;
	shf.l.wrap.b32 	%r1485, %r1481, %r1480, 3;
	mov.b64 	%rd2013, {%r1485, %r1484};
	xor.b64  	%rd2014, %rd2012, %rd2013;
	shr.u64 	%rd2015, %rd1950, 6;
	xor.b64  	%rd2016, %rd2014, %rd2015;
	shf.r.wrap.b32 	%r1486, %r1143, %r1142, 1;
	shf.r.wrap.b32 	%r1487, %r1142, %r1143, 1;
	mov.b64 	%rd2017, {%r1487, %r1486};
	shf.r.wrap.b32 	%r1488, %r1143, %r1142, 8;
	shf.r.wrap.b32 	%r1489, %r1142, %r1143, 8;
	mov.b64 	%rd2018, {%r1489, %r1488};
	xor.b64  	%rd2019, %rd2017, %rd2018;
	shr.u64 	%rd2020, %rd1469, 7;
	xor.b64  	%rd2021, %rd2019, %rd2020;
	add.s64 	%rd2022, %rd2016, %rd1432;
	add.s64 	%rd2023, %rd2022, %rd1765;
	add.s64 	%rd2024, %rd2023, %rd2021;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1490,%dummy}, %rd2010;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1491}, %rd2010;
	}
	shf.r.wrap.b32 	%r1492, %r1491, %r1490, 14;
	shf.r.wrap.b32 	%r1493, %r1490, %r1491, 14;
	mov.b64 	%rd2025, {%r1493, %r1492};
	shf.r.wrap.b32 	%r1494, %r1491, %r1490, 18;
	shf.r.wrap.b32 	%r1495, %r1490, %r1491, 18;
	mov.b64 	%rd2026, {%r1495, %r1494};
	xor.b64  	%rd2027, %rd2025, %rd2026;
	shf.l.wrap.b32 	%r1496, %r1490, %r1491, 23;
	shf.l.wrap.b32 	%r1497, %r1491, %r1490, 23;
	mov.b64 	%rd2028, {%r1497, %r1496};
	xor.b64  	%rd2029, %rd2027, %rd2028;
	xor.b64  	%rd2030, %rd1936, %rd1973;
	and.b64  	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1936;
	add.s64 	%rd2033, %rd2029, %rd1899;
	add.s64 	%rd2034, %rd2033, %rd2032;
	add.s64 	%rd2035, %rd2034, %rd2024;
	add.s64 	%rd2036, %rd2035, -8878714635349349518;
	and.b64  	%rd2037, %rd2011, %rd1974;
	or.b64  	%rd2038, %rd2011, %rd1974;
	and.b64  	%rd2039, %rd2038, %rd1937;
	or.b64  	%rd2040, %rd2039, %rd2037;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1498,%dummy}, %rd2011;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1499}, %rd2011;
	}
	shf.r.wrap.b32 	%r1500, %r1499, %r1498, 28;
	shf.r.wrap.b32 	%r1501, %r1498, %r1499, 28;
	mov.b64 	%rd2041, {%r1501, %r1500};
	shf.l.wrap.b32 	%r1502, %r1498, %r1499, 30;
	shf.l.wrap.b32 	%r1503, %r1499, %r1498, 30;
	mov.b64 	%rd2042, {%r1503, %r1502};
	xor.b64  	%rd2043, %rd2041, %rd2042;
	shf.l.wrap.b32 	%r1504, %r1498, %r1499, 25;
	shf.l.wrap.b32 	%r1505, %r1499, %r1498, 25;
	mov.b64 	%rd2044, {%r1505, %r1504};
	xor.b64  	%rd2045, %rd2043, %rd2044;
	add.s64 	%rd2046, %rd2040, %rd2045;
	add.s64 	%rd2047, %rd2036, %rd1900;
	add.s64 	%rd2048, %rd2046, %rd2036;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1506,%dummy}, %rd1987;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1507}, %rd1987;
	}
	shf.r.wrap.b32 	%r1508, %r1507, %r1506, 19;
	shf.r.wrap.b32 	%r1509, %r1506, %r1507, 19;
	mov.b64 	%rd2049, {%r1509, %r1508};
	shf.l.wrap.b32 	%r1510, %r1506, %r1507, 3;
	shf.l.wrap.b32 	%r1511, %r1507, %r1506, 3;
	mov.b64 	%rd2050, {%r1511, %r1510};
	xor.b64  	%rd2051, %rd2049, %rd2050;
	shr.u64 	%rd2052, %rd1987, 6;
	xor.b64  	%rd2053, %rd2051, %rd2052;
	shf.r.wrap.b32 	%r1512, %r1169, %r1168, 1;
	shf.r.wrap.b32 	%r1513, %r1168, %r1169, 1;
	mov.b64 	%rd2054, {%r1513, %r1512};
	shf.r.wrap.b32 	%r1514, %r1169, %r1168, 8;
	shf.r.wrap.b32 	%r1515, %r1168, %r1169, 8;
	mov.b64 	%rd2055, {%r1515, %r1514};
	xor.b64  	%rd2056, %rd2054, %rd2055;
	shr.u64 	%rd2057, %rd1506, 7;
	xor.b64  	%rd2058, %rd2056, %rd2057;
	add.s64 	%rd2059, %rd2053, %rd1469;
	add.s64 	%rd2060, %rd2059, %rd1802;
	add.s64 	%rd2061, %rd2060, %rd2058;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1516,%dummy}, %rd2047;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1517}, %rd2047;
	}
	shf.r.wrap.b32 	%r1518, %r1517, %r1516, 14;
	shf.r.wrap.b32 	%r1519, %r1516, %r1517, 14;
	mov.b64 	%rd2062, {%r1519, %r1518};
	shf.r.wrap.b32 	%r1520, %r1517, %r1516, 18;
	shf.r.wrap.b32 	%r1521, %r1516, %r1517, 18;
	mov.b64 	%rd2063, {%r1521, %r1520};
	xor.b64  	%rd2064, %rd2062, %rd2063;
	shf.l.wrap.b32 	%r1522, %r1516, %r1517, 23;
	shf.l.wrap.b32 	%r1523, %r1517, %r1516, 23;
	mov.b64 	%rd2065, {%r1523, %r1522};
	xor.b64  	%rd2066, %rd2064, %rd2065;
	xor.b64  	%rd2067, %rd1973, %rd2010;
	and.b64  	%rd2068, %rd2067, %rd2047;
	xor.b64  	%rd2069, %rd2068, %rd1973;
	add.s64 	%rd2070, %rd2066, %rd1936;
	add.s64 	%rd2071, %rd2070, %rd2069;
	add.s64 	%rd2072, %rd2071, %rd2061;
	add.s64 	%rd2073, %rd2072, -8302665154208450068;
	and.b64  	%rd2074, %rd2048, %rd2011;
	or.b64  	%rd2075, %rd2048, %rd2011;
	and.b64  	%rd2076, %rd2075, %rd1974;
	or.b64  	%rd2077, %rd2076, %rd2074;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1524,%dummy}, %rd2048;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1525}, %rd2048;
	}
	shf.r.wrap.b32 	%r1526, %r1525, %r1524, 28;
	shf.r.wrap.b32 	%r1527, %r1524, %r1525, 28;
	mov.b64 	%rd2078, {%r1527, %r1526};
	shf.l.wrap.b32 	%r1528, %r1524, %r1525, 30;
	shf.l.wrap.b32 	%r1529, %r1525, %r1524, 30;
	mov.b64 	%rd2079, {%r1529, %r1528};
	xor.b64  	%rd2080, %rd2078, %rd2079;
	shf.l.wrap.b32 	%r1530, %r1524, %r1525, 25;
	shf.l.wrap.b32 	%r1531, %r1525, %r1524, 25;
	mov.b64 	%rd2081, {%r1531, %r1530};
	xor.b64  	%rd2082, %rd2080, %rd2081;
	add.s64 	%rd2083, %rd2077, %rd2082;
	add.s64 	%rd2084, %rd2073, %rd1937;
	add.s64 	%rd2085, %rd2083, %rd2073;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1532,%dummy}, %rd2024;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1533}, %rd2024;
	}
	shf.r.wrap.b32 	%r1534, %r1533, %r1532, 19;
	shf.r.wrap.b32 	%r1535, %r1532, %r1533, 19;
	mov.b64 	%rd2086, {%r1535, %r1534};
	shf.l.wrap.b32 	%r1536, %r1532, %r1533, 3;
	shf.l.wrap.b32 	%r1537, %r1533, %r1532, 3;
	mov.b64 	%rd2087, {%r1537, %r1536};
	xor.b64  	%rd2088, %rd2086, %rd2087;
	shr.u64 	%rd2089, %rd2024, 6;
	xor.b64  	%rd2090, %rd2088, %rd2089;
	shf.r.wrap.b32 	%r1538, %r1195, %r1194, 1;
	shf.r.wrap.b32 	%r1539, %r1194, %r1195, 1;
	mov.b64 	%rd2091, {%r1539, %r1538};
	shf.r.wrap.b32 	%r1540, %r1195, %r1194, 8;
	shf.r.wrap.b32 	%r1541, %r1194, %r1195, 8;
	mov.b64 	%rd2092, {%r1541, %r1540};
	xor.b64  	%rd2093, %rd2091, %rd2092;
	shr.u64 	%rd2094, %rd1543, 7;
	xor.b64  	%rd2095, %rd2093, %rd2094;
	add.s64 	%rd2096, %rd2090, %rd1506;
	add.s64 	%rd2097, %rd2096, %rd1839;
	add.s64 	%rd2098, %rd2097, %rd2095;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1542,%dummy}, %rd2084;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1543}, %rd2084;
	}
	shf.r.wrap.b32 	%r1544, %r1543, %r1542, 14;
	shf.r.wrap.b32 	%r1545, %r1542, %r1543, 14;
	mov.b64 	%rd2099, {%r1545, %r1544};
	shf.r.wrap.b32 	%r1546, %r1543, %r1542, 18;
	shf.r.wrap.b32 	%r1547, %r1542, %r1543, 18;
	mov.b64 	%rd2100, {%r1547, %r1546};
	xor.b64  	%rd2101, %rd2099, %rd2100;
	shf.l.wrap.b32 	%r1548, %r1542, %r1543, 23;
	shf.l.wrap.b32 	%r1549, %r1543, %r1542, 23;
	mov.b64 	%rd2102, {%r1549, %r1548};
	xor.b64  	%rd2103, %rd2101, %rd2102;
	xor.b64  	%rd2104, %rd2010, %rd2047;
	and.b64  	%rd2105, %rd2104, %rd2084;
	xor.b64  	%rd2106, %rd2105, %rd2010;
	add.s64 	%rd2107, %rd2103, %rd1973;
	add.s64 	%rd2108, %rd2107, %rd2106;
	add.s64 	%rd2109, %rd2108, %rd2098;
	add.s64 	%rd2110, %rd2109, -8016688836872298968;
	and.b64  	%rd2111, %rd2085, %rd2048;
	or.b64  	%rd2112, %rd2085, %rd2048;
	and.b64  	%rd2113, %rd2112, %rd2011;
	or.b64  	%rd2114, %rd2113, %rd2111;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1550,%dummy}, %rd2085;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1551}, %rd2085;
	}
	shf.r.wrap.b32 	%r1552, %r1551, %r1550, 28;
	shf.r.wrap.b32 	%r1553, %r1550, %r1551, 28;
	mov.b64 	%rd2115, {%r1553, %r1552};
	shf.l.wrap.b32 	%r1554, %r1550, %r1551, 30;
	shf.l.wrap.b32 	%r1555, %r1551, %r1550, 30;
	mov.b64 	%rd2116, {%r1555, %r1554};
	xor.b64  	%rd2117, %rd2115, %rd2116;
	shf.l.wrap.b32 	%r1556, %r1550, %r1551, 25;
	shf.l.wrap.b32 	%r1557, %r1551, %r1550, 25;
	mov.b64 	%rd2118, {%r1557, %r1556};
	xor.b64  	%rd2119, %rd2117, %rd2118;
	add.s64 	%rd2120, %rd2114, %rd2119;
	add.s64 	%rd2121, %rd2110, %rd1974;
	add.s64 	%rd2122, %rd2120, %rd2110;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1558,%dummy}, %rd2061;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1559}, %rd2061;
	}
	shf.r.wrap.b32 	%r1560, %r1559, %r1558, 19;
	shf.r.wrap.b32 	%r1561, %r1558, %r1559, 19;
	mov.b64 	%rd2123, {%r1561, %r1560};
	shf.l.wrap.b32 	%r1562, %r1558, %r1559, 3;
	shf.l.wrap.b32 	%r1563, %r1559, %r1558, 3;
	mov.b64 	%rd2124, {%r1563, %r1562};
	xor.b64  	%rd2125, %rd2123, %rd2124;
	shr.u64 	%rd2126, %rd2061, 6;
	xor.b64  	%rd2127, %rd2125, %rd2126;
	shf.r.wrap.b32 	%r1564, %r1221, %r1220, 1;
	shf.r.wrap.b32 	%r1565, %r1220, %r1221, 1;
	mov.b64 	%rd2128, {%r1565, %r1564};
	shf.r.wrap.b32 	%r1566, %r1221, %r1220, 8;
	shf.r.wrap.b32 	%r1567, %r1220, %r1221, 8;
	mov.b64 	%rd2129, {%r1567, %r1566};
	xor.b64  	%rd2130, %rd2128, %rd2129;
	shr.u64 	%rd2131, %rd1580, 7;
	xor.b64  	%rd2132, %rd2130, %rd2131;
	add.s64 	%rd2133, %rd2127, %rd1543;
	add.s64 	%rd2134, %rd2133, %rd1876;
	add.s64 	%rd2135, %rd2134, %rd2132;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1568,%dummy}, %rd2121;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1569}, %rd2121;
	}
	shf.r.wrap.b32 	%r1570, %r1569, %r1568, 14;
	shf.r.wrap.b32 	%r1571, %r1568, %r1569, 14;
	mov.b64 	%rd2136, {%r1571, %r1570};
	shf.r.wrap.b32 	%r1572, %r1569, %r1568, 18;
	shf.r.wrap.b32 	%r1573, %r1568, %r1569, 18;
	mov.b64 	%rd2137, {%r1573, %r1572};
	xor.b64  	%rd2138, %rd2136, %rd2137;
	shf.l.wrap.b32 	%r1574, %r1568, %r1569, 23;
	shf.l.wrap.b32 	%r1575, %r1569, %r1568, 23;
	mov.b64 	%rd2139, {%r1575, %r1574};
	xor.b64  	%rd2140, %rd2138, %rd2139;
	xor.b64  	%rd2141, %rd2047, %rd2084;
	and.b64  	%rd2142, %rd2141, %rd2121;
	xor.b64  	%rd2143, %rd2142, %rd2047;
	add.s64 	%rd2144, %rd2140, %rd2010;
	add.s64 	%rd2145, %rd2144, %rd2143;
	add.s64 	%rd2146, %rd2145, %rd2135;
	add.s64 	%rd2147, %rd2146, -6606660893046293015;
	and.b64  	%rd2148, %rd2122, %rd2085;
	or.b64  	%rd2149, %rd2122, %rd2085;
	and.b64  	%rd2150, %rd2149, %rd2048;
	or.b64  	%rd2151, %rd2150, %rd2148;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1576,%dummy}, %rd2122;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1577}, %rd2122;
	}
	shf.r.wrap.b32 	%r1578, %r1577, %r1576, 28;
	shf.r.wrap.b32 	%r1579, %r1576, %r1577, 28;
	mov.b64 	%rd2152, {%r1579, %r1578};
	shf.l.wrap.b32 	%r1580, %r1576, %r1577, 30;
	shf.l.wrap.b32 	%r1581, %r1577, %r1576, 30;
	mov.b64 	%rd2153, {%r1581, %r1580};
	xor.b64  	%rd2154, %rd2152, %rd2153;
	shf.l.wrap.b32 	%r1582, %r1576, %r1577, 25;
	shf.l.wrap.b32 	%r1583, %r1577, %r1576, 25;
	mov.b64 	%rd2155, {%r1583, %r1582};
	xor.b64  	%rd2156, %rd2154, %rd2155;
	add.s64 	%rd2157, %rd2151, %rd2156;
	add.s64 	%rd2158, %rd2147, %rd2011;
	add.s64 	%rd2159, %rd2157, %rd2147;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1584,%dummy}, %rd2098;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1585}, %rd2098;
	}
	shf.r.wrap.b32 	%r1586, %r1585, %r1584, 19;
	shf.r.wrap.b32 	%r1587, %r1584, %r1585, 19;
	mov.b64 	%rd2160, {%r1587, %r1586};
	shf.l.wrap.b32 	%r1588, %r1584, %r1585, 3;
	shf.l.wrap.b32 	%r1589, %r1585, %r1584, 3;
	mov.b64 	%rd2161, {%r1589, %r1588};
	xor.b64  	%rd2162, %rd2160, %rd2161;
	shr.u64 	%rd2163, %rd2098, 6;
	xor.b64  	%rd2164, %rd2162, %rd2163;
	shf.r.wrap.b32 	%r1590, %r1247, %r1246, 1;
	shf.r.wrap.b32 	%r1591, %r1246, %r1247, 1;
	mov.b64 	%rd2165, {%r1591, %r1590};
	shf.r.wrap.b32 	%r1592, %r1247, %r1246, 8;
	shf.r.wrap.b32 	%r1593, %r1246, %r1247, 8;
	mov.b64 	%rd2166, {%r1593, %r1592};
	xor.b64  	%rd2167, %rd2165, %rd2166;
	shr.u64 	%rd2168, %rd1617, 7;
	xor.b64  	%rd2169, %rd2167, %rd2168;
	add.s64 	%rd2170, %rd2164, %rd1580;
	add.s64 	%rd2171, %rd2170, %rd1913;
	add.s64 	%rd2172, %rd2171, %rd2169;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1594,%dummy}, %rd2158;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1595}, %rd2158;
	}
	shf.r.wrap.b32 	%r1596, %r1595, %r1594, 14;
	shf.r.wrap.b32 	%r1597, %r1594, %r1595, 14;
	mov.b64 	%rd2173, {%r1597, %r1596};
	shf.r.wrap.b32 	%r1598, %r1595, %r1594, 18;
	shf.r.wrap.b32 	%r1599, %r1594, %r1595, 18;
	mov.b64 	%rd2174, {%r1599, %r1598};
	xor.b64  	%rd2175, %rd2173, %rd2174;
	shf.l.wrap.b32 	%r1600, %r1594, %r1595, 23;
	shf.l.wrap.b32 	%r1601, %r1595, %r1594, 23;
	mov.b64 	%rd2176, {%r1601, %r1600};
	xor.b64  	%rd2177, %rd2175, %rd2176;
	xor.b64  	%rd2178, %rd2084, %rd2121;
	and.b64  	%rd2179, %rd2178, %rd2158;
	xor.b64  	%rd2180, %rd2179, %rd2084;
	add.s64 	%rd2181, %rd2177, %rd2047;
	add.s64 	%rd2182, %rd2181, %rd2180;
	add.s64 	%rd2183, %rd2182, %rd2172;
	add.s64 	%rd2184, %rd2183, -4685533653050689259;
	and.b64  	%rd2185, %rd2159, %rd2122;
	or.b64  	%rd2186, %rd2159, %rd2122;
	and.b64  	%rd2187, %rd2186, %rd2085;
	or.b64  	%rd2188, %rd2187, %rd2185;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1602,%dummy}, %rd2159;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1603}, %rd2159;
	}
	shf.r.wrap.b32 	%r1604, %r1603, %r1602, 28;
	shf.r.wrap.b32 	%r1605, %r1602, %r1603, 28;
	mov.b64 	%rd2189, {%r1605, %r1604};
	shf.l.wrap.b32 	%r1606, %r1602, %r1603, 30;
	shf.l.wrap.b32 	%r1607, %r1603, %r1602, 30;
	mov.b64 	%rd2190, {%r1607, %r1606};
	xor.b64  	%rd2191, %rd2189, %rd2190;
	shf.l.wrap.b32 	%r1608, %r1602, %r1603, 25;
	shf.l.wrap.b32 	%r1609, %r1603, %r1602, 25;
	mov.b64 	%rd2192, {%r1609, %r1608};
	xor.b64  	%rd2193, %rd2191, %rd2192;
	add.s64 	%rd2194, %rd2188, %rd2193;
	add.s64 	%rd2195, %rd2184, %rd2048;
	add.s64 	%rd2196, %rd2194, %rd2184;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1610,%dummy}, %rd2135;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1611}, %rd2135;
	}
	shf.r.wrap.b32 	%r1612, %r1611, %r1610, 19;
	shf.r.wrap.b32 	%r1613, %r1610, %r1611, 19;
	mov.b64 	%rd2197, {%r1613, %r1612};
	shf.l.wrap.b32 	%r1614, %r1610, %r1611, 3;
	shf.l.wrap.b32 	%r1615, %r1611, %r1610, 3;
	mov.b64 	%rd2198, {%r1615, %r1614};
	xor.b64  	%rd2199, %rd2197, %rd2198;
	shr.u64 	%rd2200, %rd2135, 6;
	xor.b64  	%rd2201, %rd2199, %rd2200;
	shf.r.wrap.b32 	%r1616, %r1273, %r1272, 1;
	shf.r.wrap.b32 	%r1617, %r1272, %r1273, 1;
	mov.b64 	%rd2202, {%r1617, %r1616};
	shf.r.wrap.b32 	%r1618, %r1273, %r1272, 8;
	shf.r.wrap.b32 	%r1619, %r1272, %r1273, 8;
	mov.b64 	%rd2203, {%r1619, %r1618};
	xor.b64  	%rd2204, %rd2202, %rd2203;
	shr.u64 	%rd2205, %rd1654, 7;
	xor.b64  	%rd2206, %rd2204, %rd2205;
	add.s64 	%rd2207, %rd2201, %rd1617;
	add.s64 	%rd2208, %rd2207, %rd1950;
	add.s64 	%rd2209, %rd2208, %rd2206;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1620,%dummy}, %rd2195;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1621}, %rd2195;
	}
	shf.r.wrap.b32 	%r1622, %r1621, %r1620, 14;
	shf.r.wrap.b32 	%r1623, %r1620, %r1621, 14;
	mov.b64 	%rd2210, {%r1623, %r1622};
	shf.r.wrap.b32 	%r1624, %r1621, %r1620, 18;
	shf.r.wrap.b32 	%r1625, %r1620, %r1621, 18;
	mov.b64 	%rd2211, {%r1625, %r1624};
	xor.b64  	%rd2212, %rd2210, %rd2211;
	shf.l.wrap.b32 	%r1626, %r1620, %r1621, 23;
	shf.l.wrap.b32 	%r1627, %r1621, %r1620, 23;
	mov.b64 	%rd2213, {%r1627, %r1626};
	xor.b64  	%rd2214, %rd2212, %rd2213;
	xor.b64  	%rd2215, %rd2121, %rd2158;
	and.b64  	%rd2216, %rd2215, %rd2195;
	xor.b64  	%rd2217, %rd2216, %rd2121;
	add.s64 	%rd2218, %rd2214, %rd2084;
	add.s64 	%rd2219, %rd2218, %rd2217;
	add.s64 	%rd2220, %rd2219, %rd2209;
	add.s64 	%rd2221, %rd2220, -4147400797238176981;
	and.b64  	%rd2222, %rd2196, %rd2159;
	or.b64  	%rd2223, %rd2196, %rd2159;
	and.b64  	%rd2224, %rd2223, %rd2122;
	or.b64  	%rd2225, %rd2224, %rd2222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1628,%dummy}, %rd2196;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1629}, %rd2196;
	}
	shf.r.wrap.b32 	%r1630, %r1629, %r1628, 28;
	shf.r.wrap.b32 	%r1631, %r1628, %r1629, 28;
	mov.b64 	%rd2226, {%r1631, %r1630};
	shf.l.wrap.b32 	%r1632, %r1628, %r1629, 30;
	shf.l.wrap.b32 	%r1633, %r1629, %r1628, 30;
	mov.b64 	%rd2227, {%r1633, %r1632};
	xor.b64  	%rd2228, %rd2226, %rd2227;
	shf.l.wrap.b32 	%r1634, %r1628, %r1629, 25;
	shf.l.wrap.b32 	%r1635, %r1629, %r1628, 25;
	mov.b64 	%rd2229, {%r1635, %r1634};
	xor.b64  	%rd2230, %rd2228, %rd2229;
	add.s64 	%rd2231, %rd2225, %rd2230;
	add.s64 	%rd2232, %rd2221, %rd2085;
	add.s64 	%rd2233, %rd2231, %rd2221;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1636,%dummy}, %rd2172;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1637}, %rd2172;
	}
	shf.r.wrap.b32 	%r1638, %r1637, %r1636, 19;
	shf.r.wrap.b32 	%r1639, %r1636, %r1637, 19;
	mov.b64 	%rd2234, {%r1639, %r1638};
	shf.l.wrap.b32 	%r1640, %r1636, %r1637, 3;
	shf.l.wrap.b32 	%r1641, %r1637, %r1636, 3;
	mov.b64 	%rd2235, {%r1641, %r1640};
	xor.b64  	%rd2236, %rd2234, %rd2235;
	shr.u64 	%rd2237, %rd2172, 6;
	xor.b64  	%rd2238, %rd2236, %rd2237;
	shf.r.wrap.b32 	%r1642, %r1299, %r1298, 1;
	shf.r.wrap.b32 	%r1643, %r1298, %r1299, 1;
	mov.b64 	%rd2239, {%r1643, %r1642};
	shf.r.wrap.b32 	%r1644, %r1299, %r1298, 8;
	shf.r.wrap.b32 	%r1645, %r1298, %r1299, 8;
	mov.b64 	%rd2240, {%r1645, %r1644};
	xor.b64  	%rd2241, %rd2239, %rd2240;
	shr.u64 	%rd2242, %rd1691, 7;
	xor.b64  	%rd2243, %rd2241, %rd2242;
	add.s64 	%rd2244, %rd2238, %rd1654;
	add.s64 	%rd2245, %rd2244, %rd1987;
	add.s64 	%rd2246, %rd2245, %rd2243;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1646,%dummy}, %rd2232;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1647}, %rd2232;
	}
	shf.r.wrap.b32 	%r1648, %r1647, %r1646, 14;
	shf.r.wrap.b32 	%r1649, %r1646, %r1647, 14;
	mov.b64 	%rd2247, {%r1649, %r1648};
	shf.r.wrap.b32 	%r1650, %r1647, %r1646, 18;
	shf.r.wrap.b32 	%r1651, %r1646, %r1647, 18;
	mov.b64 	%rd2248, {%r1651, %r1650};
	xor.b64  	%rd2249, %rd2247, %rd2248;
	shf.l.wrap.b32 	%r1652, %r1646, %r1647, 23;
	shf.l.wrap.b32 	%r1653, %r1647, %r1646, 23;
	mov.b64 	%rd2250, {%r1653, %r1652};
	xor.b64  	%rd2251, %rd2249, %rd2250;
	xor.b64  	%rd2252, %rd2158, %rd2195;
	and.b64  	%rd2253, %rd2252, %rd2232;
	xor.b64  	%rd2254, %rd2253, %rd2158;
	add.s64 	%rd2255, %rd2251, %rd2121;
	add.s64 	%rd2256, %rd2255, %rd2254;
	add.s64 	%rd2257, %rd2256, %rd2246;
	add.s64 	%rd2258, %rd2257, -3880063495543823972;
	and.b64  	%rd2259, %rd2233, %rd2196;
	or.b64  	%rd2260, %rd2233, %rd2196;
	and.b64  	%rd2261, %rd2260, %rd2159;
	or.b64  	%rd2262, %rd2261, %rd2259;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1654,%dummy}, %rd2233;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1655}, %rd2233;
	}
	shf.r.wrap.b32 	%r1656, %r1655, %r1654, 28;
	shf.r.wrap.b32 	%r1657, %r1654, %r1655, 28;
	mov.b64 	%rd2263, {%r1657, %r1656};
	shf.l.wrap.b32 	%r1658, %r1654, %r1655, 30;
	shf.l.wrap.b32 	%r1659, %r1655, %r1654, 30;
	mov.b64 	%rd2264, {%r1659, %r1658};
	xor.b64  	%rd2265, %rd2263, %rd2264;
	shf.l.wrap.b32 	%r1660, %r1654, %r1655, 25;
	shf.l.wrap.b32 	%r1661, %r1655, %r1654, 25;
	mov.b64 	%rd2266, {%r1661, %r1660};
	xor.b64  	%rd2267, %rd2265, %rd2266;
	add.s64 	%rd2268, %rd2262, %rd2267;
	add.s64 	%rd2269, %rd2258, %rd2122;
	add.s64 	%rd2270, %rd2268, %rd2258;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1662,%dummy}, %rd2209;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1663}, %rd2209;
	}
	shf.r.wrap.b32 	%r1664, %r1663, %r1662, 19;
	shf.r.wrap.b32 	%r1665, %r1662, %r1663, 19;
	mov.b64 	%rd2271, {%r1665, %r1664};
	shf.l.wrap.b32 	%r1666, %r1662, %r1663, 3;
	shf.l.wrap.b32 	%r1667, %r1663, %r1662, 3;
	mov.b64 	%rd2272, {%r1667, %r1666};
	xor.b64  	%rd2273, %rd2271, %rd2272;
	shr.u64 	%rd2274, %rd2209, 6;
	xor.b64  	%rd2275, %rd2273, %rd2274;
	shf.r.wrap.b32 	%r1668, %r1325, %r1324, 1;
	shf.r.wrap.b32 	%r1669, %r1324, %r1325, 1;
	mov.b64 	%rd2276, {%r1669, %r1668};
	shf.r.wrap.b32 	%r1670, %r1325, %r1324, 8;
	shf.r.wrap.b32 	%r1671, %r1324, %r1325, 8;
	mov.b64 	%rd2277, {%r1671, %r1670};
	xor.b64  	%rd2278, %rd2276, %rd2277;
	shr.u64 	%rd2279, %rd1728, 7;
	xor.b64  	%rd2280, %rd2278, %rd2279;
	add.s64 	%rd2281, %rd2275, %rd1691;
	add.s64 	%rd2282, %rd2281, %rd2024;
	add.s64 	%rd2283, %rd2282, %rd2280;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1672,%dummy}, %rd2269;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1673}, %rd2269;
	}
	shf.r.wrap.b32 	%r1674, %r1673, %r1672, 14;
	shf.r.wrap.b32 	%r1675, %r1672, %r1673, 14;
	mov.b64 	%rd2284, {%r1675, %r1674};
	shf.r.wrap.b32 	%r1676, %r1673, %r1672, 18;
	shf.r.wrap.b32 	%r1677, %r1672, %r1673, 18;
	mov.b64 	%rd2285, {%r1677, %r1676};
	xor.b64  	%rd2286, %rd2284, %rd2285;
	shf.l.wrap.b32 	%r1678, %r1672, %r1673, 23;
	shf.l.wrap.b32 	%r1679, %r1673, %r1672, 23;
	mov.b64 	%rd2287, {%r1679, %r1678};
	xor.b64  	%rd2288, %rd2286, %rd2287;
	xor.b64  	%rd2289, %rd2195, %rd2232;
	and.b64  	%rd2290, %rd2289, %rd2269;
	xor.b64  	%rd2291, %rd2290, %rd2195;
	add.s64 	%rd2292, %rd2288, %rd2158;
	add.s64 	%rd2293, %rd2292, %rd2291;
	add.s64 	%rd2294, %rd2293, %rd2283;
	add.s64 	%rd2295, %rd2294, -3348786107499101689;
	and.b64  	%rd2296, %rd2270, %rd2233;
	or.b64  	%rd2297, %rd2270, %rd2233;
	and.b64  	%rd2298, %rd2297, %rd2196;
	or.b64  	%rd2299, %rd2298, %rd2296;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1680,%dummy}, %rd2270;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1681}, %rd2270;
	}
	shf.r.wrap.b32 	%r1682, %r1681, %r1680, 28;
	shf.r.wrap.b32 	%r1683, %r1680, %r1681, 28;
	mov.b64 	%rd2300, {%r1683, %r1682};
	shf.l.wrap.b32 	%r1684, %r1680, %r1681, 30;
	shf.l.wrap.b32 	%r1685, %r1681, %r1680, 30;
	mov.b64 	%rd2301, {%r1685, %r1684};
	xor.b64  	%rd2302, %rd2300, %rd2301;
	shf.l.wrap.b32 	%r1686, %r1680, %r1681, 25;
	shf.l.wrap.b32 	%r1687, %r1681, %r1680, 25;
	mov.b64 	%rd2303, {%r1687, %r1686};
	xor.b64  	%rd2304, %rd2302, %rd2303;
	add.s64 	%rd2305, %rd2299, %rd2304;
	add.s64 	%rd2306, %rd2295, %rd2159;
	add.s64 	%rd2307, %rd2305, %rd2295;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1688,%dummy}, %rd2246;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1689}, %rd2246;
	}
	shf.r.wrap.b32 	%r1690, %r1689, %r1688, 19;
	shf.r.wrap.b32 	%r1691, %r1688, %r1689, 19;
	mov.b64 	%rd2308, {%r1691, %r1690};
	shf.l.wrap.b32 	%r1692, %r1688, %r1689, 3;
	shf.l.wrap.b32 	%r1693, %r1689, %r1688, 3;
	mov.b64 	%rd2309, {%r1693, %r1692};
	xor.b64  	%rd2310, %rd2308, %rd2309;
	shr.u64 	%rd2311, %rd2246, 6;
	xor.b64  	%rd2312, %rd2310, %rd2311;
	shf.r.wrap.b32 	%r1694, %r1351, %r1350, 1;
	shf.r.wrap.b32 	%r1695, %r1350, %r1351, 1;
	mov.b64 	%rd2313, {%r1695, %r1694};
	shf.r.wrap.b32 	%r1696, %r1351, %r1350, 8;
	shf.r.wrap.b32 	%r1697, %r1350, %r1351, 8;
	mov.b64 	%rd2314, {%r1697, %r1696};
	xor.b64  	%rd2315, %rd2313, %rd2314;
	shr.u64 	%rd2316, %rd1765, 7;
	xor.b64  	%rd2317, %rd2315, %rd2316;
	add.s64 	%rd2318, %rd2312, %rd1728;
	add.s64 	%rd2319, %rd2318, %rd2061;
	add.s64 	%rd2320, %rd2319, %rd2317;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1698,%dummy}, %rd2306;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1699}, %rd2306;
	}
	shf.r.wrap.b32 	%r1700, %r1699, %r1698, 14;
	shf.r.wrap.b32 	%r1701, %r1698, %r1699, 14;
	mov.b64 	%rd2321, {%r1701, %r1700};
	shf.r.wrap.b32 	%r1702, %r1699, %r1698, 18;
	shf.r.wrap.b32 	%r1703, %r1698, %r1699, 18;
	mov.b64 	%rd2322, {%r1703, %r1702};
	xor.b64  	%rd2323, %rd2321, %rd2322;
	shf.l.wrap.b32 	%r1704, %r1698, %r1699, 23;
	shf.l.wrap.b32 	%r1705, %r1699, %r1698, 23;
	mov.b64 	%rd2324, {%r1705, %r1704};
	xor.b64  	%rd2325, %rd2323, %rd2324;
	xor.b64  	%rd2326, %rd2232, %rd2269;
	and.b64  	%rd2327, %rd2326, %rd2306;
	xor.b64  	%rd2328, %rd2327, %rd2232;
	add.s64 	%rd2329, %rd2325, %rd2195;
	add.s64 	%rd2330, %rd2329, %rd2328;
	add.s64 	%rd2331, %rd2330, %rd2320;
	add.s64 	%rd2332, %rd2331, -1523767162380948706;
	and.b64  	%rd2333, %rd2307, %rd2270;
	or.b64  	%rd2334, %rd2307, %rd2270;
	and.b64  	%rd2335, %rd2334, %rd2233;
	or.b64  	%rd2336, %rd2335, %rd2333;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1706,%dummy}, %rd2307;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1707}, %rd2307;
	}
	shf.r.wrap.b32 	%r1708, %r1707, %r1706, 28;
	shf.r.wrap.b32 	%r1709, %r1706, %r1707, 28;
	mov.b64 	%rd2337, {%r1709, %r1708};
	shf.l.wrap.b32 	%r1710, %r1706, %r1707, 30;
	shf.l.wrap.b32 	%r1711, %r1707, %r1706, 30;
	mov.b64 	%rd2338, {%r1711, %r1710};
	xor.b64  	%rd2339, %rd2337, %rd2338;
	shf.l.wrap.b32 	%r1712, %r1706, %r1707, 25;
	shf.l.wrap.b32 	%r1713, %r1707, %r1706, 25;
	mov.b64 	%rd2340, {%r1713, %r1712};
	xor.b64  	%rd2341, %rd2339, %rd2340;
	add.s64 	%rd2342, %rd2336, %rd2341;
	add.s64 	%rd2343, %rd2332, %rd2196;
	add.s64 	%rd2344, %rd2342, %rd2332;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1714,%dummy}, %rd2283;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1715}, %rd2283;
	}
	shf.r.wrap.b32 	%r1716, %r1715, %r1714, 19;
	shf.r.wrap.b32 	%r1717, %r1714, %r1715, 19;
	mov.b64 	%rd2345, {%r1717, %r1716};
	shf.l.wrap.b32 	%r1718, %r1714, %r1715, 3;
	shf.l.wrap.b32 	%r1719, %r1715, %r1714, 3;
	mov.b64 	%rd2346, {%r1719, %r1718};
	xor.b64  	%rd2347, %rd2345, %rd2346;
	shr.u64 	%rd2348, %rd2283, 6;
	xor.b64  	%rd2349, %rd2347, %rd2348;
	shf.r.wrap.b32 	%r1720, %r1377, %r1376, 1;
	shf.r.wrap.b32 	%r1721, %r1376, %r1377, 1;
	mov.b64 	%rd2350, {%r1721, %r1720};
	shf.r.wrap.b32 	%r1722, %r1377, %r1376, 8;
	shf.r.wrap.b32 	%r1723, %r1376, %r1377, 8;
	mov.b64 	%rd2351, {%r1723, %r1722};
	xor.b64  	%rd2352, %rd2350, %rd2351;
	shr.u64 	%rd2353, %rd1802, 7;
	xor.b64  	%rd2354, %rd2352, %rd2353;
	add.s64 	%rd2355, %rd2349, %rd1765;
	add.s64 	%rd2356, %rd2355, %rd2098;
	add.s64 	%rd2357, %rd2356, %rd2354;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1724,%dummy}, %rd2343;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1725}, %rd2343;
	}
	shf.r.wrap.b32 	%r1726, %r1725, %r1724, 14;
	shf.r.wrap.b32 	%r1727, %r1724, %r1725, 14;
	mov.b64 	%rd2358, {%r1727, %r1726};
	shf.r.wrap.b32 	%r1728, %r1725, %r1724, 18;
	shf.r.wrap.b32 	%r1729, %r1724, %r1725, 18;
	mov.b64 	%rd2359, {%r1729, %r1728};
	xor.b64  	%rd2360, %rd2358, %rd2359;
	shf.l.wrap.b32 	%r1730, %r1724, %r1725, 23;
	shf.l.wrap.b32 	%r1731, %r1725, %r1724, 23;
	mov.b64 	%rd2361, {%r1731, %r1730};
	xor.b64  	%rd2362, %rd2360, %rd2361;
	xor.b64  	%rd2363, %rd2269, %rd2306;
	and.b64  	%rd2364, %rd2363, %rd2343;
	xor.b64  	%rd2365, %rd2364, %rd2269;
	add.s64 	%rd2366, %rd2362, %rd2232;
	add.s64 	%rd2367, %rd2366, %rd2365;
	add.s64 	%rd2368, %rd2367, %rd2357;
	add.s64 	%rd2369, %rd2368, -757361751448694408;
	and.b64  	%rd2370, %rd2344, %rd2307;
	or.b64  	%rd2371, %rd2344, %rd2307;
	and.b64  	%rd2372, %rd2371, %rd2270;
	or.b64  	%rd2373, %rd2372, %rd2370;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1732,%dummy}, %rd2344;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1733}, %rd2344;
	}
	shf.r.wrap.b32 	%r1734, %r1733, %r1732, 28;
	shf.r.wrap.b32 	%r1735, %r1732, %r1733, 28;
	mov.b64 	%rd2374, {%r1735, %r1734};
	shf.l.wrap.b32 	%r1736, %r1732, %r1733, 30;
	shf.l.wrap.b32 	%r1737, %r1733, %r1732, 30;
	mov.b64 	%rd2375, {%r1737, %r1736};
	xor.b64  	%rd2376, %rd2374, %rd2375;
	shf.l.wrap.b32 	%r1738, %r1732, %r1733, 25;
	shf.l.wrap.b32 	%r1739, %r1733, %r1732, 25;
	mov.b64 	%rd2377, {%r1739, %r1738};
	xor.b64  	%rd2378, %rd2376, %rd2377;
	add.s64 	%rd2379, %rd2373, %rd2378;
	add.s64 	%rd2380, %rd2369, %rd2233;
	add.s64 	%rd2381, %rd2379, %rd2369;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1740,%dummy}, %rd2320;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1741}, %rd2320;
	}
	shf.r.wrap.b32 	%r1742, %r1741, %r1740, 19;
	shf.r.wrap.b32 	%r1743, %r1740, %r1741, 19;
	mov.b64 	%rd2382, {%r1743, %r1742};
	shf.l.wrap.b32 	%r1744, %r1740, %r1741, 3;
	shf.l.wrap.b32 	%r1745, %r1741, %r1740, 3;
	mov.b64 	%rd2383, {%r1745, %r1744};
	xor.b64  	%rd2384, %rd2382, %rd2383;
	shr.u64 	%rd2385, %rd2320, 6;
	xor.b64  	%rd2386, %rd2384, %rd2385;
	shf.r.wrap.b32 	%r1746, %r1403, %r1402, 1;
	shf.r.wrap.b32 	%r1747, %r1402, %r1403, 1;
	mov.b64 	%rd2387, {%r1747, %r1746};
	shf.r.wrap.b32 	%r1748, %r1403, %r1402, 8;
	shf.r.wrap.b32 	%r1749, %r1402, %r1403, 8;
	mov.b64 	%rd2388, {%r1749, %r1748};
	xor.b64  	%rd2389, %rd2387, %rd2388;
	shr.u64 	%rd2390, %rd1839, 7;
	xor.b64  	%rd2391, %rd2389, %rd2390;
	add.s64 	%rd2392, %rd2386, %rd1802;
	add.s64 	%rd2393, %rd2392, %rd2135;
	add.s64 	%rd2394, %rd2393, %rd2391;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1750,%dummy}, %rd2380;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1751}, %rd2380;
	}
	shf.r.wrap.b32 	%r1752, %r1751, %r1750, 14;
	shf.r.wrap.b32 	%r1753, %r1750, %r1751, 14;
	mov.b64 	%rd2395, {%r1753, %r1752};
	shf.r.wrap.b32 	%r1754, %r1751, %r1750, 18;
	shf.r.wrap.b32 	%r1755, %r1750, %r1751, 18;
	mov.b64 	%rd2396, {%r1755, %r1754};
	xor.b64  	%rd2397, %rd2395, %rd2396;
	shf.l.wrap.b32 	%r1756, %r1750, %r1751, 23;
	shf.l.wrap.b32 	%r1757, %r1751, %r1750, 23;
	mov.b64 	%rd2398, {%r1757, %r1756};
	xor.b64  	%rd2399, %rd2397, %rd2398;
	xor.b64  	%rd2400, %rd2306, %rd2343;
	and.b64  	%rd2401, %rd2400, %rd2380;
	xor.b64  	%rd2402, %rd2401, %rd2306;
	add.s64 	%rd2403, %rd2399, %rd2269;
	add.s64 	%rd2404, %rd2403, %rd2402;
	add.s64 	%rd2405, %rd2404, %rd2394;
	add.s64 	%rd2406, %rd2405, 500013540394364858;
	and.b64  	%rd2407, %rd2381, %rd2344;
	or.b64  	%rd2408, %rd2381, %rd2344;
	and.b64  	%rd2409, %rd2408, %rd2307;
	or.b64  	%rd2410, %rd2409, %rd2407;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1758,%dummy}, %rd2381;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1759}, %rd2381;
	}
	shf.r.wrap.b32 	%r1760, %r1759, %r1758, 28;
	shf.r.wrap.b32 	%r1761, %r1758, %r1759, 28;
	mov.b64 	%rd2411, {%r1761, %r1760};
	shf.l.wrap.b32 	%r1762, %r1758, %r1759, 30;
	shf.l.wrap.b32 	%r1763, %r1759, %r1758, 30;
	mov.b64 	%rd2412, {%r1763, %r1762};
	xor.b64  	%rd2413, %rd2411, %rd2412;
	shf.l.wrap.b32 	%r1764, %r1758, %r1759, 25;
	shf.l.wrap.b32 	%r1765, %r1759, %r1758, 25;
	mov.b64 	%rd2414, {%r1765, %r1764};
	xor.b64  	%rd2415, %rd2413, %rd2414;
	add.s64 	%rd2416, %rd2410, %rd2415;
	add.s64 	%rd2417, %rd2406, %rd2270;
	add.s64 	%rd2418, %rd2416, %rd2406;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1766,%dummy}, %rd2357;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1767}, %rd2357;
	}
	shf.r.wrap.b32 	%r1768, %r1767, %r1766, 19;
	shf.r.wrap.b32 	%r1769, %r1766, %r1767, 19;
	mov.b64 	%rd2419, {%r1769, %r1768};
	shf.l.wrap.b32 	%r1770, %r1766, %r1767, 3;
	shf.l.wrap.b32 	%r1771, %r1767, %r1766, 3;
	mov.b64 	%rd2420, {%r1771, %r1770};
	xor.b64  	%rd2421, %rd2419, %rd2420;
	shr.u64 	%rd2422, %rd2357, 6;
	xor.b64  	%rd2423, %rd2421, %rd2422;
	shf.r.wrap.b32 	%r1772, %r1429, %r1428, 1;
	shf.r.wrap.b32 	%r1773, %r1428, %r1429, 1;
	mov.b64 	%rd2424, {%r1773, %r1772};
	shf.r.wrap.b32 	%r1774, %r1429, %r1428, 8;
	shf.r.wrap.b32 	%r1775, %r1428, %r1429, 8;
	mov.b64 	%rd2425, {%r1775, %r1774};
	xor.b64  	%rd2426, %rd2424, %rd2425;
	shr.u64 	%rd2427, %rd1876, 7;
	xor.b64  	%rd2428, %rd2426, %rd2427;
	add.s64 	%rd2429, %rd2423, %rd1839;
	add.s64 	%rd2430, %rd2429, %rd2172;
	add.s64 	%rd2431, %rd2430, %rd2428;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1776,%dummy}, %rd2417;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1777}, %rd2417;
	}
	shf.r.wrap.b32 	%r1778, %r1777, %r1776, 14;
	shf.r.wrap.b32 	%r1779, %r1776, %r1777, 14;
	mov.b64 	%rd2432, {%r1779, %r1778};
	shf.r.wrap.b32 	%r1780, %r1777, %r1776, 18;
	shf.r.wrap.b32 	%r1781, %r1776, %r1777, 18;
	mov.b64 	%rd2433, {%r1781, %r1780};
	xor.b64  	%rd2434, %rd2432, %rd2433;
	shf.l.wrap.b32 	%r1782, %r1776, %r1777, 23;
	shf.l.wrap.b32 	%r1783, %r1777, %r1776, 23;
	mov.b64 	%rd2435, {%r1783, %r1782};
	xor.b64  	%rd2436, %rd2434, %rd2435;
	xor.b64  	%rd2437, %rd2343, %rd2380;
	and.b64  	%rd2438, %rd2437, %rd2417;
	xor.b64  	%rd2439, %rd2438, %rd2343;
	add.s64 	%rd2440, %rd2436, %rd2306;
	add.s64 	%rd2441, %rd2440, %rd2439;
	add.s64 	%rd2442, %rd2441, %rd2431;
	add.s64 	%rd2443, %rd2442, 748580250866718886;
	and.b64  	%rd2444, %rd2418, %rd2381;
	or.b64  	%rd2445, %rd2418, %rd2381;
	and.b64  	%rd2446, %rd2445, %rd2344;
	or.b64  	%rd2447, %rd2446, %rd2444;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1784,%dummy}, %rd2418;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1785}, %rd2418;
	}
	shf.r.wrap.b32 	%r1786, %r1785, %r1784, 28;
	shf.r.wrap.b32 	%r1787, %r1784, %r1785, 28;
	mov.b64 	%rd2448, {%r1787, %r1786};
	shf.l.wrap.b32 	%r1788, %r1784, %r1785, 30;
	shf.l.wrap.b32 	%r1789, %r1785, %r1784, 30;
	mov.b64 	%rd2449, {%r1789, %r1788};
	xor.b64  	%rd2450, %rd2448, %rd2449;
	shf.l.wrap.b32 	%r1790, %r1784, %r1785, 25;
	shf.l.wrap.b32 	%r1791, %r1785, %r1784, 25;
	mov.b64 	%rd2451, {%r1791, %r1790};
	xor.b64  	%rd2452, %rd2450, %rd2451;
	add.s64 	%rd2453, %rd2447, %rd2452;
	add.s64 	%rd2454, %rd2443, %rd2307;
	add.s64 	%rd2455, %rd2453, %rd2443;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1792,%dummy}, %rd2394;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1793}, %rd2394;
	}
	shf.r.wrap.b32 	%r1794, %r1793, %r1792, 19;
	shf.r.wrap.b32 	%r1795, %r1792, %r1793, 19;
	mov.b64 	%rd2456, {%r1795, %r1794};
	shf.l.wrap.b32 	%r1796, %r1792, %r1793, 3;
	shf.l.wrap.b32 	%r1797, %r1793, %r1792, 3;
	mov.b64 	%rd2457, {%r1797, %r1796};
	xor.b64  	%rd2458, %rd2456, %rd2457;
	shr.u64 	%rd2459, %rd2394, 6;
	xor.b64  	%rd2460, %rd2458, %rd2459;
	shf.r.wrap.b32 	%r1798, %r1455, %r1454, 1;
	shf.r.wrap.b32 	%r1799, %r1454, %r1455, 1;
	mov.b64 	%rd2461, {%r1799, %r1798};
	shf.r.wrap.b32 	%r1800, %r1455, %r1454, 8;
	shf.r.wrap.b32 	%r1801, %r1454, %r1455, 8;
	mov.b64 	%rd2462, {%r1801, %r1800};
	xor.b64  	%rd2463, %rd2461, %rd2462;
	shr.u64 	%rd2464, %rd1913, 7;
	xor.b64  	%rd2465, %rd2463, %rd2464;
	add.s64 	%rd2466, %rd2460, %rd1876;
	add.s64 	%rd2467, %rd2466, %rd2209;
	add.s64 	%rd2468, %rd2467, %rd2465;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1802,%dummy}, %rd2454;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1803}, %rd2454;
	}
	shf.r.wrap.b32 	%r1804, %r1803, %r1802, 14;
	shf.r.wrap.b32 	%r1805, %r1802, %r1803, 14;
	mov.b64 	%rd2469, {%r1805, %r1804};
	shf.r.wrap.b32 	%r1806, %r1803, %r1802, 18;
	shf.r.wrap.b32 	%r1807, %r1802, %r1803, 18;
	mov.b64 	%rd2470, {%r1807, %r1806};
	xor.b64  	%rd2471, %rd2469, %rd2470;
	shf.l.wrap.b32 	%r1808, %r1802, %r1803, 23;
	shf.l.wrap.b32 	%r1809, %r1803, %r1802, 23;
	mov.b64 	%rd2472, {%r1809, %r1808};
	xor.b64  	%rd2473, %rd2471, %rd2472;
	xor.b64  	%rd2474, %rd2380, %rd2417;
	and.b64  	%rd2475, %rd2474, %rd2454;
	xor.b64  	%rd2476, %rd2475, %rd2380;
	add.s64 	%rd2477, %rd2473, %rd2343;
	add.s64 	%rd2478, %rd2477, %rd2476;
	add.s64 	%rd2479, %rd2478, %rd2468;
	add.s64 	%rd2480, %rd2479, 1242879168328830382;
	and.b64  	%rd2481, %rd2455, %rd2418;
	or.b64  	%rd2482, %rd2455, %rd2418;
	and.b64  	%rd2483, %rd2482, %rd2381;
	or.b64  	%rd2484, %rd2483, %rd2481;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1810,%dummy}, %rd2455;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1811}, %rd2455;
	}
	shf.r.wrap.b32 	%r1812, %r1811, %r1810, 28;
	shf.r.wrap.b32 	%r1813, %r1810, %r1811, 28;
	mov.b64 	%rd2485, {%r1813, %r1812};
	shf.l.wrap.b32 	%r1814, %r1810, %r1811, 30;
	shf.l.wrap.b32 	%r1815, %r1811, %r1810, 30;
	mov.b64 	%rd2486, {%r1815, %r1814};
	xor.b64  	%rd2487, %rd2485, %rd2486;
	shf.l.wrap.b32 	%r1816, %r1810, %r1811, 25;
	shf.l.wrap.b32 	%r1817, %r1811, %r1810, 25;
	mov.b64 	%rd2488, {%r1817, %r1816};
	xor.b64  	%rd2489, %rd2487, %rd2488;
	add.s64 	%rd2490, %rd2484, %rd2489;
	add.s64 	%rd2491, %rd2480, %rd2344;
	add.s64 	%rd2492, %rd2490, %rd2480;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1818,%dummy}, %rd2431;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1819}, %rd2431;
	}
	shf.r.wrap.b32 	%r1820, %r1819, %r1818, 19;
	shf.r.wrap.b32 	%r1821, %r1818, %r1819, 19;
	mov.b64 	%rd2493, {%r1821, %r1820};
	shf.l.wrap.b32 	%r1822, %r1818, %r1819, 3;
	shf.l.wrap.b32 	%r1823, %r1819, %r1818, 3;
	mov.b64 	%rd2494, {%r1823, %r1822};
	xor.b64  	%rd2495, %rd2493, %rd2494;
	shr.u64 	%rd2496, %rd2431, 6;
	xor.b64  	%rd2497, %rd2495, %rd2496;
	shf.r.wrap.b32 	%r1824, %r1481, %r1480, 1;
	shf.r.wrap.b32 	%r1825, %r1480, %r1481, 1;
	mov.b64 	%rd2498, {%r1825, %r1824};
	shf.r.wrap.b32 	%r1826, %r1481, %r1480, 8;
	shf.r.wrap.b32 	%r1827, %r1480, %r1481, 8;
	mov.b64 	%rd2499, {%r1827, %r1826};
	xor.b64  	%rd2500, %rd2498, %rd2499;
	shr.u64 	%rd2501, %rd1950, 7;
	xor.b64  	%rd2502, %rd2500, %rd2501;
	add.s64 	%rd2503, %rd2497, %rd1913;
	add.s64 	%rd2504, %rd2503, %rd2246;
	add.s64 	%rd2505, %rd2504, %rd2502;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1828,%dummy}, %rd2491;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1829}, %rd2491;
	}
	shf.r.wrap.b32 	%r1830, %r1829, %r1828, 14;
	shf.r.wrap.b32 	%r1831, %r1828, %r1829, 14;
	mov.b64 	%rd2506, {%r1831, %r1830};
	shf.r.wrap.b32 	%r1832, %r1829, %r1828, 18;
	shf.r.wrap.b32 	%r1833, %r1828, %r1829, 18;
	mov.b64 	%rd2507, {%r1833, %r1832};
	xor.b64  	%rd2508, %rd2506, %rd2507;
	shf.l.wrap.b32 	%r1834, %r1828, %r1829, 23;
	shf.l.wrap.b32 	%r1835, %r1829, %r1828, 23;
	mov.b64 	%rd2509, {%r1835, %r1834};
	xor.b64  	%rd2510, %rd2508, %rd2509;
	xor.b64  	%rd2511, %rd2417, %rd2454;
	and.b64  	%rd2512, %rd2511, %rd2491;
	xor.b64  	%rd2513, %rd2512, %rd2417;
	add.s64 	%rd2514, %rd2510, %rd2380;
	add.s64 	%rd2515, %rd2514, %rd2513;
	add.s64 	%rd2516, %rd2515, %rd2505;
	add.s64 	%rd2517, %rd2516, 1977374033974150939;
	and.b64  	%rd2518, %rd2492, %rd2455;
	or.b64  	%rd2519, %rd2492, %rd2455;
	and.b64  	%rd2520, %rd2519, %rd2418;
	or.b64  	%rd2521, %rd2520, %rd2518;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1836,%dummy}, %rd2492;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1837}, %rd2492;
	}
	shf.r.wrap.b32 	%r1838, %r1837, %r1836, 28;
	shf.r.wrap.b32 	%r1839, %r1836, %r1837, 28;
	mov.b64 	%rd2522, {%r1839, %r1838};
	shf.l.wrap.b32 	%r1840, %r1836, %r1837, 30;
	shf.l.wrap.b32 	%r1841, %r1837, %r1836, 30;
	mov.b64 	%rd2523, {%r1841, %r1840};
	xor.b64  	%rd2524, %rd2522, %rd2523;
	shf.l.wrap.b32 	%r1842, %r1836, %r1837, 25;
	shf.l.wrap.b32 	%r1843, %r1837, %r1836, 25;
	mov.b64 	%rd2525, {%r1843, %r1842};
	xor.b64  	%rd2526, %rd2524, %rd2525;
	add.s64 	%rd2527, %rd2521, %rd2526;
	add.s64 	%rd2528, %rd2517, %rd2381;
	add.s64 	%rd2529, %rd2527, %rd2517;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1844,%dummy}, %rd2468;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1845}, %rd2468;
	}
	shf.r.wrap.b32 	%r1846, %r1845, %r1844, 19;
	shf.r.wrap.b32 	%r1847, %r1844, %r1845, 19;
	mov.b64 	%rd2530, {%r1847, %r1846};
	shf.l.wrap.b32 	%r1848, %r1844, %r1845, 3;
	shf.l.wrap.b32 	%r1849, %r1845, %r1844, 3;
	mov.b64 	%rd2531, {%r1849, %r1848};
	xor.b64  	%rd2532, %rd2530, %rd2531;
	shr.u64 	%rd2533, %rd2468, 6;
	xor.b64  	%rd2534, %rd2532, %rd2533;
	shf.r.wrap.b32 	%r1850, %r1507, %r1506, 1;
	shf.r.wrap.b32 	%r1851, %r1506, %r1507, 1;
	mov.b64 	%rd2535, {%r1851, %r1850};
	shf.r.wrap.b32 	%r1852, %r1507, %r1506, 8;
	shf.r.wrap.b32 	%r1853, %r1506, %r1507, 8;
	mov.b64 	%rd2536, {%r1853, %r1852};
	xor.b64  	%rd2537, %rd2535, %rd2536;
	shr.u64 	%rd2538, %rd1987, 7;
	xor.b64  	%rd2539, %rd2537, %rd2538;
	add.s64 	%rd2540, %rd2534, %rd1950;
	add.s64 	%rd2541, %rd2540, %rd2283;
	add.s64 	%rd2542, %rd2541, %rd2539;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1854,%dummy}, %rd2528;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1855}, %rd2528;
	}
	shf.r.wrap.b32 	%r1856, %r1855, %r1854, 14;
	shf.r.wrap.b32 	%r1857, %r1854, %r1855, 14;
	mov.b64 	%rd2543, {%r1857, %r1856};
	shf.r.wrap.b32 	%r1858, %r1855, %r1854, 18;
	shf.r.wrap.b32 	%r1859, %r1854, %r1855, 18;
	mov.b64 	%rd2544, {%r1859, %r1858};
	xor.b64  	%rd2545, %rd2543, %rd2544;
	shf.l.wrap.b32 	%r1860, %r1854, %r1855, 23;
	shf.l.wrap.b32 	%r1861, %r1855, %r1854, 23;
	mov.b64 	%rd2546, {%r1861, %r1860};
	xor.b64  	%rd2547, %rd2545, %rd2546;
	xor.b64  	%rd2548, %rd2454, %rd2491;
	and.b64  	%rd2549, %rd2548, %rd2528;
	xor.b64  	%rd2550, %rd2549, %rd2454;
	add.s64 	%rd2551, %rd2547, %rd2417;
	add.s64 	%rd2552, %rd2551, %rd2550;
	add.s64 	%rd2553, %rd2552, %rd2542;
	add.s64 	%rd2554, %rd2553, 2944078676154940804;
	and.b64  	%rd2555, %rd2529, %rd2492;
	or.b64  	%rd2556, %rd2529, %rd2492;
	and.b64  	%rd2557, %rd2556, %rd2455;
	or.b64  	%rd2558, %rd2557, %rd2555;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1862,%dummy}, %rd2529;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1863}, %rd2529;
	}
	shf.r.wrap.b32 	%r1864, %r1863, %r1862, 28;
	shf.r.wrap.b32 	%r1865, %r1862, %r1863, 28;
	mov.b64 	%rd2559, {%r1865, %r1864};
	shf.l.wrap.b32 	%r1866, %r1862, %r1863, 30;
	shf.l.wrap.b32 	%r1867, %r1863, %r1862, 30;
	mov.b64 	%rd2560, {%r1867, %r1866};
	xor.b64  	%rd2561, %rd2559, %rd2560;
	shf.l.wrap.b32 	%r1868, %r1862, %r1863, 25;
	shf.l.wrap.b32 	%r1869, %r1863, %r1862, 25;
	mov.b64 	%rd2562, {%r1869, %r1868};
	xor.b64  	%rd2563, %rd2561, %rd2562;
	add.s64 	%rd2564, %rd2558, %rd2563;
	add.s64 	%rd2565, %rd2554, %rd2418;
	add.s64 	%rd2566, %rd2564, %rd2554;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1870,%dummy}, %rd2505;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1871}, %rd2505;
	}
	shf.r.wrap.b32 	%r1872, %r1871, %r1870, 19;
	shf.r.wrap.b32 	%r1873, %r1870, %r1871, 19;
	mov.b64 	%rd2567, {%r1873, %r1872};
	shf.l.wrap.b32 	%r1874, %r1870, %r1871, 3;
	shf.l.wrap.b32 	%r1875, %r1871, %r1870, 3;
	mov.b64 	%rd2568, {%r1875, %r1874};
	xor.b64  	%rd2569, %rd2567, %rd2568;
	shr.u64 	%rd2570, %rd2505, 6;
	xor.b64  	%rd2571, %rd2569, %rd2570;
	shf.r.wrap.b32 	%r1876, %r1533, %r1532, 1;
	shf.r.wrap.b32 	%r1877, %r1532, %r1533, 1;
	mov.b64 	%rd2572, {%r1877, %r1876};
	shf.r.wrap.b32 	%r1878, %r1533, %r1532, 8;
	shf.r.wrap.b32 	%r1879, %r1532, %r1533, 8;
	mov.b64 	%rd2573, {%r1879, %r1878};
	xor.b64  	%rd2574, %rd2572, %rd2573;
	shr.u64 	%rd2575, %rd2024, 7;
	xor.b64  	%rd2576, %rd2574, %rd2575;
	add.s64 	%rd2577, %rd2571, %rd1987;
	add.s64 	%rd2578, %rd2577, %rd2320;
	add.s64 	%rd2579, %rd2578, %rd2576;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1880,%dummy}, %rd2565;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1881}, %rd2565;
	}
	shf.r.wrap.b32 	%r1882, %r1881, %r1880, 14;
	shf.r.wrap.b32 	%r1883, %r1880, %r1881, 14;
	mov.b64 	%rd2580, {%r1883, %r1882};
	shf.r.wrap.b32 	%r1884, %r1881, %r1880, 18;
	shf.r.wrap.b32 	%r1885, %r1880, %r1881, 18;
	mov.b64 	%rd2581, {%r1885, %r1884};
	xor.b64  	%rd2582, %rd2580, %rd2581;
	shf.l.wrap.b32 	%r1886, %r1880, %r1881, 23;
	shf.l.wrap.b32 	%r1887, %r1881, %r1880, 23;
	mov.b64 	%rd2583, {%r1887, %r1886};
	xor.b64  	%rd2584, %rd2582, %rd2583;
	xor.b64  	%rd2585, %rd2491, %rd2528;
	and.b64  	%rd2586, %rd2585, %rd2565;
	xor.b64  	%rd2587, %rd2586, %rd2491;
	add.s64 	%rd2588, %rd2584, %rd2454;
	add.s64 	%rd2589, %rd2588, %rd2587;
	add.s64 	%rd2590, %rd2589, %rd2579;
	add.s64 	%rd2591, %rd2590, 3659926193048069267;
	and.b64  	%rd2592, %rd2566, %rd2529;
	or.b64  	%rd2593, %rd2566, %rd2529;
	and.b64  	%rd2594, %rd2593, %rd2492;
	or.b64  	%rd2595, %rd2594, %rd2592;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1888,%dummy}, %rd2566;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1889}, %rd2566;
	}
	shf.r.wrap.b32 	%r1890, %r1889, %r1888, 28;
	shf.r.wrap.b32 	%r1891, %r1888, %r1889, 28;
	mov.b64 	%rd2596, {%r1891, %r1890};
	shf.l.wrap.b32 	%r1892, %r1888, %r1889, 30;
	shf.l.wrap.b32 	%r1893, %r1889, %r1888, 30;
	mov.b64 	%rd2597, {%r1893, %r1892};
	xor.b64  	%rd2598, %rd2596, %rd2597;
	shf.l.wrap.b32 	%r1894, %r1888, %r1889, 25;
	shf.l.wrap.b32 	%r1895, %r1889, %r1888, 25;
	mov.b64 	%rd2599, {%r1895, %r1894};
	xor.b64  	%rd2600, %rd2598, %rd2599;
	add.s64 	%rd2601, %rd2595, %rd2600;
	add.s64 	%rd2602, %rd2591, %rd2455;
	add.s64 	%rd2603, %rd2601, %rd2591;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1896,%dummy}, %rd2542;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1897}, %rd2542;
	}
	shf.r.wrap.b32 	%r1898, %r1897, %r1896, 19;
	shf.r.wrap.b32 	%r1899, %r1896, %r1897, 19;
	mov.b64 	%rd2604, {%r1899, %r1898};
	shf.l.wrap.b32 	%r1900, %r1896, %r1897, 3;
	shf.l.wrap.b32 	%r1901, %r1897, %r1896, 3;
	mov.b64 	%rd2605, {%r1901, %r1900};
	xor.b64  	%rd2606, %rd2604, %rd2605;
	shr.u64 	%rd2607, %rd2542, 6;
	xor.b64  	%rd2608, %rd2606, %rd2607;
	shf.r.wrap.b32 	%r1902, %r1559, %r1558, 1;
	shf.r.wrap.b32 	%r1903, %r1558, %r1559, 1;
	mov.b64 	%rd2609, {%r1903, %r1902};
	shf.r.wrap.b32 	%r1904, %r1559, %r1558, 8;
	shf.r.wrap.b32 	%r1905, %r1558, %r1559, 8;
	mov.b64 	%rd2610, {%r1905, %r1904};
	xor.b64  	%rd2611, %rd2609, %rd2610;
	shr.u64 	%rd2612, %rd2061, 7;
	xor.b64  	%rd2613, %rd2611, %rd2612;
	add.s64 	%rd2614, %rd2608, %rd2024;
	add.s64 	%rd2615, %rd2614, %rd2357;
	add.s64 	%rd2616, %rd2615, %rd2613;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1906,%dummy}, %rd2602;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1907}, %rd2602;
	}
	shf.r.wrap.b32 	%r1908, %r1907, %r1906, 14;
	shf.r.wrap.b32 	%r1909, %r1906, %r1907, 14;
	mov.b64 	%rd2617, {%r1909, %r1908};
	shf.r.wrap.b32 	%r1910, %r1907, %r1906, 18;
	shf.r.wrap.b32 	%r1911, %r1906, %r1907, 18;
	mov.b64 	%rd2618, {%r1911, %r1910};
	xor.b64  	%rd2619, %rd2617, %rd2618;
	shf.l.wrap.b32 	%r1912, %r1906, %r1907, 23;
	shf.l.wrap.b32 	%r1913, %r1907, %r1906, 23;
	mov.b64 	%rd2620, {%r1913, %r1912};
	xor.b64  	%rd2621, %rd2619, %rd2620;
	xor.b64  	%rd2622, %rd2528, %rd2565;
	and.b64  	%rd2623, %rd2622, %rd2602;
	xor.b64  	%rd2624, %rd2623, %rd2528;
	add.s64 	%rd2625, %rd2621, %rd2491;
	add.s64 	%rd2626, %rd2625, %rd2624;
	add.s64 	%rd2627, %rd2626, %rd2616;
	add.s64 	%rd2628, %rd2627, 4368137639120453308;
	and.b64  	%rd2629, %rd2603, %rd2566;
	or.b64  	%rd2630, %rd2603, %rd2566;
	and.b64  	%rd2631, %rd2630, %rd2529;
	or.b64  	%rd2632, %rd2631, %rd2629;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1914,%dummy}, %rd2603;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1915}, %rd2603;
	}
	shf.r.wrap.b32 	%r1916, %r1915, %r1914, 28;
	shf.r.wrap.b32 	%r1917, %r1914, %r1915, 28;
	mov.b64 	%rd2633, {%r1917, %r1916};
	shf.l.wrap.b32 	%r1918, %r1914, %r1915, 30;
	shf.l.wrap.b32 	%r1919, %r1915, %r1914, 30;
	mov.b64 	%rd2634, {%r1919, %r1918};
	xor.b64  	%rd2635, %rd2633, %rd2634;
	shf.l.wrap.b32 	%r1920, %r1914, %r1915, 25;
	shf.l.wrap.b32 	%r1921, %r1915, %r1914, 25;
	mov.b64 	%rd2636, {%r1921, %r1920};
	xor.b64  	%rd2637, %rd2635, %rd2636;
	add.s64 	%rd2638, %rd2632, %rd2637;
	add.s64 	%rd2639, %rd2628, %rd2492;
	add.s64 	%rd2640, %rd2638, %rd2628;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1922,%dummy}, %rd2579;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1923}, %rd2579;
	}
	shf.r.wrap.b32 	%r1924, %r1923, %r1922, 19;
	shf.r.wrap.b32 	%r1925, %r1922, %r1923, 19;
	mov.b64 	%rd2641, {%r1925, %r1924};
	shf.l.wrap.b32 	%r1926, %r1922, %r1923, 3;
	shf.l.wrap.b32 	%r1927, %r1923, %r1922, 3;
	mov.b64 	%rd2642, {%r1927, %r1926};
	xor.b64  	%rd2643, %rd2641, %rd2642;
	shr.u64 	%rd2644, %rd2579, 6;
	xor.b64  	%rd2645, %rd2643, %rd2644;
	shf.r.wrap.b32 	%r1928, %r1585, %r1584, 1;
	shf.r.wrap.b32 	%r1929, %r1584, %r1585, 1;
	mov.b64 	%rd2646, {%r1929, %r1928};
	shf.r.wrap.b32 	%r1930, %r1585, %r1584, 8;
	shf.r.wrap.b32 	%r1931, %r1584, %r1585, 8;
	mov.b64 	%rd2647, {%r1931, %r1930};
	xor.b64  	%rd2648, %rd2646, %rd2647;
	shr.u64 	%rd2649, %rd2098, 7;
	xor.b64  	%rd2650, %rd2648, %rd2649;
	add.s64 	%rd2651, %rd2645, %rd2061;
	add.s64 	%rd2652, %rd2651, %rd2394;
	add.s64 	%rd2653, %rd2652, %rd2650;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1932,%dummy}, %rd2639;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1933}, %rd2639;
	}
	shf.r.wrap.b32 	%r1934, %r1933, %r1932, 14;
	shf.r.wrap.b32 	%r1935, %r1932, %r1933, 14;
	mov.b64 	%rd2654, {%r1935, %r1934};
	shf.r.wrap.b32 	%r1936, %r1933, %r1932, 18;
	shf.r.wrap.b32 	%r1937, %r1932, %r1933, 18;
	mov.b64 	%rd2655, {%r1937, %r1936};
	xor.b64  	%rd2656, %rd2654, %rd2655;
	shf.l.wrap.b32 	%r1938, %r1932, %r1933, 23;
	shf.l.wrap.b32 	%r1939, %r1933, %r1932, 23;
	mov.b64 	%rd2657, {%r1939, %r1938};
	xor.b64  	%rd2658, %rd2656, %rd2657;
	xor.b64  	%rd2659, %rd2565, %rd2602;
	and.b64  	%rd2660, %rd2659, %rd2639;
	xor.b64  	%rd2661, %rd2660, %rd2565;
	add.s64 	%rd2662, %rd2658, %rd2528;
	add.s64 	%rd2663, %rd2662, %rd2661;
	add.s64 	%rd2664, %rd2663, %rd2653;
	add.s64 	%rd2665, %rd2664, 4836135668995329356;
	and.b64  	%rd2666, %rd2640, %rd2603;
	or.b64  	%rd2667, %rd2640, %rd2603;
	and.b64  	%rd2668, %rd2667, %rd2566;
	or.b64  	%rd2669, %rd2668, %rd2666;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1940,%dummy}, %rd2640;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1941}, %rd2640;
	}
	shf.r.wrap.b32 	%r1942, %r1941, %r1940, 28;
	shf.r.wrap.b32 	%r1943, %r1940, %r1941, 28;
	mov.b64 	%rd2670, {%r1943, %r1942};
	shf.l.wrap.b32 	%r1944, %r1940, %r1941, 30;
	shf.l.wrap.b32 	%r1945, %r1941, %r1940, 30;
	mov.b64 	%rd2671, {%r1945, %r1944};
	xor.b64  	%rd2672, %rd2670, %rd2671;
	shf.l.wrap.b32 	%r1946, %r1940, %r1941, 25;
	shf.l.wrap.b32 	%r1947, %r1941, %r1940, 25;
	mov.b64 	%rd2673, {%r1947, %r1946};
	xor.b64  	%rd2674, %rd2672, %rd2673;
	add.s64 	%rd2675, %rd2669, %rd2674;
	add.s64 	%rd2676, %rd2665, %rd2529;
	add.s64 	%rd2677, %rd2675, %rd2665;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1948,%dummy}, %rd2616;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1949}, %rd2616;
	}
	shf.r.wrap.b32 	%r1950, %r1949, %r1948, 19;
	shf.r.wrap.b32 	%r1951, %r1948, %r1949, 19;
	mov.b64 	%rd2678, {%r1951, %r1950};
	shf.l.wrap.b32 	%r1952, %r1948, %r1949, 3;
	shf.l.wrap.b32 	%r1953, %r1949, %r1948, 3;
	mov.b64 	%rd2679, {%r1953, %r1952};
	xor.b64  	%rd2680, %rd2678, %rd2679;
	shr.u64 	%rd2681, %rd2616, 6;
	xor.b64  	%rd2682, %rd2680, %rd2681;
	shf.r.wrap.b32 	%r1954, %r1611, %r1610, 1;
	shf.r.wrap.b32 	%r1955, %r1610, %r1611, 1;
	mov.b64 	%rd2683, {%r1955, %r1954};
	shf.r.wrap.b32 	%r1956, %r1611, %r1610, 8;
	shf.r.wrap.b32 	%r1957, %r1610, %r1611, 8;
	mov.b64 	%rd2684, {%r1957, %r1956};
	xor.b64  	%rd2685, %rd2683, %rd2684;
	shr.u64 	%rd2686, %rd2135, 7;
	xor.b64  	%rd2687, %rd2685, %rd2686;
	add.s64 	%rd2688, %rd2682, %rd2098;
	add.s64 	%rd2689, %rd2688, %rd2431;
	add.s64 	%rd2690, %rd2689, %rd2687;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1958,%dummy}, %rd2676;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1959}, %rd2676;
	}
	shf.r.wrap.b32 	%r1960, %r1959, %r1958, 14;
	shf.r.wrap.b32 	%r1961, %r1958, %r1959, 14;
	mov.b64 	%rd2691, {%r1961, %r1960};
	shf.r.wrap.b32 	%r1962, %r1959, %r1958, 18;
	shf.r.wrap.b32 	%r1963, %r1958, %r1959, 18;
	mov.b64 	%rd2692, {%r1963, %r1962};
	xor.b64  	%rd2693, %rd2691, %rd2692;
	shf.l.wrap.b32 	%r1964, %r1958, %r1959, 23;
	shf.l.wrap.b32 	%r1965, %r1959, %r1958, 23;
	mov.b64 	%rd2694, {%r1965, %r1964};
	xor.b64  	%rd2695, %rd2693, %rd2694;
	xor.b64  	%rd2696, %rd2602, %rd2639;
	and.b64  	%rd2697, %rd2696, %rd2676;
	xor.b64  	%rd2698, %rd2697, %rd2602;
	add.s64 	%rd2699, %rd2695, %rd2565;
	add.s64 	%rd2700, %rd2699, %rd2698;
	add.s64 	%rd2701, %rd2700, %rd2690;
	and.b64  	%rd2702, %rd2677, %rd2640;
	or.b64  	%rd2703, %rd2677, %rd2640;
	and.b64  	%rd2704, %rd2703, %rd2603;
	or.b64  	%rd2705, %rd2704, %rd2702;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1966,%dummy}, %rd2677;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1967}, %rd2677;
	}
	shf.r.wrap.b32 	%r1968, %r1967, %r1966, 28;
	shf.r.wrap.b32 	%r1969, %r1966, %r1967, 28;
	mov.b64 	%rd2706, {%r1969, %r1968};
	shf.l.wrap.b32 	%r1970, %r1966, %r1967, 30;
	shf.l.wrap.b32 	%r1971, %r1967, %r1966, 30;
	mov.b64 	%rd2707, {%r1971, %r1970};
	xor.b64  	%rd2708, %rd2706, %rd2707;
	shf.l.wrap.b32 	%r1972, %r1966, %r1967, 25;
	shf.l.wrap.b32 	%r1973, %r1967, %r1966, 25;
	mov.b64 	%rd2709, {%r1973, %r1972};
	xor.b64  	%rd2710, %rd2708, %rd2709;
	add.s64 	%rd2711, %rd2705, %rd2710;
	add.s64 	%rd2712, %rd2711, %rd2701;
	add.s64 	%rd2713, %rd2712, 5532061633213252278;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1974, %temp}, %rd2713;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1975}, %rd2713;
	}
	prmt.b32 	%r1976, %r1974, %r47, %r46;
	prmt.b32 	%r1977, %r1975, %r47, %r46;
	mov.b64 	%rd2714, {%r1977, %r1976};
	shl.b64 	%rd2715, %rd2, 3;
	add.s64 	%rd2716, %rd18, %rd2715;
	st.global.u64 	[%rd2716], %rd2714;
	ret;

}
	// .globl	kernel_cmp
.entry kernel_cmp(
	.param .u64 .ptr .const .align 8 kernel_cmp_param_0,
	.param .u64 .ptr .global .align 8 kernel_cmp_param_1,
	.param .u64 .ptr .global .align 4 kernel_cmp_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [kernel_cmp_param_0];
	ld.param.u64 	%rd2, [kernel_cmp_param_1];
	ld.param.u64 	%rd3, [kernel_cmp_param_2];
	mov.b32 	%r2, %envreg3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r5, %r2;
	mad.lo.s32 	%r1, %r4, %r3, %r6;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB1_2;

	mov.u32 	%r7, 0;
	st.global.u32 	[%rd3], %r7;

$L__BB1_2:
	bar.sync 	0;
	mul.wide.u32 	%rd4, %r1, 8;
	add.s64 	%rd5, %rd2, %rd4;
	ld.global.u64 	%rd6, [%rd5];
	ld.const.u64 	%rd7, [%rd1];
	setp.ne.s64 	%p2, %rd7, %rd6;
	@%p2 bra 	$L__BB1_4;

	mov.u32 	%r8, 1;
	st.global.u32 	[%rd3], %r8;

$L__BB1_4:
	ret;

}

  