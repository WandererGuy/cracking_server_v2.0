//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.6
.target sm_86, texmode_independent
.address_size 64

	// .globl	pre_processing

.entry pre_processing(
	.param .u64 .ptr .global .align 4 pre_processing_param_0,
	.param .u64 .ptr .global .align 8 pre_processing_param_1,
	.param .u32 pre_processing_param_2,
	.param .u32 pre_processing_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<1368>;
	.reg .b64 	%rd<1357>;


	ld.param.u64 	%rd33, [pre_processing_param_0];
	ld.param.u64 	%rd34, [pre_processing_param_1];
	ld.param.u32 	%r6, [pre_processing_param_2];
	ld.param.u32 	%rd35, [pre_processing_param_3];
	mov.b32 	%r7, %envreg4;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r9, %tid.y;
	add.s32 	%r10, %r9, %r7;
	mad.lo.s32 	%r11, %r1, %r8, %r10;
	cvt.s64.s32 	%rd1346, %r11;
	mov.b32 	%r2, %envreg7;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %tid.x;
	mov.b32 	%r15, %envreg3;
	add.s32 	%r16, %r14, %r15;
	mad.lo.s32 	%r3, %r13, %r12, %r16;
	cvt.s64.s32 	%rd36, %r3;
	shr.u64 	%rd2, %rd36, 1;
	setp.ge.u64 	%p1, %rd2, %rd35;
	@%p1 bra 	$L__BB0_4;

	and.b32  	%r18, %r3, 1;
	shl.b64 	%rd37, %rd2, 6;
	add.s64 	%rd38, %rd33, %rd37;
	shl.b64 	%rd39, %rd1346, 7;
	mul.lo.s32 	%r19, %r1, %r2;
	mul.lo.s32 	%r20, %r19, %r18;
	shl.b32 	%r21, %r20, 7;
	and.b32  	%r22, %r21, 536870784;
	cvt.u64.u32 	%rd40, %r22;
	add.s64 	%rd41, %rd39, %rd40;
	cvt.u64.u32 	%rd42, %r6;
	mul.lo.s64 	%rd43, %rd2, %rd42;
	add.s64 	%rd44, %rd41, %rd43;
	ld.global.u32 	%rd45, [%rd38];
	shl.b64 	%rd46, %rd45, 32;
	or.b64  	%rd1355, %rd46, 1024;
	ld.global.u32 	%rd47, [%rd38+8];
	ld.global.u32 	%rd48, [%rd38+4];
	bfi.b64 	%rd1354, %rd47, %rd48, 32, 32;
	ld.global.u32 	%rd49, [%rd38+16];
	ld.global.u32 	%rd50, [%rd38+12];
	bfi.b64 	%rd1353, %rd49, %rd50, 32, 32;
	ld.global.u32 	%rd51, [%rd38+24];
	ld.global.u32 	%rd52, [%rd38+20];
	bfi.b64 	%rd1352, %rd51, %rd52, 32, 32;
	ld.global.u32 	%rd53, [%rd38+32];
	ld.global.u32 	%rd54, [%rd38+28];
	bfi.b64 	%rd1351, %rd53, %rd54, 32, 32;
	ld.global.u32 	%rd55, [%rd38+40];
	ld.global.u32 	%rd56, [%rd38+36];
	bfi.b64 	%rd1350, %rd55, %rd56, 32, 32;
	ld.global.u32 	%rd57, [%rd38+48];
	ld.global.u32 	%rd58, [%rd38+44];
	bfi.b64 	%rd1349, %rd57, %rd58, 32, 32;
	ld.global.u32 	%rd59, [%rd38+56];
	ld.global.u32 	%rd60, [%rd38+52];
	bfi.b64 	%rd1348, %rd59, %rd60, 32, 32;
	shl.b64 	%rd61, %rd44, 3;
	add.s64 	%rd1356, %rd34, %rd61;
	cvt.u64.u32 	%rd62, %r18;
	ld.global.u32 	%rd63, [%rd38+60];
	bfi.b64 	%rd1347, %rd62, %rd63, 32, 32;
	mov.u32 	%r1367, 0;

$L__BB0_2:
	setp.eq.s32 	%p2, %r1367, 0;
	selp.b64 	%rd66, 5840696475078001309, 5840696475078001297, %p2;
	add.s64 	%rd67, %rd1355, -4965156021692249063;
	xor.b64  	%rd68, %rd66, %rd67;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r23,%dummy}, %rd68;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r24}, %rd68;
	}
	mov.b64 	%rd69, {%r24, %r23};
	add.s64 	%rd70, %rd69, 7640891576956012808;
	xor.b64  	%rd71, %rd70, 5840696475078001361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r25,%dummy}, %rd71;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r26}, %rd71;
	}
	shf.r.wrap.b32 	%r27, %r26, %r25, 24;
	shf.r.wrap.b32 	%r28, %r25, %r26, 24;
	mov.b64 	%rd72, {%r28, %r27};
	add.s64 	%rd73, %rd67, %rd72;
	add.s64 	%rd74, %rd73, %rd1354;
	xor.b64  	%rd75, %rd69, %rd74;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r29,%dummy}, %rd75;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r30}, %rd75;
	}
	shf.r.wrap.b32 	%r31, %r30, %r29, 16;
	shf.r.wrap.b32 	%r32, %r29, %r30, 16;
	mov.b64 	%rd76, {%r32, %r31};
	add.s64 	%rd77, %rd70, %rd76;
	xor.b64  	%rd78, %rd72, %rd77;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r33}, %rd78;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r34,%dummy}, %rd78;
	}
	shf.l.wrap.b32 	%r35, %r34, %r33, 1;
	shf.l.wrap.b32 	%r36, %r33, %r34, 1;
	mov.b64 	%rd79, {%r36, %r35};
	add.s64 	%rd80, %rd1353, 6227659224458531674;
	xor.b64  	%rd81, %rd80, -7276294671716946913;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r37,%dummy}, %rd81;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r38}, %rd81;
	}
	mov.b64 	%rd82, {%r38, %r37};
	add.s64 	%rd83, %rd82, -4942790177534073029;
	xor.b64  	%rd84, %rd83, -7276294671716946913;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r39,%dummy}, %rd84;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r40}, %rd84;
	}
	shf.r.wrap.b32 	%r41, %r40, %r39, 24;
	shf.r.wrap.b32 	%r42, %r39, %r40, 24;
	mov.b64 	%rd85, {%r42, %r41};
	add.s64 	%rd86, %rd80, %rd85;
	add.s64 	%rd87, %rd86, %rd1352;
	xor.b64  	%rd88, %rd82, %rd87;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r43,%dummy}, %rd88;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r44}, %rd88;
	}
	shf.r.wrap.b32 	%r45, %r44, %r43, 16;
	shf.r.wrap.b32 	%r46, %r43, %r44, 16;
	mov.b64 	%rd89, {%r46, %r45};
	add.s64 	%rd90, %rd83, %rd89;
	xor.b64  	%rd91, %rd85, %rd90;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r47}, %rd91;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r48,%dummy}, %rd91;
	}
	shf.l.wrap.b32 	%r49, %r48, %r47, 1;
	shf.l.wrap.b32 	%r50, %r47, %r48, 1;
	mov.b64 	%rd92, {%r50, %r49};
	add.s64 	%rd93, %rd1351, 6625583534739731862;
	xor.b64  	%rd94, %rd93, -2270897969802886508;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r51,%dummy}, %rd94;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r52}, %rd94;
	}
	mov.b64 	%rd95, {%r52, %r51};
	add.s64 	%rd96, %rd95, 4354685564936845355;
	xor.b64  	%rd97, %rd96, 2270897969802886507;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r53,%dummy}, %rd97;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r54}, %rd97;
	}
	shf.r.wrap.b32 	%r55, %r54, %r53, 24;
	shf.r.wrap.b32 	%r56, %r53, %r54, 24;
	mov.b64 	%rd98, {%r56, %r55};
	add.s64 	%rd99, %rd93, %rd98;
	add.s64 	%rd100, %rd99, %rd1350;
	xor.b64  	%rd101, %rd95, %rd100;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r57,%dummy}, %rd101;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r58}, %rd101;
	}
	shf.r.wrap.b32 	%r59, %r58, %r57, 16;
	shf.r.wrap.b32 	%r60, %r57, %r58, 16;
	mov.b64 	%rd102, {%r60, %r59};
	add.s64 	%rd103, %rd96, %rd102;
	xor.b64  	%rd104, %rd98, %rd103;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r61}, %rd104;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r62,%dummy}, %rd104;
	}
	shf.l.wrap.b32 	%r63, %r62, %r61, 1;
	shf.l.wrap.b32 	%r64, %r61, %r62, 1;
	mov.b64 	%rd105, {%r64, %r63};
	add.s64 	%rd106, %rd1349, 85782056580896874;
	xor.b64  	%rd107, %rd106, 6620516959819538809;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r65,%dummy}, %rd107;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r66}, %rd107;
	}
	mov.b64 	%rd108, {%r66, %r65};
	add.s64 	%rd109, %rd108, -6534734903238641935;
	xor.b64  	%rd110, %rd109, 6620516959819538809;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r67,%dummy}, %rd110;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r68}, %rd110;
	}
	shf.r.wrap.b32 	%r69, %r68, %r67, 24;
	shf.r.wrap.b32 	%r70, %r67, %r68, 24;
	mov.b64 	%rd111, {%r70, %r69};
	add.s64 	%rd112, %rd106, %rd111;
	add.s64 	%rd113, %rd112, %rd1348;
	xor.b64  	%rd114, %rd108, %rd113;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r71,%dummy}, %rd114;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r72}, %rd114;
	}
	shf.r.wrap.b32 	%r73, %r72, %r71, 16;
	shf.r.wrap.b32 	%r74, %r71, %r72, 16;
	mov.b64 	%rd115, {%r74, %r73};
	add.s64 	%rd116, %rd109, %rd115;
	xor.b64  	%rd117, %rd111, %rd116;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r75}, %rd117;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r76,%dummy}, %rd117;
	}
	shf.l.wrap.b32 	%r77, %r76, %r75, 1;
	shf.l.wrap.b32 	%r78, %r75, %r76, 1;
	mov.b64 	%rd118, {%r78, %r77};
	add.s64 	%rd119, %rd92, %rd74;
	add.s64 	%rd120, %rd119, %rd1347;
	xor.b64  	%rd121, %rd115, %rd120;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r79,%dummy}, %rd121;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r80}, %rd121;
	}
	mov.b64 	%rd122, {%r80, %r79};
	add.s64 	%rd123, %rd103, %rd122;
	xor.b64  	%rd124, %rd92, %rd123;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r81,%dummy}, %rd124;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r82}, %rd124;
	}
	shf.r.wrap.b32 	%r83, %r82, %r81, 24;
	shf.r.wrap.b32 	%r84, %r81, %r82, 24;
	mov.b64 	%rd125, {%r84, %r83};
	add.s64 	%rd126, %rd120, %rd125;
	add.s64 	%rd127, %rd126, %rd1346;
	xor.b64  	%rd128, %rd122, %rd127;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r85,%dummy}, %rd128;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r86}, %rd128;
	}
	shf.r.wrap.b32 	%r87, %r86, %r85, 16;
	shf.r.wrap.b32 	%r88, %r85, %r86, 16;
	mov.b64 	%rd129, {%r88, %r87};
	add.s64 	%rd130, %rd123, %rd129;
	xor.b64  	%rd131, %rd125, %rd130;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r89}, %rd131;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r90,%dummy}, %rd131;
	}
	shf.l.wrap.b32 	%r91, %r90, %r89, 1;
	shf.l.wrap.b32 	%r92, %r89, %r90, 1;
	mov.b64 	%rd132, {%r92, %r91};
	add.s64 	%rd133, %rd105, %rd87;
	xor.b64  	%rd134, %rd76, %rd133;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r93,%dummy}, %rd134;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r94}, %rd134;
	}
	mov.b64 	%rd135, {%r94, %r93};
	add.s64 	%rd136, %rd116, %rd135;
	xor.b64  	%rd137, %rd105, %rd136;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r95,%dummy}, %rd137;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r96}, %rd137;
	}
	shf.r.wrap.b32 	%r97, %r96, %r95, 24;
	shf.r.wrap.b32 	%r98, %r95, %r96, 24;
	mov.b64 	%rd138, {%r98, %r97};
	add.s64 	%rd139, %rd133, %rd138;
	xor.b64  	%rd140, %rd135, %rd139;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r99,%dummy}, %rd140;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r100}, %rd140;
	}
	shf.r.wrap.b32 	%r101, %r100, %r99, 16;
	shf.r.wrap.b32 	%r102, %r99, %r100, 16;
	mov.b64 	%rd141, {%r102, %r101};
	add.s64 	%rd142, %rd136, %rd141;
	xor.b64  	%rd143, %rd138, %rd142;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r103}, %rd143;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r104,%dummy}, %rd143;
	}
	shf.l.wrap.b32 	%r105, %r104, %r103, 1;
	shf.l.wrap.b32 	%r106, %r103, %r104, 1;
	mov.b64 	%rd144, {%r106, %r105};
	add.s64 	%rd145, %rd118, %rd100;
	xor.b64  	%rd146, %rd89, %rd145;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r107,%dummy}, %rd146;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r108}, %rd146;
	}
	mov.b64 	%rd147, {%r108, %r107};
	add.s64 	%rd148, %rd77, %rd147;
	xor.b64  	%rd149, %rd118, %rd148;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r109,%dummy}, %rd149;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r110}, %rd149;
	}
	shf.r.wrap.b32 	%r111, %r110, %r109, 24;
	shf.r.wrap.b32 	%r112, %r109, %r110, 24;
	mov.b64 	%rd150, {%r112, %r111};
	add.s64 	%rd151, %rd145, %rd150;
	xor.b64  	%rd152, %rd147, %rd151;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r113,%dummy}, %rd152;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r114}, %rd152;
	}
	shf.r.wrap.b32 	%r115, %r114, %r113, 16;
	shf.r.wrap.b32 	%r116, %r113, %r114, 16;
	mov.b64 	%rd153, {%r116, %r115};
	add.s64 	%rd154, %rd148, %rd153;
	xor.b64  	%rd155, %rd150, %rd154;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r117}, %rd155;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r118,%dummy}, %rd155;
	}
	shf.l.wrap.b32 	%r119, %r118, %r117, 1;
	shf.l.wrap.b32 	%r120, %r117, %r118, 1;
	mov.b64 	%rd156, {%r120, %r119};
	add.s64 	%rd157, %rd79, %rd113;
	xor.b64  	%rd158, %rd102, %rd157;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r121,%dummy}, %rd158;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r122}, %rd158;
	}
	mov.b64 	%rd159, {%r122, %r121};
	add.s64 	%rd160, %rd90, %rd159;
	xor.b64  	%rd161, %rd79, %rd160;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r123,%dummy}, %rd161;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r124}, %rd161;
	}
	shf.r.wrap.b32 	%r125, %r124, %r123, 24;
	shf.r.wrap.b32 	%r126, %r123, %r124, 24;
	mov.b64 	%rd162, {%r126, %r125};
	add.s64 	%rd163, %rd157, %rd162;
	xor.b64  	%rd164, %rd159, %rd163;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r127,%dummy}, %rd164;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r128}, %rd164;
	}
	shf.r.wrap.b32 	%r129, %r128, %r127, 16;
	shf.r.wrap.b32 	%r130, %r127, %r128, 16;
	mov.b64 	%rd165, {%r130, %r129};
	add.s64 	%rd166, %rd160, %rd165;
	xor.b64  	%rd167, %rd162, %rd166;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r131}, %rd167;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r132,%dummy}, %rd167;
	}
	shf.l.wrap.b32 	%r133, %r132, %r131, 1;
	shf.l.wrap.b32 	%r134, %r131, %r132, 1;
	mov.b64 	%rd168, {%r134, %r133};
	add.s64 	%rd169, %rd168, %rd127;
	xor.b64  	%rd170, %rd141, %rd169;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r135,%dummy}, %rd170;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r136}, %rd170;
	}
	mov.b64 	%rd171, {%r136, %r135};
	add.s64 	%rd172, %rd154, %rd171;
	xor.b64  	%rd173, %rd168, %rd172;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r137,%dummy}, %rd173;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r138}, %rd173;
	}
	shf.r.wrap.b32 	%r139, %r138, %r137, 24;
	shf.r.wrap.b32 	%r140, %r137, %r138, 24;
	mov.b64 	%rd174, {%r140, %r139};
	add.s64 	%rd175, %rd169, %rd174;
	xor.b64  	%rd176, %rd171, %rd175;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r141,%dummy}, %rd176;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r142}, %rd176;
	}
	shf.r.wrap.b32 	%r143, %r142, %r141, 16;
	shf.r.wrap.b32 	%r144, %r141, %r142, 16;
	mov.b64 	%rd177, {%r144, %r143};
	add.s64 	%rd178, %rd172, %rd177;
	xor.b64  	%rd179, %rd174, %rd178;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r145}, %rd179;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r146,%dummy}, %rd179;
	}
	shf.l.wrap.b32 	%r147, %r146, %r145, 1;
	shf.l.wrap.b32 	%r148, %r145, %r146, 1;
	mov.b64 	%rd180, {%r148, %r147};
	add.s64 	%rd181, %rd132, %rd139;
	add.s64 	%rd182, %rd181, %rd1351;
	xor.b64  	%rd183, %rd153, %rd182;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r149,%dummy}, %rd183;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r150}, %rd183;
	}
	mov.b64 	%rd184, {%r150, %r149};
	add.s64 	%rd185, %rd166, %rd184;
	xor.b64  	%rd186, %rd132, %rd185;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r151,%dummy}, %rd186;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r152}, %rd186;
	}
	shf.r.wrap.b32 	%r153, %r152, %r151, 24;
	shf.r.wrap.b32 	%r154, %r151, %r152, 24;
	mov.b64 	%rd187, {%r154, %r153};
	add.s64 	%rd188, %rd182, %rd187;
	add.s64 	%rd189, %rd188, %rd1347;
	xor.b64  	%rd190, %rd184, %rd189;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r155,%dummy}, %rd190;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r156}, %rd190;
	}
	shf.r.wrap.b32 	%r157, %r156, %r155, 16;
	shf.r.wrap.b32 	%r158, %r155, %r156, 16;
	mov.b64 	%rd191, {%r158, %r157};
	add.s64 	%rd192, %rd185, %rd191;
	xor.b64  	%rd193, %rd187, %rd192;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r159}, %rd193;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r160,%dummy}, %rd193;
	}
	shf.l.wrap.b32 	%r161, %r160, %r159, 1;
	shf.l.wrap.b32 	%r162, %r159, %r160, 1;
	mov.b64 	%rd194, {%r162, %r161};
	add.s64 	%rd195, %rd144, %rd151;
	add.s64 	%rd196, %rd195, %rd1346;
	xor.b64  	%rd197, %rd165, %rd196;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r163,%dummy}, %rd197;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r164}, %rd197;
	}
	mov.b64 	%rd198, {%r164, %r163};
	add.s64 	%rd199, %rd130, %rd198;
	xor.b64  	%rd200, %rd144, %rd199;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r165,%dummy}, %rd200;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r166}, %rd200;
	}
	shf.r.wrap.b32 	%r167, %r166, %r165, 24;
	shf.r.wrap.b32 	%r168, %r165, %r166, 24;
	mov.b64 	%rd201, {%r168, %r167};
	add.s64 	%rd202, %rd196, %rd201;
	xor.b64  	%rd203, %rd198, %rd202;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r169,%dummy}, %rd203;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r170}, %rd203;
	}
	shf.r.wrap.b32 	%r171, %r170, %r169, 16;
	shf.r.wrap.b32 	%r172, %r169, %r170, 16;
	mov.b64 	%rd204, {%r172, %r171};
	add.s64 	%rd205, %rd199, %rd204;
	xor.b64  	%rd206, %rd201, %rd205;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r173}, %rd206;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r174,%dummy}, %rd206;
	}
	shf.l.wrap.b32 	%r175, %r174, %r173, 1;
	shf.l.wrap.b32 	%r176, %r173, %r174, 1;
	mov.b64 	%rd207, {%r176, %r175};
	add.s64 	%rd208, %rd156, %rd163;
	xor.b64  	%rd209, %rd129, %rd208;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r177,%dummy}, %rd209;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r178}, %rd209;
	}
	mov.b64 	%rd210, {%r178, %r177};
	add.s64 	%rd211, %rd142, %rd210;
	xor.b64  	%rd212, %rd156, %rd211;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r179,%dummy}, %rd212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r180}, %rd212;
	}
	shf.r.wrap.b32 	%r181, %r180, %r179, 24;
	shf.r.wrap.b32 	%r182, %r179, %r180, 24;
	mov.b64 	%rd213, {%r182, %r181};
	add.s64 	%rd214, %rd208, %rd213;
	add.s64 	%rd215, %rd214, %rd1349;
	xor.b64  	%rd216, %rd210, %rd215;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r183,%dummy}, %rd216;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r184}, %rd216;
	}
	shf.r.wrap.b32 	%r185, %r184, %r183, 16;
	shf.r.wrap.b32 	%r186, %r183, %r184, 16;
	mov.b64 	%rd217, {%r186, %r185};
	add.s64 	%rd218, %rd211, %rd217;
	xor.b64  	%rd219, %rd213, %rd218;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r187}, %rd219;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r188,%dummy}, %rd219;
	}
	shf.l.wrap.b32 	%r189, %r188, %r187, 1;
	shf.l.wrap.b32 	%r190, %r187, %r188, 1;
	mov.b64 	%rd220, {%r190, %r189};
	add.s64 	%rd221, %rd194, %rd175;
	add.s64 	%rd222, %rd221, %rd1354;
	xor.b64  	%rd223, %rd217, %rd222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r191,%dummy}, %rd223;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r192}, %rd223;
	}
	mov.b64 	%rd224, {%r192, %r191};
	add.s64 	%rd225, %rd205, %rd224;
	xor.b64  	%rd226, %rd194, %rd225;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r193,%dummy}, %rd226;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r194}, %rd226;
	}
	shf.r.wrap.b32 	%r195, %r194, %r193, 24;
	shf.r.wrap.b32 	%r196, %r193, %r194, 24;
	mov.b64 	%rd227, {%r196, %r195};
	add.s64 	%rd228, %rd222, %rd227;
	xor.b64  	%rd229, %rd224, %rd228;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r197,%dummy}, %rd229;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r198}, %rd229;
	}
	shf.r.wrap.b32 	%r199, %r198, %r197, 16;
	shf.r.wrap.b32 	%r200, %r197, %r198, 16;
	mov.b64 	%rd230, {%r200, %r199};
	add.s64 	%rd231, %rd225, %rd230;
	xor.b64  	%rd232, %rd227, %rd231;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r201}, %rd232;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r202,%dummy}, %rd232;
	}
	shf.l.wrap.b32 	%r203, %r202, %r201, 1;
	shf.l.wrap.b32 	%r204, %r201, %r202, 1;
	mov.b64 	%rd233, {%r204, %r203};
	add.s64 	%rd234, %rd207, %rd189;
	add.s64 	%rd235, %rd234, %rd1355;
	xor.b64  	%rd236, %rd177, %rd235;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r205,%dummy}, %rd236;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r206}, %rd236;
	}
	mov.b64 	%rd237, {%r206, %r205};
	add.s64 	%rd238, %rd218, %rd237;
	xor.b64  	%rd239, %rd207, %rd238;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r207,%dummy}, %rd239;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r208}, %rd239;
	}
	shf.r.wrap.b32 	%r209, %r208, %r207, 24;
	shf.r.wrap.b32 	%r210, %r207, %r208, 24;
	mov.b64 	%rd240, {%r210, %r209};
	add.s64 	%rd241, %rd235, %rd240;
	add.s64 	%rd242, %rd241, %rd1353;
	xor.b64  	%rd243, %rd237, %rd242;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r211,%dummy}, %rd243;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r212}, %rd243;
	}
	shf.r.wrap.b32 	%r213, %r212, %r211, 16;
	shf.r.wrap.b32 	%r214, %r211, %r212, 16;
	mov.b64 	%rd244, {%r214, %r213};
	add.s64 	%rd245, %rd238, %rd244;
	xor.b64  	%rd246, %rd240, %rd245;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r215}, %rd246;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r216,%dummy}, %rd246;
	}
	shf.l.wrap.b32 	%r217, %r216, %r215, 1;
	shf.l.wrap.b32 	%r218, %r215, %r216, 1;
	mov.b64 	%rd247, {%r218, %r217};
	add.s64 	%rd248, %rd220, %rd202;
	xor.b64  	%rd249, %rd191, %rd248;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r219,%dummy}, %rd249;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r220}, %rd249;
	}
	mov.b64 	%rd250, {%r220, %r219};
	add.s64 	%rd251, %rd178, %rd250;
	xor.b64  	%rd252, %rd220, %rd251;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r221,%dummy}, %rd252;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r222}, %rd252;
	}
	shf.r.wrap.b32 	%r223, %r222, %r221, 24;
	shf.r.wrap.b32 	%r224, %r221, %r222, 24;
	mov.b64 	%rd253, {%r224, %r223};
	add.s64 	%rd254, %rd248, %rd253;
	add.s64 	%rd255, %rd254, %rd1348;
	xor.b64  	%rd256, %rd250, %rd255;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r225,%dummy}, %rd256;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r226}, %rd256;
	}
	shf.r.wrap.b32 	%r227, %r226, %r225, 16;
	shf.r.wrap.b32 	%r228, %r225, %r226, 16;
	mov.b64 	%rd257, {%r228, %r227};
	add.s64 	%rd258, %rd251, %rd257;
	xor.b64  	%rd259, %rd253, %rd258;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r229}, %rd259;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r230,%dummy}, %rd259;
	}
	shf.l.wrap.b32 	%r231, %r230, %r229, 1;
	shf.l.wrap.b32 	%r232, %r229, %r230, 1;
	mov.b64 	%rd260, {%r232, %r231};
	add.s64 	%rd261, %rd180, %rd215;
	add.s64 	%rd262, %rd261, %rd1350;
	xor.b64  	%rd263, %rd204, %rd262;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r233,%dummy}, %rd263;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r234}, %rd263;
	}
	mov.b64 	%rd264, {%r234, %r233};
	add.s64 	%rd265, %rd192, %rd264;
	xor.b64  	%rd266, %rd180, %rd265;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r235,%dummy}, %rd266;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r236}, %rd266;
	}
	shf.r.wrap.b32 	%r237, %r236, %r235, 24;
	shf.r.wrap.b32 	%r238, %r235, %r236, 24;
	mov.b64 	%rd267, {%r238, %r237};
	add.s64 	%rd268, %rd262, %rd267;
	add.s64 	%rd269, %rd268, %rd1352;
	xor.b64  	%rd270, %rd264, %rd269;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r239,%dummy}, %rd270;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r240}, %rd270;
	}
	shf.r.wrap.b32 	%r241, %r240, %r239, 16;
	shf.r.wrap.b32 	%r242, %r239, %r240, 16;
	mov.b64 	%rd271, {%r242, %r241};
	add.s64 	%rd272, %rd265, %rd271;
	xor.b64  	%rd273, %rd267, %rd272;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r243}, %rd273;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r244,%dummy}, %rd273;
	}
	shf.l.wrap.b32 	%r245, %r244, %r243, 1;
	shf.l.wrap.b32 	%r246, %r243, %r244, 1;
	mov.b64 	%rd274, {%r246, %r245};
	add.s64 	%rd275, %rd274, %rd228;
	xor.b64  	%rd276, %rd244, %rd275;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r247,%dummy}, %rd276;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r248}, %rd276;
	}
	mov.b64 	%rd277, {%r248, %r247};
	add.s64 	%rd278, %rd258, %rd277;
	xor.b64  	%rd279, %rd274, %rd278;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r249,%dummy}, %rd279;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r250}, %rd279;
	}
	shf.r.wrap.b32 	%r251, %r250, %r249, 24;
	shf.r.wrap.b32 	%r252, %r249, %r250, 24;
	mov.b64 	%rd280, {%r252, %r251};
	add.s64 	%rd281, %rd275, %rd280;
	add.s64 	%rd282, %rd281, %rd1347;
	xor.b64  	%rd283, %rd277, %rd282;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r253,%dummy}, %rd283;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r254}, %rd283;
	}
	shf.r.wrap.b32 	%r255, %r254, %r253, 16;
	shf.r.wrap.b32 	%r256, %r253, %r254, 16;
	mov.b64 	%rd284, {%r256, %r255};
	add.s64 	%rd285, %rd278, %rd284;
	xor.b64  	%rd286, %rd280, %rd285;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r257}, %rd286;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r258,%dummy}, %rd286;
	}
	shf.l.wrap.b32 	%r259, %r258, %r257, 1;
	shf.l.wrap.b32 	%r260, %r257, %r258, 1;
	mov.b64 	%rd287, {%r260, %r259};
	add.s64 	%rd288, %rd233, %rd242;
	xor.b64  	%rd289, %rd257, %rd288;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r261,%dummy}, %rd289;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r262}, %rd289;
	}
	mov.b64 	%rd290, {%r262, %r261};
	add.s64 	%rd291, %rd272, %rd290;
	xor.b64  	%rd292, %rd233, %rd291;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r263,%dummy}, %rd292;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r264}, %rd292;
	}
	shf.r.wrap.b32 	%r265, %r264, %r263, 24;
	shf.r.wrap.b32 	%r266, %r263, %r264, 24;
	mov.b64 	%rd293, {%r266, %r265};
	add.s64 	%rd294, %rd288, %rd293;
	add.s64 	%rd295, %rd294, %rd1355;
	xor.b64  	%rd296, %rd290, %rd295;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r267,%dummy}, %rd296;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r268}, %rd296;
	}
	shf.r.wrap.b32 	%r269, %r268, %r267, 16;
	shf.r.wrap.b32 	%r270, %r267, %r268, 16;
	mov.b64 	%rd297, {%r270, %r269};
	add.s64 	%rd298, %rd291, %rd297;
	xor.b64  	%rd299, %rd293, %rd298;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r271}, %rd299;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r272,%dummy}, %rd299;
	}
	shf.l.wrap.b32 	%r273, %r272, %r271, 1;
	shf.l.wrap.b32 	%r274, %r271, %r272, 1;
	mov.b64 	%rd300, {%r274, %r273};
	add.s64 	%rd301, %rd247, %rd255;
	add.s64 	%rd302, %rd301, %rd1350;
	xor.b64  	%rd303, %rd271, %rd302;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r275,%dummy}, %rd303;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r276}, %rd303;
	}
	mov.b64 	%rd304, {%r276, %r275};
	add.s64 	%rd305, %rd231, %rd304;
	xor.b64  	%rd306, %rd247, %rd305;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r277,%dummy}, %rd306;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r278}, %rd306;
	}
	shf.r.wrap.b32 	%r279, %r278, %r277, 24;
	shf.r.wrap.b32 	%r280, %r277, %r278, 24;
	mov.b64 	%rd307, {%r280, %r279};
	add.s64 	%rd308, %rd302, %rd307;
	add.s64 	%rd309, %rd308, %rd1353;
	xor.b64  	%rd310, %rd304, %rd309;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r281,%dummy}, %rd310;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r282}, %rd310;
	}
	shf.r.wrap.b32 	%r283, %r282, %r281, 16;
	shf.r.wrap.b32 	%r284, %r281, %r282, 16;
	mov.b64 	%rd311, {%r284, %r283};
	add.s64 	%rd312, %rd305, %rd311;
	xor.b64  	%rd313, %rd307, %rd312;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r285}, %rd313;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r286,%dummy}, %rd313;
	}
	shf.l.wrap.b32 	%r287, %r286, %r285, 1;
	shf.l.wrap.b32 	%r288, %r285, %r286, 1;
	mov.b64 	%rd314, {%r288, %r287};
	add.s64 	%rd315, %rd260, %rd269;
	xor.b64  	%rd316, %rd230, %rd315;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r289,%dummy}, %rd316;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r290}, %rd316;
	}
	mov.b64 	%rd317, {%r290, %r289};
	add.s64 	%rd318, %rd245, %rd317;
	xor.b64  	%rd319, %rd260, %rd318;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r291,%dummy}, %rd319;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r292}, %rd319;
	}
	shf.r.wrap.b32 	%r293, %r292, %r291, 24;
	shf.r.wrap.b32 	%r294, %r291, %r292, 24;
	mov.b64 	%rd320, {%r294, %r293};
	add.s64 	%rd321, %rd315, %rd320;
	xor.b64  	%rd322, %rd317, %rd321;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r295,%dummy}, %rd322;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r296}, %rd322;
	}
	shf.r.wrap.b32 	%r297, %r296, %r295, 16;
	shf.r.wrap.b32 	%r298, %r295, %r296, 16;
	mov.b64 	%rd323, {%r298, %r297};
	add.s64 	%rd324, %rd318, %rd323;
	xor.b64  	%rd325, %rd320, %rd324;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r299}, %rd325;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r300,%dummy}, %rd325;
	}
	shf.l.wrap.b32 	%r301, %r300, %r299, 1;
	shf.l.wrap.b32 	%r302, %r299, %r300, 1;
	mov.b64 	%rd326, {%r302, %r301};
	add.s64 	%rd327, %rd300, %rd282;
	xor.b64  	%rd328, %rd323, %rd327;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r303,%dummy}, %rd328;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r304}, %rd328;
	}
	mov.b64 	%rd329, {%r304, %r303};
	add.s64 	%rd330, %rd312, %rd329;
	xor.b64  	%rd331, %rd300, %rd330;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r305,%dummy}, %rd331;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r306}, %rd331;
	}
	shf.r.wrap.b32 	%r307, %r306, %r305, 24;
	shf.r.wrap.b32 	%r308, %r305, %r306, 24;
	mov.b64 	%rd332, {%r308, %r307};
	add.s64 	%rd333, %rd327, %rd332;
	xor.b64  	%rd334, %rd329, %rd333;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r309,%dummy}, %rd334;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r310}, %rd334;
	}
	shf.r.wrap.b32 	%r311, %r310, %r309, 16;
	shf.r.wrap.b32 	%r312, %r309, %r310, 16;
	mov.b64 	%rd335, {%r312, %r311};
	add.s64 	%rd336, %rd330, %rd335;
	xor.b64  	%rd337, %rd332, %rd336;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r313}, %rd337;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r314,%dummy}, %rd337;
	}
	shf.l.wrap.b32 	%r315, %r314, %r313, 1;
	shf.l.wrap.b32 	%r316, %r313, %r314, 1;
	mov.b64 	%rd338, {%r316, %r315};
	add.s64 	%rd339, %rd314, %rd295;
	add.s64 	%rd340, %rd339, %rd1352;
	xor.b64  	%rd341, %rd284, %rd340;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r317,%dummy}, %rd341;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r318}, %rd341;
	}
	mov.b64 	%rd342, {%r318, %r317};
	add.s64 	%rd343, %rd324, %rd342;
	xor.b64  	%rd344, %rd314, %rd343;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r319,%dummy}, %rd344;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r320}, %rd344;
	}
	shf.r.wrap.b32 	%r321, %r320, %r319, 24;
	shf.r.wrap.b32 	%r322, %r319, %r320, 24;
	mov.b64 	%rd345, {%r322, %r321};
	add.s64 	%rd346, %rd340, %rd345;
	add.s64 	%rd347, %rd346, %rd1349;
	xor.b64  	%rd348, %rd342, %rd347;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r323,%dummy}, %rd348;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r324}, %rd348;
	}
	shf.r.wrap.b32 	%r325, %r324, %r323, 16;
	shf.r.wrap.b32 	%r326, %r323, %r324, 16;
	mov.b64 	%rd349, {%r326, %r325};
	add.s64 	%rd350, %rd343, %rd349;
	xor.b64  	%rd351, %rd345, %rd350;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r327}, %rd351;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r328,%dummy}, %rd351;
	}
	shf.l.wrap.b32 	%r329, %r328, %r327, 1;
	shf.l.wrap.b32 	%r330, %r327, %r328, 1;
	mov.b64 	%rd352, {%r330, %r329};
	add.s64 	%rd353, %rd326, %rd309;
	add.s64 	%rd354, %rd353, %rd1348;
	xor.b64  	%rd355, %rd297, %rd354;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r331,%dummy}, %rd355;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r332}, %rd355;
	}
	mov.b64 	%rd356, {%r332, %r331};
	add.s64 	%rd357, %rd285, %rd356;
	xor.b64  	%rd358, %rd326, %rd357;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r333,%dummy}, %rd358;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r334}, %rd358;
	}
	shf.r.wrap.b32 	%r335, %r334, %r333, 24;
	shf.r.wrap.b32 	%r336, %r333, %r334, 24;
	mov.b64 	%rd359, {%r336, %r335};
	add.s64 	%rd360, %rd354, %rd359;
	add.s64 	%rd361, %rd360, %rd1354;
	xor.b64  	%rd362, %rd356, %rd361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r337,%dummy}, %rd362;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r338}, %rd362;
	}
	shf.r.wrap.b32 	%r339, %r338, %r337, 16;
	shf.r.wrap.b32 	%r340, %r337, %r338, 16;
	mov.b64 	%rd363, {%r340, %r339};
	add.s64 	%rd364, %rd357, %rd363;
	xor.b64  	%rd365, %rd359, %rd364;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r341}, %rd365;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r342,%dummy}, %rd365;
	}
	shf.l.wrap.b32 	%r343, %r342, %r341, 1;
	shf.l.wrap.b32 	%r344, %r341, %r342, 1;
	mov.b64 	%rd366, {%r344, %r343};
	add.s64 	%rd367, %rd287, %rd321;
	add.s64 	%rd368, %rd367, %rd1346;
	xor.b64  	%rd369, %rd311, %rd368;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r345,%dummy}, %rd369;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r346}, %rd369;
	}
	mov.b64 	%rd370, {%r346, %r345};
	add.s64 	%rd371, %rd298, %rd370;
	xor.b64  	%rd372, %rd287, %rd371;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r347,%dummy}, %rd372;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r348}, %rd372;
	}
	shf.r.wrap.b32 	%r349, %r348, %r347, 24;
	shf.r.wrap.b32 	%r350, %r347, %r348, 24;
	mov.b64 	%rd373, {%r350, %r349};
	add.s64 	%rd374, %rd368, %rd373;
	add.s64 	%rd375, %rd374, %rd1351;
	xor.b64  	%rd376, %rd370, %rd375;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r351,%dummy}, %rd376;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r352}, %rd376;
	}
	shf.r.wrap.b32 	%r353, %r352, %r351, 16;
	shf.r.wrap.b32 	%r354, %r351, %r352, 16;
	mov.b64 	%rd377, {%r354, %r353};
	add.s64 	%rd378, %rd371, %rd377;
	xor.b64  	%rd379, %rd373, %rd378;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r355}, %rd379;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r356,%dummy}, %rd379;
	}
	shf.l.wrap.b32 	%r357, %r356, %r355, 1;
	shf.l.wrap.b32 	%r358, %r355, %r356, 1;
	mov.b64 	%rd380, {%r358, %r357};
	add.s64 	%rd381, %rd380, %rd333;
	add.s64 	%rd382, %rd381, %rd1348;
	xor.b64  	%rd383, %rd349, %rd382;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r359,%dummy}, %rd383;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r360}, %rd383;
	}
	mov.b64 	%rd384, {%r360, %r359};
	add.s64 	%rd385, %rd364, %rd384;
	xor.b64  	%rd386, %rd380, %rd385;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r361,%dummy}, %rd386;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r362}, %rd386;
	}
	shf.r.wrap.b32 	%r363, %r362, %r361, 24;
	shf.r.wrap.b32 	%r364, %r361, %r362, 24;
	mov.b64 	%rd387, {%r364, %r363};
	add.s64 	%rd388, %rd382, %rd387;
	add.s64 	%rd389, %rd388, %rd1346;
	xor.b64  	%rd390, %rd384, %rd389;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r365,%dummy}, %rd390;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r366}, %rd390;
	}
	shf.r.wrap.b32 	%r367, %r366, %r365, 16;
	shf.r.wrap.b32 	%r368, %r365, %r366, 16;
	mov.b64 	%rd391, {%r368, %r367};
	add.s64 	%rd392, %rd385, %rd391;
	xor.b64  	%rd393, %rd387, %rd392;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r369}, %rd393;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r370,%dummy}, %rd393;
	}
	shf.l.wrap.b32 	%r371, %r370, %r369, 1;
	shf.l.wrap.b32 	%r372, %r369, %r370, 1;
	mov.b64 	%rd394, {%r372, %r371};
	add.s64 	%rd395, %rd338, %rd347;
	add.s64 	%rd396, %rd395, %rd1352;
	xor.b64  	%rd397, %rd363, %rd396;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r373,%dummy}, %rd397;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r374}, %rd397;
	}
	mov.b64 	%rd398, {%r374, %r373};
	add.s64 	%rd399, %rd378, %rd398;
	xor.b64  	%rd400, %rd338, %rd399;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r375,%dummy}, %rd400;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r376}, %rd400;
	}
	shf.r.wrap.b32 	%r377, %r376, %r375, 24;
	shf.r.wrap.b32 	%r378, %r375, %r376, 24;
	mov.b64 	%rd401, {%r378, %r377};
	add.s64 	%rd402, %rd396, %rd401;
	add.s64 	%rd403, %rd402, %rd1354;
	xor.b64  	%rd404, %rd398, %rd403;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r379,%dummy}, %rd404;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r380}, %rd404;
	}
	shf.r.wrap.b32 	%r381, %r380, %r379, 16;
	shf.r.wrap.b32 	%r382, %r379, %r380, 16;
	mov.b64 	%rd405, {%r382, %r381};
	add.s64 	%rd406, %rd399, %rd405;
	xor.b64  	%rd407, %rd401, %rd406;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r383}, %rd407;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r384,%dummy}, %rd407;
	}
	shf.l.wrap.b32 	%r385, %r384, %r383, 1;
	shf.l.wrap.b32 	%r386, %r383, %r384, 1;
	mov.b64 	%rd408, {%r386, %r385};
	add.s64 	%rd409, %rd352, %rd361;
	xor.b64  	%rd410, %rd377, %rd409;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r387,%dummy}, %rd410;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r388}, %rd410;
	}
	mov.b64 	%rd411, {%r388, %r387};
	add.s64 	%rd412, %rd336, %rd411;
	xor.b64  	%rd413, %rd352, %rd412;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r389,%dummy}, %rd413;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r390}, %rd413;
	}
	shf.r.wrap.b32 	%r391, %r390, %r389, 24;
	shf.r.wrap.b32 	%r392, %r389, %r390, 24;
	mov.b64 	%rd414, {%r392, %r391};
	add.s64 	%rd415, %rd409, %rd414;
	xor.b64  	%rd416, %rd411, %rd415;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r393,%dummy}, %rd416;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r394}, %rd416;
	}
	shf.r.wrap.b32 	%r395, %r394, %r393, 16;
	shf.r.wrap.b32 	%r396, %r393, %r394, 16;
	mov.b64 	%rd417, {%r396, %r395};
	add.s64 	%rd418, %rd412, %rd417;
	xor.b64  	%rd419, %rd414, %rd418;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r397}, %rd419;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r398,%dummy}, %rd419;
	}
	shf.l.wrap.b32 	%r399, %r398, %r397, 1;
	shf.l.wrap.b32 	%r400, %r397, %r398, 1;
	mov.b64 	%rd420, {%r400, %r399};
	add.s64 	%rd421, %rd366, %rd375;
	xor.b64  	%rd422, %rd335, %rd421;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r401,%dummy}, %rd422;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r402}, %rd422;
	}
	mov.b64 	%rd423, {%r402, %r401};
	add.s64 	%rd424, %rd350, %rd423;
	xor.b64  	%rd425, %rd366, %rd424;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r403,%dummy}, %rd425;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r404}, %rd425;
	}
	shf.r.wrap.b32 	%r405, %r404, %r403, 24;
	shf.r.wrap.b32 	%r406, %r403, %r404, 24;
	mov.b64 	%rd426, {%r406, %r405};
	add.s64 	%rd427, %rd421, %rd426;
	xor.b64  	%rd428, %rd423, %rd427;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r407,%dummy}, %rd428;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r408}, %rd428;
	}
	shf.r.wrap.b32 	%r409, %r408, %r407, 16;
	shf.r.wrap.b32 	%r410, %r407, %r408, 16;
	mov.b64 	%rd429, {%r410, %r409};
	add.s64 	%rd430, %rd424, %rd429;
	xor.b64  	%rd431, %rd426, %rd430;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r411}, %rd431;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r412,%dummy}, %rd431;
	}
	shf.l.wrap.b32 	%r413, %r412, %r411, 1;
	shf.l.wrap.b32 	%r414, %r411, %r412, 1;
	mov.b64 	%rd432, {%r414, %r413};
	add.s64 	%rd433, %rd408, %rd389;
	add.s64 	%rd434, %rd433, %rd1353;
	xor.b64  	%rd435, %rd429, %rd434;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r415,%dummy}, %rd435;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r416}, %rd435;
	}
	mov.b64 	%rd436, {%r416, %r415};
	add.s64 	%rd437, %rd418, %rd436;
	xor.b64  	%rd438, %rd408, %rd437;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r417,%dummy}, %rd438;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r418}, %rd438;
	}
	shf.r.wrap.b32 	%r419, %r418, %r417, 24;
	shf.r.wrap.b32 	%r420, %r417, %r418, 24;
	mov.b64 	%rd439, {%r420, %r419};
	add.s64 	%rd440, %rd434, %rd439;
	add.s64 	%rd441, %rd440, %rd1349;
	xor.b64  	%rd442, %rd436, %rd441;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r421,%dummy}, %rd442;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r422}, %rd442;
	}
	shf.r.wrap.b32 	%r423, %r422, %r421, 16;
	shf.r.wrap.b32 	%r424, %r421, %r422, 16;
	mov.b64 	%rd443, {%r424, %r423};
	add.s64 	%rd444, %rd437, %rd443;
	xor.b64  	%rd445, %rd439, %rd444;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r425}, %rd445;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r426,%dummy}, %rd445;
	}
	shf.l.wrap.b32 	%r427, %r426, %r425, 1;
	shf.l.wrap.b32 	%r428, %r425, %r426, 1;
	mov.b64 	%rd446, {%r428, %r427};
	add.s64 	%rd447, %rd420, %rd403;
	add.s64 	%rd448, %rd447, %rd1350;
	xor.b64  	%rd449, %rd391, %rd448;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r429,%dummy}, %rd449;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r430}, %rd449;
	}
	mov.b64 	%rd450, {%r430, %r429};
	add.s64 	%rd451, %rd430, %rd450;
	xor.b64  	%rd452, %rd420, %rd451;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r431,%dummy}, %rd452;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r432}, %rd452;
	}
	shf.r.wrap.b32 	%r433, %r432, %r431, 24;
	shf.r.wrap.b32 	%r434, %r431, %r432, 24;
	mov.b64 	%rd453, {%r434, %r433};
	add.s64 	%rd454, %rd448, %rd453;
	xor.b64  	%rd455, %rd450, %rd454;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r435,%dummy}, %rd455;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r436}, %rd455;
	}
	shf.r.wrap.b32 	%r437, %r436, %r435, 16;
	shf.r.wrap.b32 	%r438, %r435, %r436, 16;
	mov.b64 	%rd456, {%r438, %r437};
	add.s64 	%rd457, %rd451, %rd456;
	xor.b64  	%rd458, %rd453, %rd457;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r439}, %rd458;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r440,%dummy}, %rd458;
	}
	shf.l.wrap.b32 	%r441, %r440, %r439, 1;
	shf.l.wrap.b32 	%r442, %r439, %r440, 1;
	mov.b64 	%rd459, {%r442, %r441};
	add.s64 	%rd460, %rd432, %rd415;
	add.s64 	%rd461, %rd460, %rd1351;
	xor.b64  	%rd462, %rd405, %rd461;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r443,%dummy}, %rd462;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r444}, %rd462;
	}
	mov.b64 	%rd463, {%r444, %r443};
	add.s64 	%rd464, %rd392, %rd463;
	xor.b64  	%rd465, %rd432, %rd464;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r445,%dummy}, %rd465;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r446}, %rd465;
	}
	shf.r.wrap.b32 	%r447, %r446, %r445, 24;
	shf.r.wrap.b32 	%r448, %r445, %r446, 24;
	mov.b64 	%rd466, {%r448, %r447};
	add.s64 	%rd467, %rd461, %rd466;
	add.s64 	%rd468, %rd467, %rd1355;
	xor.b64  	%rd469, %rd463, %rd468;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r449,%dummy}, %rd469;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r450}, %rd469;
	}
	shf.r.wrap.b32 	%r451, %r450, %r449, 16;
	shf.r.wrap.b32 	%r452, %r449, %r450, 16;
	mov.b64 	%rd470, {%r452, %r451};
	add.s64 	%rd471, %rd464, %rd470;
	xor.b64  	%rd472, %rd466, %rd471;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r453}, %rd472;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r454,%dummy}, %rd472;
	}
	shf.l.wrap.b32 	%r455, %r454, %r453, 1;
	shf.l.wrap.b32 	%r456, %r453, %r454, 1;
	mov.b64 	%rd473, {%r456, %r455};
	add.s64 	%rd474, %rd394, %rd427;
	xor.b64  	%rd475, %rd417, %rd474;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r457,%dummy}, %rd475;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r458}, %rd475;
	}
	mov.b64 	%rd476, {%r458, %r457};
	add.s64 	%rd477, %rd406, %rd476;
	xor.b64  	%rd478, %rd394, %rd477;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r459,%dummy}, %rd478;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r460}, %rd478;
	}
	shf.r.wrap.b32 	%r461, %r460, %r459, 24;
	shf.r.wrap.b32 	%r462, %r459, %r460, 24;
	mov.b64 	%rd479, {%r462, %r461};
	add.s64 	%rd480, %rd474, %rd479;
	add.s64 	%rd481, %rd480, %rd1347;
	xor.b64  	%rd482, %rd476, %rd481;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r463,%dummy}, %rd482;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r464}, %rd482;
	}
	shf.r.wrap.b32 	%r465, %r464, %r463, 16;
	shf.r.wrap.b32 	%r466, %r463, %r464, 16;
	mov.b64 	%rd483, {%r466, %r465};
	add.s64 	%rd484, %rd477, %rd483;
	xor.b64  	%rd485, %rd479, %rd484;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r467}, %rd485;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r468,%dummy}, %rd485;
	}
	shf.l.wrap.b32 	%r469, %r468, %r467, 1;
	shf.l.wrap.b32 	%r470, %r467, %r468, 1;
	mov.b64 	%rd486, {%r470, %r469};
	add.s64 	%rd487, %rd486, %rd441;
	add.s64 	%rd488, %rd487, %rd1346;
	xor.b64  	%rd489, %rd456, %rd488;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r471,%dummy}, %rd489;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r472}, %rd489;
	}
	mov.b64 	%rd490, {%r472, %r471};
	add.s64 	%rd491, %rd471, %rd490;
	xor.b64  	%rd492, %rd486, %rd491;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r473,%dummy}, %rd492;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r474}, %rd492;
	}
	shf.r.wrap.b32 	%r475, %r474, %r473, 24;
	shf.r.wrap.b32 	%r476, %r473, %r474, 24;
	mov.b64 	%rd493, {%r476, %r475};
	add.s64 	%rd494, %rd488, %rd493;
	add.s64 	%rd495, %rd494, %rd1355;
	xor.b64  	%rd496, %rd490, %rd495;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r477,%dummy}, %rd496;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r478}, %rd496;
	}
	shf.r.wrap.b32 	%r479, %r478, %r477, 16;
	shf.r.wrap.b32 	%r480, %r477, %r478, 16;
	mov.b64 	%rd497, {%r480, %r479};
	add.s64 	%rd498, %rd491, %rd497;
	xor.b64  	%rd499, %rd493, %rd498;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r481}, %rd499;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r482,%dummy}, %rd499;
	}
	shf.l.wrap.b32 	%r483, %r482, %r481, 1;
	shf.l.wrap.b32 	%r484, %r481, %r482, 1;
	mov.b64 	%rd500, {%r484, %r483};
	add.s64 	%rd501, %rd446, %rd454;
	add.s64 	%rd502, %rd501, %rd1350;
	xor.b64  	%rd503, %rd470, %rd502;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r485,%dummy}, %rd503;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r486}, %rd503;
	}
	mov.b64 	%rd504, {%r486, %r485};
	add.s64 	%rd505, %rd484, %rd504;
	xor.b64  	%rd506, %rd446, %rd505;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r487,%dummy}, %rd506;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r488}, %rd506;
	}
	shf.r.wrap.b32 	%r489, %r488, %r487, 24;
	shf.r.wrap.b32 	%r490, %r487, %r488, 24;
	mov.b64 	%rd507, {%r490, %r489};
	add.s64 	%rd508, %rd502, %rd507;
	add.s64 	%rd509, %rd508, %rd1348;
	xor.b64  	%rd510, %rd504, %rd509;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r491,%dummy}, %rd510;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r492}, %rd510;
	}
	shf.r.wrap.b32 	%r493, %r492, %r491, 16;
	shf.r.wrap.b32 	%r494, %r491, %r492, 16;
	mov.b64 	%rd511, {%r494, %r493};
	add.s64 	%rd512, %rd505, %rd511;
	xor.b64  	%rd513, %rd507, %rd512;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r495}, %rd513;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r496,%dummy}, %rd513;
	}
	shf.l.wrap.b32 	%r497, %r496, %r495, 1;
	shf.l.wrap.b32 	%r498, %r495, %r496, 1;
	mov.b64 	%rd514, {%r498, %r497};
	add.s64 	%rd515, %rd459, %rd468;
	add.s64 	%rd516, %rd515, %rd1353;
	xor.b64  	%rd517, %rd483, %rd516;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r499,%dummy}, %rd517;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r500}, %rd517;
	}
	mov.b64 	%rd518, {%r500, %r499};
	add.s64 	%rd519, %rd444, %rd518;
	xor.b64  	%rd520, %rd459, %rd519;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r501,%dummy}, %rd520;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r502}, %rd520;
	}
	shf.r.wrap.b32 	%r503, %r502, %r501, 24;
	shf.r.wrap.b32 	%r504, %r501, %r502, 24;
	mov.b64 	%rd521, {%r504, %r503};
	add.s64 	%rd522, %rd516, %rd521;
	add.s64 	%rd523, %rd522, %rd1351;
	xor.b64  	%rd524, %rd518, %rd523;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r505,%dummy}, %rd524;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r506}, %rd524;
	}
	shf.r.wrap.b32 	%r507, %r506, %r505, 16;
	shf.r.wrap.b32 	%r508, %r505, %r506, 16;
	mov.b64 	%rd525, {%r508, %r507};
	add.s64 	%rd526, %rd519, %rd525;
	xor.b64  	%rd527, %rd521, %rd526;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r509}, %rd527;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r510,%dummy}, %rd527;
	}
	shf.l.wrap.b32 	%r511, %r510, %r509, 1;
	shf.l.wrap.b32 	%r512, %r509, %r510, 1;
	mov.b64 	%rd528, {%r512, %r511};
	add.s64 	%rd529, %rd473, %rd481;
	xor.b64  	%rd530, %rd443, %rd529;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r513,%dummy}, %rd530;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r514}, %rd530;
	}
	mov.b64 	%rd531, {%r514, %r513};
	add.s64 	%rd532, %rd457, %rd531;
	xor.b64  	%rd533, %rd473, %rd532;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r515,%dummy}, %rd533;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r516}, %rd533;
	}
	shf.r.wrap.b32 	%r517, %r516, %r515, 24;
	shf.r.wrap.b32 	%r518, %r515, %r516, 24;
	mov.b64 	%rd534, {%r518, %r517};
	add.s64 	%rd535, %rd529, %rd534;
	xor.b64  	%rd536, %rd531, %rd535;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r519,%dummy}, %rd536;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r520}, %rd536;
	}
	shf.r.wrap.b32 	%r521, %r520, %r519, 16;
	shf.r.wrap.b32 	%r522, %r519, %r520, 16;
	mov.b64 	%rd537, {%r522, %r521};
	add.s64 	%rd538, %rd532, %rd537;
	xor.b64  	%rd539, %rd534, %rd538;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r523}, %rd539;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r524,%dummy}, %rd539;
	}
	shf.l.wrap.b32 	%r525, %r524, %r523, 1;
	shf.l.wrap.b32 	%r526, %r523, %r524, 1;
	mov.b64 	%rd540, {%r526, %r525};
	add.s64 	%rd541, %rd514, %rd495;
	xor.b64  	%rd542, %rd537, %rd541;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r527,%dummy}, %rd542;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r528}, %rd542;
	}
	mov.b64 	%rd543, {%r528, %r527};
	add.s64 	%rd544, %rd526, %rd543;
	xor.b64  	%rd545, %rd514, %rd544;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r529,%dummy}, %rd545;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r530}, %rd545;
	}
	shf.r.wrap.b32 	%r531, %r530, %r529, 24;
	shf.r.wrap.b32 	%r532, %r529, %r530, 24;
	mov.b64 	%rd546, {%r532, %r531};
	add.s64 	%rd547, %rd541, %rd546;
	add.s64 	%rd548, %rd547, %rd1354;
	xor.b64  	%rd549, %rd543, %rd548;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r533,%dummy}, %rd549;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r534}, %rd549;
	}
	shf.r.wrap.b32 	%r535, %r534, %r533, 16;
	shf.r.wrap.b32 	%r536, %r533, %r534, 16;
	mov.b64 	%rd550, {%r536, %r535};
	add.s64 	%rd551, %rd544, %rd550;
	xor.b64  	%rd552, %rd546, %rd551;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r537}, %rd552;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r538,%dummy}, %rd552;
	}
	shf.l.wrap.b32 	%r539, %r538, %r537, 1;
	shf.l.wrap.b32 	%r540, %r537, %r538, 1;
	mov.b64 	%rd553, {%r540, %r539};
	add.s64 	%rd554, %rd528, %rd509;
	xor.b64  	%rd555, %rd497, %rd554;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r541,%dummy}, %rd555;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r542}, %rd555;
	}
	mov.b64 	%rd556, {%r542, %r541};
	add.s64 	%rd557, %rd538, %rd556;
	xor.b64  	%rd558, %rd528, %rd557;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r543,%dummy}, %rd558;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r544}, %rd558;
	}
	shf.r.wrap.b32 	%r545, %r544, %r543, 24;
	shf.r.wrap.b32 	%r546, %r543, %r544, 24;
	mov.b64 	%rd559, {%r546, %r545};
	add.s64 	%rd560, %rd554, %rd559;
	xor.b64  	%rd561, %rd556, %rd560;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r547,%dummy}, %rd561;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r548}, %rd561;
	}
	shf.r.wrap.b32 	%r549, %r548, %r547, 16;
	shf.r.wrap.b32 	%r550, %r547, %r548, 16;
	mov.b64 	%rd562, {%r550, %r549};
	add.s64 	%rd563, %rd557, %rd562;
	xor.b64  	%rd564, %rd559, %rd563;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r551}, %rd564;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r552,%dummy}, %rd564;
	}
	shf.l.wrap.b32 	%r553, %r552, %r551, 1;
	shf.l.wrap.b32 	%r554, %r551, %r552, 1;
	mov.b64 	%rd565, {%r554, %r553};
	add.s64 	%rd566, %rd540, %rd523;
	add.s64 	%rd567, %rd566, %rd1349;
	xor.b64  	%rd568, %rd511, %rd567;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r555,%dummy}, %rd568;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r556}, %rd568;
	}
	mov.b64 	%rd569, {%r556, %r555};
	add.s64 	%rd570, %rd498, %rd569;
	xor.b64  	%rd571, %rd540, %rd570;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r557,%dummy}, %rd571;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r558}, %rd571;
	}
	shf.r.wrap.b32 	%r559, %r558, %r557, 24;
	shf.r.wrap.b32 	%r560, %r557, %r558, 24;
	mov.b64 	%rd572, {%r560, %r559};
	add.s64 	%rd573, %rd567, %rd572;
	add.s64 	%rd574, %rd573, %rd1347;
	xor.b64  	%rd575, %rd569, %rd574;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r561,%dummy}, %rd575;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r562}, %rd575;
	}
	shf.r.wrap.b32 	%r563, %r562, %r561, 16;
	shf.r.wrap.b32 	%r564, %r561, %r562, 16;
	mov.b64 	%rd576, {%r564, %r563};
	add.s64 	%rd577, %rd570, %rd576;
	xor.b64  	%rd578, %rd572, %rd577;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r565}, %rd578;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r566,%dummy}, %rd578;
	}
	shf.l.wrap.b32 	%r567, %r566, %r565, 1;
	shf.l.wrap.b32 	%r568, %r565, %r566, 1;
	mov.b64 	%rd579, {%r568, %r567};
	add.s64 	%rd580, %rd500, %rd535;
	add.s64 	%rd581, %rd580, %rd1352;
	xor.b64  	%rd582, %rd525, %rd581;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r569,%dummy}, %rd582;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r570}, %rd582;
	}
	mov.b64 	%rd583, {%r570, %r569};
	add.s64 	%rd584, %rd512, %rd583;
	xor.b64  	%rd585, %rd500, %rd584;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r571,%dummy}, %rd585;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r572}, %rd585;
	}
	shf.r.wrap.b32 	%r573, %r572, %r571, 24;
	shf.r.wrap.b32 	%r574, %r571, %r572, 24;
	mov.b64 	%rd586, {%r574, %r573};
	add.s64 	%rd587, %rd581, %rd586;
	xor.b64  	%rd588, %rd583, %rd587;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r575,%dummy}, %rd588;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r576}, %rd588;
	}
	shf.r.wrap.b32 	%r577, %r576, %r575, 16;
	shf.r.wrap.b32 	%r578, %r575, %r576, 16;
	mov.b64 	%rd589, {%r578, %r577};
	add.s64 	%rd590, %rd584, %rd589;
	xor.b64  	%rd591, %rd586, %rd590;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r579}, %rd591;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r580,%dummy}, %rd591;
	}
	shf.l.wrap.b32 	%r581, %r580, %r579, 1;
	shf.l.wrap.b32 	%r582, %r579, %r580, 1;
	mov.b64 	%rd592, {%r582, %r581};
	add.s64 	%rd593, %rd592, %rd548;
	add.s64 	%rd594, %rd593, %rd1353;
	xor.b64  	%rd595, %rd562, %rd594;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r583,%dummy}, %rd595;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r584}, %rd595;
	}
	mov.b64 	%rd596, {%r584, %r583};
	add.s64 	%rd597, %rd577, %rd596;
	xor.b64  	%rd598, %rd592, %rd597;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r585,%dummy}, %rd598;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r586}, %rd598;
	}
	shf.r.wrap.b32 	%r587, %r586, %r585, 24;
	shf.r.wrap.b32 	%r588, %r585, %r586, 24;
	mov.b64 	%rd599, {%r588, %r587};
	add.s64 	%rd600, %rd594, %rd599;
	xor.b64  	%rd601, %rd596, %rd600;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r589,%dummy}, %rd601;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r590}, %rd601;
	}
	shf.r.wrap.b32 	%r591, %r590, %r589, 16;
	shf.r.wrap.b32 	%r592, %r589, %r590, 16;
	mov.b64 	%rd602, {%r592, %r591};
	add.s64 	%rd603, %rd597, %rd602;
	xor.b64  	%rd604, %rd599, %rd603;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r593}, %rd604;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r594,%dummy}, %rd604;
	}
	shf.l.wrap.b32 	%r595, %r594, %r593, 1;
	shf.l.wrap.b32 	%r596, %r593, %r594, 1;
	mov.b64 	%rd605, {%r596, %r595};
	add.s64 	%rd606, %rd553, %rd560;
	add.s64 	%rd607, %rd606, %rd1349;
	xor.b64  	%rd608, %rd576, %rd607;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r597,%dummy}, %rd608;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r598}, %rd608;
	}
	mov.b64 	%rd609, {%r598, %r597};
	add.s64 	%rd610, %rd590, %rd609;
	xor.b64  	%rd611, %rd553, %rd610;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r599,%dummy}, %rd611;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r600}, %rd611;
	}
	shf.r.wrap.b32 	%r601, %r600, %r599, 24;
	shf.r.wrap.b32 	%r602, %r599, %r600, 24;
	mov.b64 	%rd612, {%r602, %r601};
	add.s64 	%rd613, %rd607, %rd612;
	xor.b64  	%rd614, %rd609, %rd613;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r603,%dummy}, %rd614;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r604}, %rd614;
	}
	shf.r.wrap.b32 	%r605, %r604, %r603, 16;
	shf.r.wrap.b32 	%r606, %r603, %r604, 16;
	mov.b64 	%rd615, {%r606, %r605};
	add.s64 	%rd616, %rd610, %rd615;
	xor.b64  	%rd617, %rd612, %rd616;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r607}, %rd617;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r608,%dummy}, %rd617;
	}
	shf.l.wrap.b32 	%r609, %r608, %r607, 1;
	shf.l.wrap.b32 	%r610, %r607, %r608, 1;
	mov.b64 	%rd618, {%r610, %r609};
	add.s64 	%rd619, %rd565, %rd574;
	add.s64 	%rd620, %rd619, %rd1355;
	xor.b64  	%rd621, %rd589, %rd620;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r611,%dummy}, %rd621;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r612}, %rd621;
	}
	mov.b64 	%rd622, {%r612, %r611};
	add.s64 	%rd623, %rd551, %rd622;
	xor.b64  	%rd624, %rd565, %rd623;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r613,%dummy}, %rd624;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r614}, %rd624;
	}
	shf.r.wrap.b32 	%r615, %r614, %r613, 24;
	shf.r.wrap.b32 	%r616, %r613, %r614, 24;
	mov.b64 	%rd625, {%r616, %r615};
	add.s64 	%rd626, %rd620, %rd625;
	xor.b64  	%rd627, %rd622, %rd626;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r617,%dummy}, %rd627;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r618}, %rd627;
	}
	shf.r.wrap.b32 	%r619, %r618, %r617, 16;
	shf.r.wrap.b32 	%r620, %r617, %r618, 16;
	mov.b64 	%rd628, {%r620, %r619};
	add.s64 	%rd629, %rd623, %rd628;
	xor.b64  	%rd630, %rd625, %rd629;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r621}, %rd630;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r622,%dummy}, %rd630;
	}
	shf.l.wrap.b32 	%r623, %r622, %r621, 1;
	shf.l.wrap.b32 	%r624, %r621, %r622, 1;
	mov.b64 	%rd631, {%r624, %r623};
	add.s64 	%rd632, %rd579, %rd587;
	add.s64 	%rd633, %rd632, %rd1347;
	xor.b64  	%rd634, %rd550, %rd633;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r625,%dummy}, %rd634;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r626}, %rd634;
	}
	mov.b64 	%rd635, {%r626, %r625};
	add.s64 	%rd636, %rd563, %rd635;
	xor.b64  	%rd637, %rd579, %rd636;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r627,%dummy}, %rd637;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r628}, %rd637;
	}
	shf.r.wrap.b32 	%r629, %r628, %r627, 24;
	shf.r.wrap.b32 	%r630, %r627, %r628, 24;
	mov.b64 	%rd638, {%r630, %r629};
	add.s64 	%rd639, %rd633, %rd638;
	add.s64 	%rd640, %rd639, %rd1352;
	xor.b64  	%rd641, %rd635, %rd640;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r631,%dummy}, %rd641;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r632}, %rd641;
	}
	shf.r.wrap.b32 	%r633, %r632, %r631, 16;
	shf.r.wrap.b32 	%r634, %r631, %r632, 16;
	mov.b64 	%rd642, {%r634, %r633};
	add.s64 	%rd643, %rd636, %rd642;
	xor.b64  	%rd644, %rd638, %rd643;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r635}, %rd644;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r636,%dummy}, %rd644;
	}
	shf.l.wrap.b32 	%r637, %r636, %r635, 1;
	shf.l.wrap.b32 	%r638, %r635, %r636, 1;
	mov.b64 	%rd645, {%r638, %r637};
	add.s64 	%rd646, %rd618, %rd600;
	add.s64 	%rd647, %rd646, %rd1351;
	xor.b64  	%rd648, %rd642, %rd647;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r639,%dummy}, %rd648;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r640}, %rd648;
	}
	mov.b64 	%rd649, {%r640, %r639};
	add.s64 	%rd650, %rd629, %rd649;
	xor.b64  	%rd651, %rd618, %rd650;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r641,%dummy}, %rd651;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r642}, %rd651;
	}
	shf.r.wrap.b32 	%r643, %r642, %r641, 24;
	shf.r.wrap.b32 	%r644, %r641, %r642, 24;
	mov.b64 	%rd652, {%r644, %r643};
	add.s64 	%rd653, %rd647, %rd652;
	xor.b64  	%rd654, %rd649, %rd653;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r645,%dummy}, %rd654;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r646}, %rd654;
	}
	shf.r.wrap.b32 	%r647, %r646, %r645, 16;
	shf.r.wrap.b32 	%r648, %r645, %r646, 16;
	mov.b64 	%rd655, {%r648, %r647};
	add.s64 	%rd656, %rd650, %rd655;
	xor.b64  	%rd657, %rd652, %rd656;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r649}, %rd657;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r650,%dummy}, %rd657;
	}
	shf.l.wrap.b32 	%r651, %r650, %r649, 1;
	shf.l.wrap.b32 	%r652, %r649, %r650, 1;
	mov.b64 	%rd658, {%r652, %r651};
	add.s64 	%rd659, %rd631, %rd613;
	add.s64 	%rd660, %rd659, %rd1348;
	xor.b64  	%rd661, %rd602, %rd660;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r653,%dummy}, %rd661;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r654}, %rd661;
	}
	mov.b64 	%rd662, {%r654, %r653};
	add.s64 	%rd663, %rd643, %rd662;
	xor.b64  	%rd664, %rd631, %rd663;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r655,%dummy}, %rd664;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r656}, %rd664;
	}
	shf.r.wrap.b32 	%r657, %r656, %r655, 24;
	shf.r.wrap.b32 	%r658, %r655, %r656, 24;
	mov.b64 	%rd665, {%r658, %r657};
	add.s64 	%rd666, %rd660, %rd665;
	add.s64 	%rd667, %rd666, %rd1350;
	xor.b64  	%rd668, %rd662, %rd667;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r659,%dummy}, %rd668;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r660}, %rd668;
	}
	shf.r.wrap.b32 	%r661, %r660, %r659, 16;
	shf.r.wrap.b32 	%r662, %r659, %r660, 16;
	mov.b64 	%rd669, {%r662, %r661};
	add.s64 	%rd670, %rd663, %rd669;
	xor.b64  	%rd671, %rd665, %rd670;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r663}, %rd671;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r664,%dummy}, %rd671;
	}
	shf.l.wrap.b32 	%r665, %r664, %r663, 1;
	shf.l.wrap.b32 	%r666, %r663, %r664, 1;
	mov.b64 	%rd672, {%r666, %r665};
	add.s64 	%rd673, %rd645, %rd626;
	xor.b64  	%rd674, %rd615, %rd673;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r667,%dummy}, %rd674;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r668}, %rd674;
	}
	mov.b64 	%rd675, {%r668, %r667};
	add.s64 	%rd676, %rd603, %rd675;
	xor.b64  	%rd677, %rd645, %rd676;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r669,%dummy}, %rd677;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r670}, %rd677;
	}
	shf.r.wrap.b32 	%r671, %r670, %r669, 24;
	shf.r.wrap.b32 	%r672, %r669, %r670, 24;
	mov.b64 	%rd678, {%r672, %r671};
	add.s64 	%rd679, %rd673, %rd678;
	xor.b64  	%rd680, %rd675, %rd679;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r673,%dummy}, %rd680;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r674}, %rd680;
	}
	shf.r.wrap.b32 	%r675, %r674, %r673, 16;
	shf.r.wrap.b32 	%r676, %r673, %r674, 16;
	mov.b64 	%rd681, {%r676, %r675};
	add.s64 	%rd682, %rd676, %rd681;
	xor.b64  	%rd683, %rd678, %rd682;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r677}, %rd683;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r678,%dummy}, %rd683;
	}
	shf.l.wrap.b32 	%r679, %r678, %r677, 1;
	shf.l.wrap.b32 	%r680, %r677, %r678, 1;
	mov.b64 	%rd684, {%r680, %r679};
	add.s64 	%rd685, %rd605, %rd640;
	add.s64 	%rd686, %rd685, %rd1354;
	xor.b64  	%rd687, %rd628, %rd686;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r681,%dummy}, %rd687;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r682}, %rd687;
	}
	mov.b64 	%rd688, {%r682, %r681};
	add.s64 	%rd689, %rd616, %rd688;
	xor.b64  	%rd690, %rd605, %rd689;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r683,%dummy}, %rd690;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r684}, %rd690;
	}
	shf.r.wrap.b32 	%r685, %r684, %r683, 24;
	shf.r.wrap.b32 	%r686, %r683, %r684, 24;
	mov.b64 	%rd691, {%r686, %r685};
	add.s64 	%rd692, %rd686, %rd691;
	add.s64 	%rd693, %rd692, %rd1346;
	xor.b64  	%rd694, %rd688, %rd693;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r687,%dummy}, %rd694;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r688}, %rd694;
	}
	shf.r.wrap.b32 	%r689, %r688, %r687, 16;
	shf.r.wrap.b32 	%r690, %r687, %r688, 16;
	mov.b64 	%rd695, {%r690, %r689};
	add.s64 	%rd696, %rd689, %rd695;
	xor.b64  	%rd697, %rd691, %rd696;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r691}, %rd697;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r692,%dummy}, %rd697;
	}
	shf.l.wrap.b32 	%r693, %r692, %r691, 1;
	shf.l.wrap.b32 	%r694, %r691, %r692, 1;
	mov.b64 	%rd698, {%r694, %r693};
	add.s64 	%rd699, %rd698, %rd653;
	xor.b64  	%rd700, %rd669, %rd699;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r695,%dummy}, %rd700;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r696}, %rd700;
	}
	mov.b64 	%rd701, {%r696, %r695};
	add.s64 	%rd702, %rd682, %rd701;
	xor.b64  	%rd703, %rd698, %rd702;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r697,%dummy}, %rd703;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r698}, %rd703;
	}
	shf.r.wrap.b32 	%r699, %r698, %r697, 24;
	shf.r.wrap.b32 	%r700, %r697, %r698, 24;
	mov.b64 	%rd704, {%r700, %r699};
	add.s64 	%rd705, %rd699, %rd704;
	add.s64 	%rd706, %rd705, %rd1350;
	xor.b64  	%rd707, %rd701, %rd706;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r701,%dummy}, %rd707;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r702}, %rd707;
	}
	shf.r.wrap.b32 	%r703, %r702, %r701, 16;
	shf.r.wrap.b32 	%r704, %r701, %r702, 16;
	mov.b64 	%rd708, {%r704, %r703};
	add.s64 	%rd709, %rd702, %rd708;
	xor.b64  	%rd710, %rd704, %rd709;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r705}, %rd710;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r706,%dummy}, %rd710;
	}
	shf.l.wrap.b32 	%r707, %r706, %r705, 1;
	shf.l.wrap.b32 	%r708, %r705, %r706, 1;
	mov.b64 	%rd711, {%r708, %r707};
	add.s64 	%rd712, %rd658, %rd667;
	add.s64 	%rd713, %rd712, %rd1354;
	xor.b64  	%rd714, %rd681, %rd713;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r709,%dummy}, %rd714;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r710}, %rd714;
	}
	mov.b64 	%rd715, {%r710, %r709};
	add.s64 	%rd716, %rd696, %rd715;
	xor.b64  	%rd717, %rd658, %rd716;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r711,%dummy}, %rd717;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r712}, %rd717;
	}
	shf.r.wrap.b32 	%r713, %r712, %r711, 24;
	shf.r.wrap.b32 	%r714, %r711, %r712, 24;
	mov.b64 	%rd718, {%r714, %r713};
	add.s64 	%rd719, %rd713, %rd718;
	xor.b64  	%rd720, %rd715, %rd719;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r715,%dummy}, %rd720;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r716}, %rd720;
	}
	shf.r.wrap.b32 	%r717, %r716, %r715, 16;
	shf.r.wrap.b32 	%r718, %r715, %r716, 16;
	mov.b64 	%rd721, {%r718, %r717};
	add.s64 	%rd722, %rd716, %rd721;
	xor.b64  	%rd723, %rd718, %rd722;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r719}, %rd723;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r720,%dummy}, %rd723;
	}
	shf.l.wrap.b32 	%r721, %r720, %r719, 1;
	shf.l.wrap.b32 	%r722, %r719, %r720, 1;
	mov.b64 	%rd724, {%r722, %r721};
	add.s64 	%rd725, %rd672, %rd679;
	xor.b64  	%rd726, %rd695, %rd725;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r723,%dummy}, %rd726;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r724}, %rd726;
	}
	mov.b64 	%rd727, {%r724, %r723};
	add.s64 	%rd728, %rd656, %rd727;
	xor.b64  	%rd729, %rd672, %rd728;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r725,%dummy}, %rd729;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r726}, %rd729;
	}
	shf.r.wrap.b32 	%r727, %r726, %r725, 24;
	shf.r.wrap.b32 	%r728, %r725, %r726, 24;
	mov.b64 	%rd730, {%r728, %r727};
	add.s64 	%rd731, %rd725, %rd730;
	xor.b64  	%rd732, %rd727, %rd731;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r729,%dummy}, %rd732;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r730}, %rd732;
	}
	shf.r.wrap.b32 	%r731, %r730, %r729, 16;
	shf.r.wrap.b32 	%r732, %r729, %r730, 16;
	mov.b64 	%rd733, {%r732, %r731};
	add.s64 	%rd734, %rd728, %rd733;
	xor.b64  	%rd735, %rd730, %rd734;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r733}, %rd735;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r734,%dummy}, %rd735;
	}
	shf.l.wrap.b32 	%r735, %r734, %r733, 1;
	shf.l.wrap.b32 	%r736, %r733, %r734, 1;
	mov.b64 	%rd736, {%r736, %r735};
	add.s64 	%rd737, %rd684, %rd693;
	add.s64 	%rd738, %rd737, %rd1351;
	xor.b64  	%rd739, %rd655, %rd738;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r737,%dummy}, %rd739;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r738}, %rd739;
	}
	mov.b64 	%rd740, {%r738, %r737};
	add.s64 	%rd741, %rd670, %rd740;
	xor.b64  	%rd742, %rd684, %rd741;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r739,%dummy}, %rd742;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r740}, %rd742;
	}
	shf.r.wrap.b32 	%r741, %r740, %r739, 24;
	shf.r.wrap.b32 	%r742, %r739, %r740, 24;
	mov.b64 	%rd743, {%r742, %r741};
	add.s64 	%rd744, %rd738, %rd743;
	xor.b64  	%rd745, %rd740, %rd744;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r743,%dummy}, %rd745;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r744}, %rd745;
	}
	shf.r.wrap.b32 	%r745, %r744, %r743, 16;
	shf.r.wrap.b32 	%r746, %r743, %r744, 16;
	mov.b64 	%rd746, {%r746, %r745};
	add.s64 	%rd747, %rd741, %rd746;
	xor.b64  	%rd748, %rd743, %rd747;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r747}, %rd748;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r748,%dummy}, %rd748;
	}
	shf.l.wrap.b32 	%r749, %r748, %r747, 1;
	shf.l.wrap.b32 	%r750, %r747, %r748, 1;
	mov.b64 	%rd749, {%r750, %r749};
	add.s64 	%rd750, %rd724, %rd706;
	add.s64 	%rd751, %rd750, %rd1355;
	xor.b64  	%rd752, %rd746, %rd751;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r751,%dummy}, %rd752;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r752}, %rd752;
	}
	mov.b64 	%rd753, {%r752, %r751};
	add.s64 	%rd754, %rd734, %rd753;
	xor.b64  	%rd755, %rd724, %rd754;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r753,%dummy}, %rd755;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r754}, %rd755;
	}
	shf.r.wrap.b32 	%r755, %r754, %r753, 24;
	shf.r.wrap.b32 	%r756, %r753, %r754, 24;
	mov.b64 	%rd756, {%r756, %r755};
	add.s64 	%rd757, %rd751, %rd756;
	add.s64 	%rd758, %rd757, %rd1348;
	xor.b64  	%rd759, %rd753, %rd758;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r757,%dummy}, %rd759;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r758}, %rd759;
	}
	shf.r.wrap.b32 	%r759, %r758, %r757, 16;
	shf.r.wrap.b32 	%r760, %r757, %r758, 16;
	mov.b64 	%rd760, {%r760, %r759};
	add.s64 	%rd761, %rd754, %rd760;
	xor.b64  	%rd762, %rd756, %rd761;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r761}, %rd762;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r762,%dummy}, %rd762;
	}
	shf.l.wrap.b32 	%r763, %r762, %r761, 1;
	shf.l.wrap.b32 	%r764, %r761, %r762, 1;
	mov.b64 	%rd763, {%r764, %r763};
	add.s64 	%rd764, %rd736, %rd719;
	add.s64 	%rd765, %rd764, %rd1349;
	xor.b64  	%rd766, %rd708, %rd765;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r765,%dummy}, %rd766;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r766}, %rd766;
	}
	mov.b64 	%rd767, {%r766, %r765};
	add.s64 	%rd768, %rd747, %rd767;
	xor.b64  	%rd769, %rd736, %rd768;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r767,%dummy}, %rd769;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r768}, %rd769;
	}
	shf.r.wrap.b32 	%r769, %r768, %r767, 24;
	shf.r.wrap.b32 	%r770, %r767, %r768, 24;
	mov.b64 	%rd770, {%r770, %r769};
	add.s64 	%rd771, %rd765, %rd770;
	add.s64 	%rd772, %rd771, %rd1352;
	xor.b64  	%rd773, %rd767, %rd772;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r771,%dummy}, %rd773;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r772}, %rd773;
	}
	shf.r.wrap.b32 	%r773, %r772, %r771, 16;
	shf.r.wrap.b32 	%r774, %r771, %r772, 16;
	mov.b64 	%rd774, {%r774, %r773};
	add.s64 	%rd775, %rd768, %rd774;
	xor.b64  	%rd776, %rd770, %rd775;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r775}, %rd776;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r776,%dummy}, %rd776;
	}
	shf.l.wrap.b32 	%r777, %r776, %r775, 1;
	shf.l.wrap.b32 	%r778, %r775, %r776, 1;
	mov.b64 	%rd777, {%r778, %r777};
	add.s64 	%rd778, %rd749, %rd731;
	add.s64 	%rd779, %rd778, %rd1346;
	xor.b64  	%rd780, %rd721, %rd779;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r779,%dummy}, %rd780;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r780}, %rd780;
	}
	mov.b64 	%rd781, {%r780, %r779};
	add.s64 	%rd782, %rd709, %rd781;
	xor.b64  	%rd783, %rd749, %rd782;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r781,%dummy}, %rd783;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r782}, %rd783;
	}
	shf.r.wrap.b32 	%r783, %r782, %r781, 24;
	shf.r.wrap.b32 	%r784, %r781, %r782, 24;
	mov.b64 	%rd784, {%r784, %r783};
	add.s64 	%rd785, %rd779, %rd784;
	add.s64 	%rd786, %rd785, %rd1353;
	xor.b64  	%rd787, %rd781, %rd786;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r785,%dummy}, %rd787;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r786}, %rd787;
	}
	shf.r.wrap.b32 	%r787, %r786, %r785, 16;
	shf.r.wrap.b32 	%r788, %r785, %r786, 16;
	mov.b64 	%rd788, {%r788, %r787};
	add.s64 	%rd789, %rd782, %rd788;
	xor.b64  	%rd790, %rd784, %rd789;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r789}, %rd790;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r790,%dummy}, %rd790;
	}
	shf.l.wrap.b32 	%r791, %r790, %r789, 1;
	shf.l.wrap.b32 	%r792, %r789, %r790, 1;
	mov.b64 	%rd791, {%r792, %r791};
	add.s64 	%rd792, %rd711, %rd744;
	add.s64 	%rd793, %rd792, %rd1347;
	xor.b64  	%rd794, %rd733, %rd793;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r793,%dummy}, %rd794;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r794}, %rd794;
	}
	mov.b64 	%rd795, {%r794, %r793};
	add.s64 	%rd796, %rd722, %rd795;
	xor.b64  	%rd797, %rd711, %rd796;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r795,%dummy}, %rd797;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r796}, %rd797;
	}
	shf.r.wrap.b32 	%r797, %r796, %r795, 24;
	shf.r.wrap.b32 	%r798, %r795, %r796, 24;
	mov.b64 	%rd798, {%r798, %r797};
	add.s64 	%rd799, %rd793, %rd798;
	xor.b64  	%rd800, %rd795, %rd799;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r799,%dummy}, %rd800;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r800}, %rd800;
	}
	shf.r.wrap.b32 	%r801, %r800, %r799, 16;
	shf.r.wrap.b32 	%r802, %r799, %r800, 16;
	mov.b64 	%rd801, {%r802, %r801};
	add.s64 	%rd802, %rd796, %rd801;
	xor.b64  	%rd803, %rd798, %rd802;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r803}, %rd803;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r804,%dummy}, %rd803;
	}
	shf.l.wrap.b32 	%r805, %r804, %r803, 1;
	shf.l.wrap.b32 	%r806, %r803, %r804, 1;
	mov.b64 	%rd804, {%r806, %r805};
	add.s64 	%rd805, %rd804, %rd758;
	xor.b64  	%rd806, %rd774, %rd805;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r807,%dummy}, %rd806;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r808}, %rd806;
	}
	mov.b64 	%rd807, {%r808, %r807};
	add.s64 	%rd808, %rd789, %rd807;
	xor.b64  	%rd809, %rd804, %rd808;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r809,%dummy}, %rd809;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r810}, %rd809;
	}
	shf.r.wrap.b32 	%r811, %r810, %r809, 24;
	shf.r.wrap.b32 	%r812, %r809, %r810, 24;
	mov.b64 	%rd810, {%r812, %r811};
	add.s64 	%rd811, %rd805, %rd810;
	xor.b64  	%rd812, %rd807, %rd811;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r813,%dummy}, %rd812;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r814}, %rd812;
	}
	shf.r.wrap.b32 	%r815, %r814, %r813, 16;
	shf.r.wrap.b32 	%r816, %r813, %r814, 16;
	mov.b64 	%rd813, {%r816, %r815};
	add.s64 	%rd814, %rd808, %rd813;
	xor.b64  	%rd815, %rd810, %rd814;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r817}, %rd815;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r818,%dummy}, %rd815;
	}
	shf.l.wrap.b32 	%r819, %r818, %r817, 1;
	shf.l.wrap.b32 	%r820, %r817, %r818, 1;
	mov.b64 	%rd816, {%r820, %r819};
	add.s64 	%rd817, %rd763, %rd772;
	add.s64 	%rd818, %rd817, %rd1348;
	xor.b64  	%rd819, %rd788, %rd818;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r821,%dummy}, %rd819;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r822}, %rd819;
	}
	mov.b64 	%rd820, {%r822, %r821};
	add.s64 	%rd821, %rd802, %rd820;
	xor.b64  	%rd822, %rd763, %rd821;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r823,%dummy}, %rd822;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r824}, %rd822;
	}
	shf.r.wrap.b32 	%r825, %r824, %r823, 24;
	shf.r.wrap.b32 	%r826, %r823, %r824, 24;
	mov.b64 	%rd823, {%r826, %r825};
	add.s64 	%rd824, %rd818, %rd823;
	xor.b64  	%rd825, %rd820, %rd824;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r827,%dummy}, %rd825;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r828}, %rd825;
	}
	shf.r.wrap.b32 	%r829, %r828, %r827, 16;
	shf.r.wrap.b32 	%r830, %r827, %r828, 16;
	mov.b64 	%rd826, {%r830, %r829};
	add.s64 	%rd827, %rd821, %rd826;
	xor.b64  	%rd828, %rd823, %rd827;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r831}, %rd828;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r832,%dummy}, %rd828;
	}
	shf.l.wrap.b32 	%r833, %r832, %r831, 1;
	shf.l.wrap.b32 	%r834, %r831, %r832, 1;
	mov.b64 	%rd829, {%r834, %r833};
	add.s64 	%rd830, %rd777, %rd786;
	xor.b64  	%rd831, %rd801, %rd830;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r835,%dummy}, %rd831;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r836}, %rd831;
	}
	mov.b64 	%rd832, {%r836, %r835};
	add.s64 	%rd833, %rd761, %rd832;
	xor.b64  	%rd834, %rd777, %rd833;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r837,%dummy}, %rd834;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r838}, %rd834;
	}
	shf.r.wrap.b32 	%r839, %r838, %r837, 24;
	shf.r.wrap.b32 	%r840, %r837, %r838, 24;
	mov.b64 	%rd835, {%r840, %r839};
	add.s64 	%rd836, %rd830, %rd835;
	add.s64 	%rd837, %rd836, %rd1354;
	xor.b64  	%rd838, %rd832, %rd837;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r841,%dummy}, %rd838;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r842}, %rd838;
	}
	shf.r.wrap.b32 	%r843, %r842, %r841, 16;
	shf.r.wrap.b32 	%r844, %r841, %r842, 16;
	mov.b64 	%rd839, {%r844, %r843};
	add.s64 	%rd840, %rd833, %rd839;
	xor.b64  	%rd841, %rd835, %rd840;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r845}, %rd841;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r846,%dummy}, %rd841;
	}
	shf.l.wrap.b32 	%r847, %r846, %r845, 1;
	shf.l.wrap.b32 	%r848, %r845, %r846, 1;
	mov.b64 	%rd842, {%r848, %r847};
	add.s64 	%rd843, %rd791, %rd799;
	add.s64 	%rd844, %rd843, %rd1352;
	xor.b64  	%rd845, %rd760, %rd844;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r849,%dummy}, %rd845;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r850}, %rd845;
	}
	mov.b64 	%rd846, {%r850, %r849};
	add.s64 	%rd847, %rd775, %rd846;
	xor.b64  	%rd848, %rd791, %rd847;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r851,%dummy}, %rd848;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r852}, %rd848;
	}
	shf.r.wrap.b32 	%r853, %r852, %r851, 24;
	shf.r.wrap.b32 	%r854, %r851, %r852, 24;
	mov.b64 	%rd849, {%r854, %r853};
	add.s64 	%rd850, %rd844, %rd849;
	add.s64 	%rd851, %rd850, %rd1346;
	xor.b64  	%rd852, %rd846, %rd851;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r855,%dummy}, %rd852;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r856}, %rd852;
	}
	shf.r.wrap.b32 	%r857, %r856, %r855, 16;
	shf.r.wrap.b32 	%r858, %r855, %r856, 16;
	mov.b64 	%rd853, {%r858, %r857};
	add.s64 	%rd854, %rd847, %rd853;
	xor.b64  	%rd855, %rd849, %rd854;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r859}, %rd855;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r860,%dummy}, %rd855;
	}
	shf.l.wrap.b32 	%r861, %r860, %r859, 1;
	shf.l.wrap.b32 	%r862, %r859, %r860, 1;
	mov.b64 	%rd856, {%r862, %r861};
	add.s64 	%rd857, %rd829, %rd811;
	add.s64 	%rd858, %rd857, %rd1350;
	xor.b64  	%rd859, %rd853, %rd858;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r863,%dummy}, %rd859;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r864}, %rd859;
	}
	mov.b64 	%rd860, {%r864, %r863};
	add.s64 	%rd861, %rd840, %rd860;
	xor.b64  	%rd862, %rd829, %rd861;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r865,%dummy}, %rd862;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r866}, %rd862;
	}
	shf.r.wrap.b32 	%r867, %r866, %r865, 24;
	shf.r.wrap.b32 	%r868, %r865, %r866, 24;
	mov.b64 	%rd863, {%r868, %r867};
	add.s64 	%rd864, %rd858, %rd863;
	add.s64 	%rd865, %rd864, %rd1355;
	xor.b64  	%rd866, %rd860, %rd865;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r869,%dummy}, %rd866;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r870}, %rd866;
	}
	shf.r.wrap.b32 	%r871, %r870, %r869, 16;
	shf.r.wrap.b32 	%r872, %r869, %r870, 16;
	mov.b64 	%rd867, {%r872, %r871};
	add.s64 	%rd868, %rd861, %rd867;
	xor.b64  	%rd869, %rd863, %rd868;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r873}, %rd869;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r874,%dummy}, %rd869;
	}
	shf.l.wrap.b32 	%r875, %r874, %r873, 1;
	shf.l.wrap.b32 	%r876, %r873, %r874, 1;
	mov.b64 	%rd870, {%r876, %r875};
	add.s64 	%rd871, %rd842, %rd824;
	xor.b64  	%rd872, %rd813, %rd871;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r877,%dummy}, %rd872;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r878}, %rd872;
	}
	mov.b64 	%rd873, {%r878, %r877};
	add.s64 	%rd874, %rd854, %rd873;
	xor.b64  	%rd875, %rd842, %rd874;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r879,%dummy}, %rd875;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r880}, %rd875;
	}
	shf.r.wrap.b32 	%r881, %r880, %r879, 24;
	shf.r.wrap.b32 	%r882, %r879, %r880, 24;
	mov.b64 	%rd876, {%r882, %r881};
	add.s64 	%rd877, %rd871, %rd876;
	add.s64 	%rd878, %rd877, %rd1351;
	xor.b64  	%rd879, %rd873, %rd878;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r883,%dummy}, %rd879;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r884}, %rd879;
	}
	shf.r.wrap.b32 	%r885, %r884, %r883, 16;
	shf.r.wrap.b32 	%r886, %r883, %r884, 16;
	mov.b64 	%rd880, {%r886, %r885};
	add.s64 	%rd881, %rd874, %rd880;
	xor.b64  	%rd882, %rd876, %rd881;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r887}, %rd882;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r888,%dummy}, %rd882;
	}
	shf.l.wrap.b32 	%r889, %r888, %r887, 1;
	shf.l.wrap.b32 	%r890, %r887, %r888, 1;
	mov.b64 	%rd883, {%r890, %r889};
	add.s64 	%rd884, %rd856, %rd837;
	add.s64 	%rd885, %rd884, %rd1347;
	xor.b64  	%rd886, %rd826, %rd885;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r891,%dummy}, %rd886;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r892}, %rd886;
	}
	mov.b64 	%rd887, {%r892, %r891};
	add.s64 	%rd888, %rd814, %rd887;
	xor.b64  	%rd889, %rd856, %rd888;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r893,%dummy}, %rd889;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r894}, %rd889;
	}
	shf.r.wrap.b32 	%r895, %r894, %r893, 24;
	shf.r.wrap.b32 	%r896, %r893, %r894, 24;
	mov.b64 	%rd890, {%r896, %r895};
	add.s64 	%rd891, %rd885, %rd890;
	add.s64 	%rd892, %rd891, %rd1349;
	xor.b64  	%rd893, %rd887, %rd892;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r897,%dummy}, %rd893;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r898}, %rd893;
	}
	shf.r.wrap.b32 	%r899, %r898, %r897, 16;
	shf.r.wrap.b32 	%r900, %r897, %r898, 16;
	mov.b64 	%rd894, {%r900, %r899};
	add.s64 	%rd895, %rd888, %rd894;
	xor.b64  	%rd896, %rd890, %rd895;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r901}, %rd896;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r902,%dummy}, %rd896;
	}
	shf.l.wrap.b32 	%r903, %r902, %r901, 1;
	shf.l.wrap.b32 	%r904, %r901, %r902, 1;
	mov.b64 	%rd897, {%r904, %r903};
	add.s64 	%rd898, %rd816, %rd851;
	add.s64 	%rd899, %rd898, %rd1353;
	xor.b64  	%rd900, %rd839, %rd899;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r905,%dummy}, %rd900;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r906}, %rd900;
	}
	mov.b64 	%rd901, {%r906, %r905};
	add.s64 	%rd902, %rd827, %rd901;
	xor.b64  	%rd903, %rd816, %rd902;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r907,%dummy}, %rd903;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r908}, %rd903;
	}
	shf.r.wrap.b32 	%r909, %r908, %r907, 24;
	shf.r.wrap.b32 	%r910, %r907, %r908, 24;
	mov.b64 	%rd904, {%r910, %r909};
	add.s64 	%rd905, %rd899, %rd904;
	xor.b64  	%rd906, %rd901, %rd905;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r911,%dummy}, %rd906;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r912}, %rd906;
	}
	shf.r.wrap.b32 	%r913, %r912, %r911, 16;
	shf.r.wrap.b32 	%r914, %r911, %r912, 16;
	mov.b64 	%rd907, {%r914, %r913};
	add.s64 	%rd908, %rd902, %rd907;
	xor.b64  	%rd909, %rd904, %rd908;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r915}, %rd909;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r916,%dummy}, %rd909;
	}
	shf.l.wrap.b32 	%r917, %r916, %r915, 1;
	shf.l.wrap.b32 	%r918, %r915, %r916, 1;
	mov.b64 	%rd910, {%r918, %r917};
	add.s64 	%rd911, %rd910, %rd865;
	add.s64 	%rd912, %rd911, %rd1349;
	xor.b64  	%rd913, %rd880, %rd912;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r919,%dummy}, %rd913;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r920}, %rd913;
	}
	mov.b64 	%rd914, {%r920, %r919};
	add.s64 	%rd915, %rd895, %rd914;
	xor.b64  	%rd916, %rd910, %rd915;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r921,%dummy}, %rd916;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r922}, %rd916;
	}
	shf.r.wrap.b32 	%r923, %r922, %r921, 24;
	shf.r.wrap.b32 	%r924, %r921, %r922, 24;
	mov.b64 	%rd917, {%r924, %r923};
	add.s64 	%rd918, %rd912, %rd917;
	xor.b64  	%rd919, %rd914, %rd918;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r925,%dummy}, %rd919;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r926}, %rd919;
	}
	shf.r.wrap.b32 	%r927, %r926, %r925, 16;
	shf.r.wrap.b32 	%r928, %r925, %r926, 16;
	mov.b64 	%rd920, {%r928, %r927};
	add.s64 	%rd921, %rd915, %rd920;
	xor.b64  	%rd922, %rd917, %rd921;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r929}, %rd922;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r930,%dummy}, %rd922;
	}
	shf.l.wrap.b32 	%r931, %r930, %r929, 1;
	shf.l.wrap.b32 	%r932, %r929, %r930, 1;
	mov.b64 	%rd923, {%r932, %r931};
	add.s64 	%rd924, %rd870, %rd878;
	xor.b64  	%rd925, %rd894, %rd924;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r933,%dummy}, %rd925;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r934}, %rd925;
	}
	mov.b64 	%rd926, {%r934, %r933};
	add.s64 	%rd927, %rd908, %rd926;
	xor.b64  	%rd928, %rd870, %rd927;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r935,%dummy}, %rd928;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r936}, %rd928;
	}
	shf.r.wrap.b32 	%r937, %r936, %r935, 24;
	shf.r.wrap.b32 	%r938, %r935, %r936, 24;
	mov.b64 	%rd929, {%r938, %r937};
	add.s64 	%rd930, %rd924, %rd929;
	add.s64 	%rd931, %rd930, %rd1346;
	xor.b64  	%rd932, %rd926, %rd931;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r939,%dummy}, %rd932;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r940}, %rd932;
	}
	shf.r.wrap.b32 	%r941, %r940, %r939, 16;
	shf.r.wrap.b32 	%r942, %r939, %r940, 16;
	mov.b64 	%rd933, {%r942, %r941};
	add.s64 	%rd934, %rd927, %rd933;
	xor.b64  	%rd935, %rd929, %rd934;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r943}, %rd935;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r944,%dummy}, %rd935;
	}
	shf.l.wrap.b32 	%r945, %r944, %r943, 1;
	shf.l.wrap.b32 	%r946, %r943, %r944, 1;
	mov.b64 	%rd936, {%r946, %r945};
	add.s64 	%rd937, %rd883, %rd892;
	xor.b64  	%rd938, %rd907, %rd937;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r947,%dummy}, %rd938;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r948}, %rd938;
	}
	mov.b64 	%rd939, {%r948, %r947};
	add.s64 	%rd940, %rd868, %rd939;
	xor.b64  	%rd941, %rd883, %rd940;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r949,%dummy}, %rd941;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r950}, %rd941;
	}
	shf.r.wrap.b32 	%r951, %r950, %r949, 24;
	shf.r.wrap.b32 	%r952, %r949, %r950, 24;
	mov.b64 	%rd942, {%r952, %r951};
	add.s64 	%rd943, %rd937, %rd942;
	add.s64 	%rd944, %rd943, %rd1352;
	xor.b64  	%rd945, %rd939, %rd944;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r953,%dummy}, %rd945;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r954}, %rd945;
	}
	shf.r.wrap.b32 	%r955, %r954, %r953, 16;
	shf.r.wrap.b32 	%r956, %r953, %r954, 16;
	mov.b64 	%rd946, {%r956, %r955};
	add.s64 	%rd947, %rd940, %rd946;
	xor.b64  	%rd948, %rd942, %rd947;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r957}, %rd948;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r958,%dummy}, %rd948;
	}
	shf.l.wrap.b32 	%r959, %r958, %r957, 1;
	shf.l.wrap.b32 	%r960, %r957, %r958, 1;
	mov.b64 	%rd949, {%r960, %r959};
	add.s64 	%rd950, %rd897, %rd905;
	add.s64 	%rd951, %rd950, %rd1355;
	xor.b64  	%rd952, %rd867, %rd951;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r961,%dummy}, %rd952;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r962}, %rd952;
	}
	mov.b64 	%rd953, {%r962, %r961};
	add.s64 	%rd954, %rd881, %rd953;
	xor.b64  	%rd955, %rd897, %rd954;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r963,%dummy}, %rd955;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r964}, %rd955;
	}
	shf.r.wrap.b32 	%r965, %r964, %r963, 24;
	shf.r.wrap.b32 	%r966, %r963, %r964, 24;
	mov.b64 	%rd956, {%r966, %r965};
	add.s64 	%rd957, %rd951, %rd956;
	add.s64 	%rd958, %rd957, %rd1347;
	xor.b64  	%rd959, %rd953, %rd958;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r967,%dummy}, %rd959;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r968}, %rd959;
	}
	shf.r.wrap.b32 	%r969, %r968, %r967, 16;
	shf.r.wrap.b32 	%r970, %r967, %r968, 16;
	mov.b64 	%rd960, {%r970, %r969};
	add.s64 	%rd961, %rd954, %rd960;
	xor.b64  	%rd962, %rd956, %rd961;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r971}, %rd962;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r972,%dummy}, %rd962;
	}
	shf.l.wrap.b32 	%r973, %r972, %r971, 1;
	shf.l.wrap.b32 	%r974, %r971, %r972, 1;
	mov.b64 	%rd963, {%r974, %r973};
	add.s64 	%rd964, %rd936, %rd918;
	xor.b64  	%rd965, %rd960, %rd964;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r975,%dummy}, %rd965;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r976}, %rd965;
	}
	mov.b64 	%rd966, {%r976, %r975};
	add.s64 	%rd967, %rd947, %rd966;
	xor.b64  	%rd968, %rd936, %rd967;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r977,%dummy}, %rd968;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r978}, %rd968;
	}
	shf.r.wrap.b32 	%r979, %r978, %r977, 24;
	shf.r.wrap.b32 	%r980, %r977, %r978, 24;
	mov.b64 	%rd969, {%r980, %r979};
	add.s64 	%rd970, %rd964, %rd969;
	add.s64 	%rd971, %rd970, %rd1353;
	xor.b64  	%rd972, %rd966, %rd971;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r981,%dummy}, %rd972;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r982}, %rd972;
	}
	shf.r.wrap.b32 	%r983, %r982, %r981, 16;
	shf.r.wrap.b32 	%r984, %r981, %r982, 16;
	mov.b64 	%rd973, {%r984, %r983};
	add.s64 	%rd974, %rd967, %rd973;
	xor.b64  	%rd975, %rd969, %rd974;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r985}, %rd975;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r986,%dummy}, %rd975;
	}
	shf.l.wrap.b32 	%r987, %r986, %r985, 1;
	shf.l.wrap.b32 	%r988, %r985, %r986, 1;
	mov.b64 	%rd976, {%r988, %r987};
	add.s64 	%rd977, %rd949, %rd931;
	xor.b64  	%rd978, %rd920, %rd977;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r989,%dummy}, %rd978;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r990}, %rd978;
	}
	mov.b64 	%rd979, {%r990, %r989};
	add.s64 	%rd980, %rd961, %rd979;
	xor.b64  	%rd981, %rd949, %rd980;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r991,%dummy}, %rd981;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r992}, %rd981;
	}
	shf.r.wrap.b32 	%r993, %r992, %r991, 24;
	shf.r.wrap.b32 	%r994, %r991, %r992, 24;
	mov.b64 	%rd982, {%r994, %r993};
	add.s64 	%rd983, %rd977, %rd982;
	add.s64 	%rd984, %rd983, %rd1348;
	xor.b64  	%rd985, %rd979, %rd984;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r995,%dummy}, %rd985;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r996}, %rd985;
	}
	shf.r.wrap.b32 	%r997, %r996, %r995, 16;
	shf.r.wrap.b32 	%r998, %r995, %r996, 16;
	mov.b64 	%rd986, {%r998, %r997};
	add.s64 	%rd987, %rd980, %rd986;
	xor.b64  	%rd988, %rd982, %rd987;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r999}, %rd988;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1000,%dummy}, %rd988;
	}
	shf.l.wrap.b32 	%r1001, %r1000, %r999, 1;
	shf.l.wrap.b32 	%r1002, %r999, %r1000, 1;
	mov.b64 	%rd989, {%r1002, %r1001};
	add.s64 	%rd990, %rd963, %rd944;
	add.s64 	%rd991, %rd990, %rd1354;
	xor.b64  	%rd992, %rd933, %rd991;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1003,%dummy}, %rd992;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1004}, %rd992;
	}
	mov.b64 	%rd993, {%r1004, %r1003};
	add.s64 	%rd994, %rd921, %rd993;
	xor.b64  	%rd995, %rd963, %rd994;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1005,%dummy}, %rd995;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1006}, %rd995;
	}
	shf.r.wrap.b32 	%r1007, %r1006, %r1005, 24;
	shf.r.wrap.b32 	%r1008, %r1005, %r1006, 24;
	mov.b64 	%rd996, {%r1008, %r1007};
	add.s64 	%rd997, %rd991, %rd996;
	add.s64 	%rd998, %rd997, %rd1351;
	xor.b64  	%rd999, %rd993, %rd998;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1009,%dummy}, %rd999;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1010}, %rd999;
	}
	shf.r.wrap.b32 	%r1011, %r1010, %r1009, 16;
	shf.r.wrap.b32 	%r1012, %r1009, %r1010, 16;
	mov.b64 	%rd1000, {%r1012, %r1011};
	add.s64 	%rd1001, %rd994, %rd1000;
	xor.b64  	%rd1002, %rd996, %rd1001;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1013}, %rd1002;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1014,%dummy}, %rd1002;
	}
	shf.l.wrap.b32 	%r1015, %r1014, %r1013, 1;
	shf.l.wrap.b32 	%r1016, %r1013, %r1014, 1;
	mov.b64 	%rd1003, {%r1016, %r1015};
	add.s64 	%rd1004, %rd923, %rd958;
	xor.b64  	%rd1005, %rd946, %rd1004;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1017,%dummy}, %rd1005;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1018}, %rd1005;
	}
	mov.b64 	%rd1006, {%r1018, %r1017};
	add.s64 	%rd1007, %rd934, %rd1006;
	xor.b64  	%rd1008, %rd923, %rd1007;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1019,%dummy}, %rd1008;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1020}, %rd1008;
	}
	shf.r.wrap.b32 	%r1021, %r1020, %r1019, 24;
	shf.r.wrap.b32 	%r1022, %r1019, %r1020, 24;
	mov.b64 	%rd1009, {%r1022, %r1021};
	add.s64 	%rd1010, %rd1004, %rd1009;
	add.s64 	%rd1011, %rd1010, %rd1350;
	xor.b64  	%rd1012, %rd1006, %rd1011;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1023,%dummy}, %rd1012;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1024}, %rd1012;
	}
	shf.r.wrap.b32 	%r1025, %r1024, %r1023, 16;
	shf.r.wrap.b32 	%r1026, %r1023, %r1024, 16;
	mov.b64 	%rd1013, {%r1026, %r1025};
	add.s64 	%rd1014, %rd1007, %rd1013;
	xor.b64  	%rd1015, %rd1009, %rd1014;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1027}, %rd1015;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1028,%dummy}, %rd1015;
	}
	shf.l.wrap.b32 	%r1029, %r1028, %r1027, 1;
	shf.l.wrap.b32 	%r1030, %r1027, %r1028, 1;
	mov.b64 	%rd1016, {%r1030, %r1029};
	add.s64 	%rd1017, %rd1016, %rd971;
	xor.b64  	%rd1018, %rd986, %rd1017;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1031,%dummy}, %rd1018;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1032}, %rd1018;
	}
	mov.b64 	%rd1019, {%r1032, %r1031};
	add.s64 	%rd1020, %rd1001, %rd1019;
	xor.b64  	%rd1021, %rd1016, %rd1020;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1033,%dummy}, %rd1021;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1034}, %rd1021;
	}
	shf.r.wrap.b32 	%r1035, %r1034, %r1033, 24;
	shf.r.wrap.b32 	%r1036, %r1033, %r1034, 24;
	mov.b64 	%rd1022, {%r1036, %r1035};
	add.s64 	%rd1023, %rd1017, %rd1022;
	add.s64 	%rd1024, %rd1023, %rd1353;
	xor.b64  	%rd1025, %rd1019, %rd1024;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1037,%dummy}, %rd1025;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1038}, %rd1025;
	}
	shf.r.wrap.b32 	%r1039, %r1038, %r1037, 16;
	shf.r.wrap.b32 	%r1040, %r1037, %r1038, 16;
	mov.b64 	%rd1026, {%r1040, %r1039};
	add.s64 	%rd1027, %rd1020, %rd1026;
	xor.b64  	%rd1028, %rd1022, %rd1027;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1041}, %rd1028;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1042,%dummy}, %rd1028;
	}
	shf.l.wrap.b32 	%r1043, %r1042, %r1041, 1;
	shf.l.wrap.b32 	%r1044, %r1041, %r1042, 1;
	mov.b64 	%rd1029, {%r1044, %r1043};
	add.s64 	%rd1030, %rd976, %rd984;
	add.s64 	%rd1031, %rd1030, %rd1347;
	xor.b64  	%rd1032, %rd1000, %rd1031;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1045,%dummy}, %rd1032;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1046}, %rd1032;
	}
	mov.b64 	%rd1033, {%r1046, %r1045};
	add.s64 	%rd1034, %rd1014, %rd1033;
	xor.b64  	%rd1035, %rd976, %rd1034;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1047,%dummy}, %rd1035;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1048}, %rd1035;
	}
	shf.r.wrap.b32 	%r1049, %r1048, %r1047, 24;
	shf.r.wrap.b32 	%r1050, %r1047, %r1048, 24;
	mov.b64 	%rd1036, {%r1050, %r1049};
	add.s64 	%rd1037, %rd1031, %rd1036;
	add.s64 	%rd1038, %rd1037, %rd1351;
	xor.b64  	%rd1039, %rd1033, %rd1038;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1051,%dummy}, %rd1039;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1052}, %rd1039;
	}
	shf.r.wrap.b32 	%r1053, %r1052, %r1051, 16;
	shf.r.wrap.b32 	%r1054, %r1051, %r1052, 16;
	mov.b64 	%rd1040, {%r1054, %r1053};
	add.s64 	%rd1041, %rd1034, %rd1040;
	xor.b64  	%rd1042, %rd1036, %rd1041;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1055}, %rd1042;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1056,%dummy}, %rd1042;
	}
	shf.l.wrap.b32 	%r1057, %r1056, %r1055, 1;
	shf.l.wrap.b32 	%r1058, %r1055, %r1056, 1;
	mov.b64 	%rd1043, {%r1058, %r1057};
	add.s64 	%rd1044, %rd989, %rd998;
	add.s64 	%rd1045, %rd1044, %rd1348;
	xor.b64  	%rd1046, %rd1013, %rd1045;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1059,%dummy}, %rd1046;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1060}, %rd1046;
	}
	mov.b64 	%rd1047, {%r1060, %r1059};
	add.s64 	%rd1048, %rd974, %rd1047;
	xor.b64  	%rd1049, %rd989, %rd1048;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1061,%dummy}, %rd1049;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1062}, %rd1049;
	}
	shf.r.wrap.b32 	%r1063, %r1062, %r1061, 24;
	shf.r.wrap.b32 	%r1064, %r1061, %r1062, 24;
	mov.b64 	%rd1050, {%r1064, %r1063};
	add.s64 	%rd1051, %rd1045, %rd1050;
	add.s64 	%rd1052, %rd1051, %rd1349;
	xor.b64  	%rd1053, %rd1047, %rd1052;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1065,%dummy}, %rd1053;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1066}, %rd1053;
	}
	shf.r.wrap.b32 	%r1067, %r1066, %r1065, 16;
	shf.r.wrap.b32 	%r1068, %r1065, %r1066, 16;
	mov.b64 	%rd1054, {%r1068, %r1067};
	add.s64 	%rd1055, %rd1048, %rd1054;
	xor.b64  	%rd1056, %rd1050, %rd1055;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1069}, %rd1056;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1070,%dummy}, %rd1056;
	}
	shf.l.wrap.b32 	%r1071, %r1070, %r1069, 1;
	shf.l.wrap.b32 	%r1072, %r1069, %r1070, 1;
	mov.b64 	%rd1057, {%r1072, %r1071};
	add.s64 	%rd1058, %rd1003, %rd1011;
	add.s64 	%rd1059, %rd1058, %rd1354;
	xor.b64  	%rd1060, %rd973, %rd1059;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1073,%dummy}, %rd1060;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1074}, %rd1060;
	}
	mov.b64 	%rd1061, {%r1074, %r1073};
	add.s64 	%rd1062, %rd987, %rd1061;
	xor.b64  	%rd1063, %rd1003, %rd1062;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1075,%dummy}, %rd1063;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1076}, %rd1063;
	}
	shf.r.wrap.b32 	%r1077, %r1076, %r1075, 24;
	shf.r.wrap.b32 	%r1078, %r1075, %r1076, 24;
	mov.b64 	%rd1064, {%r1078, %r1077};
	add.s64 	%rd1065, %rd1059, %rd1064;
	add.s64 	%rd1066, %rd1065, %rd1350;
	xor.b64  	%rd1067, %rd1061, %rd1066;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1079,%dummy}, %rd1067;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1080}, %rd1067;
	}
	shf.r.wrap.b32 	%r1081, %r1080, %r1079, 16;
	shf.r.wrap.b32 	%r1082, %r1079, %r1080, 16;
	mov.b64 	%rd1068, {%r1082, %r1081};
	add.s64 	%rd1069, %rd1062, %rd1068;
	xor.b64  	%rd1070, %rd1064, %rd1069;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1083}, %rd1070;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1084,%dummy}, %rd1070;
	}
	shf.l.wrap.b32 	%r1085, %r1084, %r1083, 1;
	shf.l.wrap.b32 	%r1086, %r1083, %r1084, 1;
	mov.b64 	%rd1071, {%r1086, %r1085};
	add.s64 	%rd1072, %rd1043, %rd1024;
	xor.b64  	%rd1073, %rd1068, %rd1072;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1087,%dummy}, %rd1073;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1088}, %rd1073;
	}
	mov.b64 	%rd1074, {%r1088, %r1087};
	add.s64 	%rd1075, %rd1055, %rd1074;
	xor.b64  	%rd1076, %rd1043, %rd1075;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1089,%dummy}, %rd1076;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1090}, %rd1076;
	}
	shf.r.wrap.b32 	%r1091, %r1090, %r1089, 24;
	shf.r.wrap.b32 	%r1092, %r1089, %r1090, 24;
	mov.b64 	%rd1077, {%r1092, %r1091};
	add.s64 	%rd1078, %rd1072, %rd1077;
	xor.b64  	%rd1079, %rd1074, %rd1078;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1093,%dummy}, %rd1079;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1094}, %rd1079;
	}
	shf.r.wrap.b32 	%r1095, %r1094, %r1093, 16;
	shf.r.wrap.b32 	%r1096, %r1093, %r1094, 16;
	mov.b64 	%rd1080, {%r1096, %r1095};
	add.s64 	%rd1081, %rd1075, %rd1080;
	xor.b64  	%rd1082, %rd1077, %rd1081;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1097}, %rd1082;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1098,%dummy}, %rd1082;
	}
	shf.l.wrap.b32 	%r1099, %r1098, %r1097, 1;
	shf.l.wrap.b32 	%r1100, %r1097, %r1098, 1;
	mov.b64 	%rd1083, {%r1100, %r1099};
	add.s64 	%rd1084, %rd1057, %rd1038;
	add.s64 	%rd1085, %rd1084, %rd1346;
	xor.b64  	%rd1086, %rd1026, %rd1085;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1101,%dummy}, %rd1086;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1102}, %rd1086;
	}
	mov.b64 	%rd1087, {%r1102, %r1101};
	add.s64 	%rd1088, %rd1069, %rd1087;
	xor.b64  	%rd1089, %rd1057, %rd1088;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1103,%dummy}, %rd1089;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1104}, %rd1089;
	}
	shf.r.wrap.b32 	%r1105, %r1104, %r1103, 24;
	shf.r.wrap.b32 	%r1106, %r1103, %r1104, 24;
	mov.b64 	%rd1090, {%r1106, %r1105};
	add.s64 	%rd1091, %rd1085, %rd1090;
	xor.b64  	%rd1092, %rd1087, %rd1091;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1107,%dummy}, %rd1092;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1108}, %rd1092;
	}
	shf.r.wrap.b32 	%r1109, %r1108, %r1107, 16;
	shf.r.wrap.b32 	%r1110, %r1107, %r1108, 16;
	mov.b64 	%rd1093, {%r1110, %r1109};
	add.s64 	%rd1094, %rd1088, %rd1093;
	xor.b64  	%rd1095, %rd1090, %rd1094;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1111}, %rd1095;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1112,%dummy}, %rd1095;
	}
	shf.l.wrap.b32 	%r1113, %r1112, %r1111, 1;
	shf.l.wrap.b32 	%r1114, %r1111, %r1112, 1;
	mov.b64 	%rd1096, {%r1114, %r1113};
	add.s64 	%rd1097, %rd1071, %rd1052;
	add.s64 	%rd1098, %rd1097, %rd1352;
	xor.b64  	%rd1099, %rd1040, %rd1098;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1115,%dummy}, %rd1099;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1116}, %rd1099;
	}
	mov.b64 	%rd1100, {%r1116, %r1115};
	add.s64 	%rd1101, %rd1027, %rd1100;
	xor.b64  	%rd1102, %rd1071, %rd1101;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1117,%dummy}, %rd1102;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1118}, %rd1102;
	}
	shf.r.wrap.b32 	%r1119, %r1118, %r1117, 24;
	shf.r.wrap.b32 	%r1120, %r1117, %r1118, 24;
	mov.b64 	%rd1103, {%r1120, %r1119};
	add.s64 	%rd1104, %rd1098, %rd1103;
	xor.b64  	%rd1105, %rd1100, %rd1104;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1121,%dummy}, %rd1105;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1122}, %rd1105;
	}
	shf.r.wrap.b32 	%r1123, %r1122, %r1121, 16;
	shf.r.wrap.b32 	%r1124, %r1121, %r1122, 16;
	mov.b64 	%rd1106, {%r1124, %r1123};
	add.s64 	%rd1107, %rd1101, %rd1106;
	xor.b64  	%rd1108, %rd1103, %rd1107;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1125}, %rd1108;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1126,%dummy}, %rd1108;
	}
	shf.l.wrap.b32 	%r1127, %r1126, %r1125, 1;
	shf.l.wrap.b32 	%r1128, %r1125, %r1126, 1;
	mov.b64 	%rd1109, {%r1128, %r1127};
	add.s64 	%rd1110, %rd1029, %rd1066;
	xor.b64  	%rd1111, %rd1054, %rd1110;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1129,%dummy}, %rd1111;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1130}, %rd1111;
	}
	mov.b64 	%rd1112, {%r1130, %r1129};
	add.s64 	%rd1113, %rd1041, %rd1112;
	xor.b64  	%rd1114, %rd1029, %rd1113;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1131,%dummy}, %rd1114;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1132}, %rd1114;
	}
	shf.r.wrap.b32 	%r1133, %r1132, %r1131, 24;
	shf.r.wrap.b32 	%r1134, %r1131, %r1132, 24;
	mov.b64 	%rd1115, {%r1134, %r1133};
	add.s64 	%rd1116, %rd1110, %rd1115;
	add.s64 	%rd1117, %rd1116, %rd1355;
	xor.b64  	%rd1118, %rd1112, %rd1117;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1135,%dummy}, %rd1118;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1136}, %rd1118;
	}
	shf.r.wrap.b32 	%r1137, %r1136, %r1135, 16;
	shf.r.wrap.b32 	%r1138, %r1135, %r1136, 16;
	mov.b64 	%rd1119, {%r1138, %r1137};
	add.s64 	%rd1120, %rd1113, %rd1119;
	xor.b64  	%rd1121, %rd1115, %rd1120;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1139}, %rd1121;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1140,%dummy}, %rd1121;
	}
	shf.l.wrap.b32 	%r1141, %r1140, %r1139, 1;
	shf.l.wrap.b32 	%r1142, %r1139, %r1140, 1;
	mov.b64 	%rd1122, {%r1142, %r1141};
	add.s64 	%rd1123, %rd1122, %rd1078;
	add.s64 	%rd1124, %rd1123, %rd1355;
	xor.b64  	%rd1125, %rd1093, %rd1124;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1143,%dummy}, %rd1125;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1144}, %rd1125;
	}
	mov.b64 	%rd1126, {%r1144, %r1143};
	add.s64 	%rd1127, %rd1107, %rd1126;
	xor.b64  	%rd1128, %rd1122, %rd1127;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1145,%dummy}, %rd1128;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1146}, %rd1128;
	}
	shf.r.wrap.b32 	%r1147, %r1146, %r1145, 24;
	shf.r.wrap.b32 	%r1148, %r1145, %r1146, 24;
	mov.b64 	%rd1129, {%r1148, %r1147};
	add.s64 	%rd1130, %rd1124, %rd1129;
	add.s64 	%rd1131, %rd1130, %rd1354;
	xor.b64  	%rd1132, %rd1126, %rd1131;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1149,%dummy}, %rd1132;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1150}, %rd1132;
	}
	shf.r.wrap.b32 	%r1151, %r1150, %r1149, 16;
	shf.r.wrap.b32 	%r1152, %r1149, %r1150, 16;
	mov.b64 	%rd1133, {%r1152, %r1151};
	add.s64 	%rd1134, %rd1127, %rd1133;
	xor.b64  	%rd1135, %rd1129, %rd1134;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1153}, %rd1135;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1154,%dummy}, %rd1135;
	}
	shf.l.wrap.b32 	%r1155, %r1154, %r1153, 1;
	shf.l.wrap.b32 	%r1156, %r1153, %r1154, 1;
	mov.b64 	%rd1136, {%r1156, %r1155};
	add.s64 	%rd1137, %rd1083, %rd1091;
	add.s64 	%rd1138, %rd1137, %rd1353;
	xor.b64  	%rd1139, %rd1106, %rd1138;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1157,%dummy}, %rd1139;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1158}, %rd1139;
	}
	mov.b64 	%rd1140, {%r1158, %r1157};
	add.s64 	%rd1141, %rd1120, %rd1140;
	xor.b64  	%rd1142, %rd1083, %rd1141;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1159,%dummy}, %rd1142;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1160}, %rd1142;
	}
	shf.r.wrap.b32 	%r1161, %r1160, %r1159, 24;
	shf.r.wrap.b32 	%r1162, %r1159, %r1160, 24;
	mov.b64 	%rd1143, {%r1162, %r1161};
	add.s64 	%rd1144, %rd1138, %rd1143;
	add.s64 	%rd1145, %rd1144, %rd1352;
	xor.b64  	%rd1146, %rd1140, %rd1145;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1163,%dummy}, %rd1146;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1164}, %rd1146;
	}
	shf.r.wrap.b32 	%r1165, %r1164, %r1163, 16;
	shf.r.wrap.b32 	%r1166, %r1163, %r1164, 16;
	mov.b64 	%rd1147, {%r1166, %r1165};
	add.s64 	%rd1148, %rd1141, %rd1147;
	xor.b64  	%rd1149, %rd1143, %rd1148;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1167}, %rd1149;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1168,%dummy}, %rd1149;
	}
	shf.l.wrap.b32 	%r1169, %r1168, %r1167, 1;
	shf.l.wrap.b32 	%r1170, %r1167, %r1168, 1;
	mov.b64 	%rd1150, {%r1170, %r1169};
	add.s64 	%rd1151, %rd1096, %rd1104;
	add.s64 	%rd1152, %rd1151, %rd1351;
	xor.b64  	%rd1153, %rd1119, %rd1152;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1171,%dummy}, %rd1153;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1172}, %rd1153;
	}
	mov.b64 	%rd1154, {%r1172, %r1171};
	add.s64 	%rd1155, %rd1081, %rd1154;
	xor.b64  	%rd1156, %rd1096, %rd1155;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1173,%dummy}, %rd1156;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1174}, %rd1156;
	}
	shf.r.wrap.b32 	%r1175, %r1174, %r1173, 24;
	shf.r.wrap.b32 	%r1176, %r1173, %r1174, 24;
	mov.b64 	%rd1157, {%r1176, %r1175};
	add.s64 	%rd1158, %rd1152, %rd1157;
	add.s64 	%rd1159, %rd1158, %rd1350;
	xor.b64  	%rd1160, %rd1154, %rd1159;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1177,%dummy}, %rd1160;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1178}, %rd1160;
	}
	shf.r.wrap.b32 	%r1179, %r1178, %r1177, 16;
	shf.r.wrap.b32 	%r1180, %r1177, %r1178, 16;
	mov.b64 	%rd1161, {%r1180, %r1179};
	add.s64 	%rd1162, %rd1155, %rd1161;
	xor.b64  	%rd1163, %rd1157, %rd1162;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1181}, %rd1163;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1182,%dummy}, %rd1163;
	}
	shf.l.wrap.b32 	%r1183, %r1182, %r1181, 1;
	shf.l.wrap.b32 	%r1184, %r1181, %r1182, 1;
	mov.b64 	%rd1164, {%r1184, %r1183};
	add.s64 	%rd1165, %rd1109, %rd1117;
	add.s64 	%rd1166, %rd1165, %rd1349;
	xor.b64  	%rd1167, %rd1080, %rd1166;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1185,%dummy}, %rd1167;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1186}, %rd1167;
	}
	mov.b64 	%rd1168, {%r1186, %r1185};
	add.s64 	%rd1169, %rd1094, %rd1168;
	xor.b64  	%rd1170, %rd1109, %rd1169;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1187,%dummy}, %rd1170;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1188}, %rd1170;
	}
	shf.r.wrap.b32 	%r1189, %r1188, %r1187, 24;
	shf.r.wrap.b32 	%r1190, %r1187, %r1188, 24;
	mov.b64 	%rd1171, {%r1190, %r1189};
	add.s64 	%rd1172, %rd1166, %rd1171;
	add.s64 	%rd1173, %rd1172, %rd1348;
	xor.b64  	%rd1174, %rd1168, %rd1173;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1191,%dummy}, %rd1174;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1192}, %rd1174;
	}
	shf.r.wrap.b32 	%r1193, %r1192, %r1191, 16;
	shf.r.wrap.b32 	%r1194, %r1191, %r1192, 16;
	mov.b64 	%rd1175, {%r1194, %r1193};
	add.s64 	%rd1176, %rd1169, %rd1175;
	xor.b64  	%rd1177, %rd1171, %rd1176;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1195}, %rd1177;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1196,%dummy}, %rd1177;
	}
	shf.l.wrap.b32 	%r1197, %r1196, %r1195, 1;
	shf.l.wrap.b32 	%r1198, %r1195, %r1196, 1;
	mov.b64 	%rd1178, {%r1198, %r1197};
	add.s64 	%rd1179, %rd1150, %rd1131;
	add.s64 	%rd1180, %rd1179, %rd1347;
	xor.b64  	%rd1181, %rd1175, %rd1180;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1199,%dummy}, %rd1181;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1200}, %rd1181;
	}
	mov.b64 	%rd1182, {%r1200, %r1199};
	add.s64 	%rd1183, %rd1162, %rd1182;
	xor.b64  	%rd1184, %rd1150, %rd1183;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1201,%dummy}, %rd1184;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1202}, %rd1184;
	}
	shf.r.wrap.b32 	%r1203, %r1202, %r1201, 24;
	shf.r.wrap.b32 	%r1204, %r1201, %r1202, 24;
	mov.b64 	%rd1185, {%r1204, %r1203};
	add.s64 	%rd1186, %rd1180, %rd1185;
	add.s64 	%rd1187, %rd1186, %rd1346;
	xor.b64  	%rd1188, %rd1182, %rd1187;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1205,%dummy}, %rd1188;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1206}, %rd1188;
	}
	shf.r.wrap.b32 	%r1207, %r1206, %r1205, 16;
	shf.r.wrap.b32 	%r1208, %r1205, %r1206, 16;
	mov.b64 	%rd1189, {%r1208, %r1207};
	add.s64 	%rd1190, %rd1183, %rd1189;
	xor.b64  	%rd1191, %rd1185, %rd1190;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1209}, %rd1191;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1210,%dummy}, %rd1191;
	}
	shf.l.wrap.b32 	%r1211, %r1210, %r1209, 1;
	shf.l.wrap.b32 	%r1212, %r1209, %r1210, 1;
	mov.b64 	%rd1192, {%r1212, %r1211};
	add.s64 	%rd1193, %rd1164, %rd1145;
	xor.b64  	%rd1194, %rd1133, %rd1193;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1213,%dummy}, %rd1194;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1214}, %rd1194;
	}
	mov.b64 	%rd1195, {%r1214, %r1213};
	add.s64 	%rd1196, %rd1176, %rd1195;
	xor.b64  	%rd1197, %rd1164, %rd1196;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1215,%dummy}, %rd1197;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1216}, %rd1197;
	}
	shf.r.wrap.b32 	%r1217, %r1216, %r1215, 24;
	shf.r.wrap.b32 	%r1218, %r1215, %r1216, 24;
	mov.b64 	%rd1198, {%r1218, %r1217};
	add.s64 	%rd1199, %rd1193, %rd1198;
	xor.b64  	%rd1200, %rd1195, %rd1199;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1219,%dummy}, %rd1200;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1220}, %rd1200;
	}
	shf.r.wrap.b32 	%r1221, %r1220, %r1219, 16;
	shf.r.wrap.b32 	%r1222, %r1219, %r1220, 16;
	mov.b64 	%rd1201, {%r1222, %r1221};
	add.s64 	%rd1202, %rd1196, %rd1201;
	xor.b64  	%rd1203, %rd1198, %rd1202;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1223}, %rd1203;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1224,%dummy}, %rd1203;
	}
	shf.l.wrap.b32 	%r1225, %r1224, %r1223, 1;
	shf.l.wrap.b32 	%r1226, %r1223, %r1224, 1;
	mov.b64 	%rd1204, {%r1226, %r1225};
	add.s64 	%rd1205, %rd1178, %rd1159;
	xor.b64  	%rd1206, %rd1147, %rd1205;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1227,%dummy}, %rd1206;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1228}, %rd1206;
	}
	mov.b64 	%rd1207, {%r1228, %r1227};
	add.s64 	%rd1208, %rd1134, %rd1207;
	xor.b64  	%rd1209, %rd1178, %rd1208;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1229,%dummy}, %rd1209;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1230}, %rd1209;
	}
	shf.r.wrap.b32 	%r1231, %r1230, %r1229, 24;
	shf.r.wrap.b32 	%r1232, %r1229, %r1230, 24;
	mov.b64 	%rd1210, {%r1232, %r1231};
	add.s64 	%rd1211, %rd1205, %rd1210;
	xor.b64  	%rd1212, %rd1207, %rd1211;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1233,%dummy}, %rd1212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1234}, %rd1212;
	}
	shf.r.wrap.b32 	%r1235, %r1234, %r1233, 16;
	shf.r.wrap.b32 	%r1236, %r1233, %r1234, 16;
	mov.b64 	%rd1213, {%r1236, %r1235};
	add.s64 	%rd1214, %rd1208, %rd1213;
	xor.b64  	%rd1215, %rd1210, %rd1214;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1237}, %rd1215;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1238,%dummy}, %rd1215;
	}
	shf.l.wrap.b32 	%r1239, %r1238, %r1237, 1;
	shf.l.wrap.b32 	%r1240, %r1237, %r1238, 1;
	mov.b64 	%rd1216, {%r1240, %r1239};
	add.s64 	%rd1217, %rd1136, %rd1173;
	xor.b64  	%rd1218, %rd1161, %rd1217;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1241,%dummy}, %rd1218;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1242}, %rd1218;
	}
	mov.b64 	%rd1219, {%r1242, %r1241};
	add.s64 	%rd1220, %rd1148, %rd1219;
	xor.b64  	%rd1221, %rd1136, %rd1220;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1243,%dummy}, %rd1221;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1244}, %rd1221;
	}
	shf.r.wrap.b32 	%r1245, %r1244, %r1243, 24;
	shf.r.wrap.b32 	%r1246, %r1243, %r1244, 24;
	mov.b64 	%rd1222, {%r1246, %r1245};
	add.s64 	%rd1223, %rd1217, %rd1222;
	xor.b64  	%rd1224, %rd1219, %rd1223;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1247,%dummy}, %rd1224;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1248}, %rd1224;
	}
	shf.r.wrap.b32 	%r1249, %r1248, %r1247, 16;
	shf.r.wrap.b32 	%r1250, %r1247, %r1248, 16;
	mov.b64 	%rd1225, {%r1250, %r1249};
	add.s64 	%rd1226, %rd1220, %rd1225;
	xor.b64  	%rd1227, %rd1222, %rd1226;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1251}, %rd1227;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1252,%dummy}, %rd1227;
	}
	shf.l.wrap.b32 	%r1253, %r1252, %r1251, 1;
	shf.l.wrap.b32 	%r1254, %r1251, %r1252, 1;
	mov.b64 	%rd1228, {%r1254, %r1253};
	add.s64 	%rd1229, %rd1228, %rd1187;
	xor.b64  	%rd1230, %rd1201, %rd1229;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1255,%dummy}, %rd1230;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1256}, %rd1230;
	}
	mov.b64 	%rd1231, {%r1256, %r1255};
	add.s64 	%rd1232, %rd1214, %rd1231;
	xor.b64  	%rd1233, %rd1228, %rd1232;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1257,%dummy}, %rd1233;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1258}, %rd1233;
	}
	shf.r.wrap.b32 	%r1259, %r1258, %r1257, 24;
	shf.r.wrap.b32 	%r1260, %r1257, %r1258, 24;
	mov.b64 	%rd1234, {%r1260, %r1259};
	add.s64 	%rd1235, %rd1229, %rd1234;
	xor.b64  	%rd1236, %rd1231, %rd1235;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1261,%dummy}, %rd1236;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1262}, %rd1236;
	}
	shf.r.wrap.b32 	%r1263, %r1262, %r1261, 16;
	shf.r.wrap.b32 	%r1264, %r1261, %r1262, 16;
	mov.b64 	%rd1237, {%r1264, %r1263};
	add.s64 	%rd1238, %rd1232, %rd1237;
	xor.b64  	%rd1239, %rd1234, %rd1238;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1265}, %rd1239;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1266,%dummy}, %rd1239;
	}
	shf.l.wrap.b32 	%r1267, %r1266, %r1265, 1;
	shf.l.wrap.b32 	%r1268, %r1265, %r1266, 1;
	mov.b64 	%rd1240, {%r1268, %r1267};
	add.s64 	%rd1241, %rd1192, %rd1199;
	add.s64 	%rd1242, %rd1241, %rd1351;
	xor.b64  	%rd1243, %rd1213, %rd1242;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1269,%dummy}, %rd1243;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1270}, %rd1243;
	}
	mov.b64 	%rd1244, {%r1270, %r1269};
	add.s64 	%rd1245, %rd1226, %rd1244;
	xor.b64  	%rd1246, %rd1192, %rd1245;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1271,%dummy}, %rd1246;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1272}, %rd1246;
	}
	shf.r.wrap.b32 	%r1273, %r1272, %r1271, 24;
	shf.r.wrap.b32 	%r1274, %r1271, %r1272, 24;
	mov.b64 	%rd1247, {%r1274, %r1273};
	add.s64 	%rd1248, %rd1242, %rd1247;
	add.s64 	%rd1249, %rd1248, %rd1347;
	xor.b64  	%rd1250, %rd1244, %rd1249;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1275,%dummy}, %rd1250;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1276}, %rd1250;
	}
	shf.r.wrap.b32 	%r1277, %r1276, %r1275, 16;
	shf.r.wrap.b32 	%r1278, %r1275, %r1276, 16;
	mov.b64 	%rd1251, {%r1278, %r1277};
	add.s64 	%rd1252, %rd1245, %rd1251;
	xor.b64  	%rd1253, %rd1247, %rd1252;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1279}, %rd1253;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1280,%dummy}, %rd1253;
	}
	shf.l.wrap.b32 	%r1281, %r1280, %r1279, 1;
	shf.l.wrap.b32 	%r1282, %r1279, %r1280, 1;
	mov.b64 	%rd1254, {%r1282, %r1281};
	add.s64 	%rd1255, %rd1204, %rd1211;
	add.s64 	%rd1256, %rd1255, %rd1346;
	xor.b64  	%rd1257, %rd1225, %rd1256;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1283,%dummy}, %rd1257;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1284}, %rd1257;
	}
	mov.b64 	%rd1258, {%r1284, %r1283};
	add.s64 	%rd1259, %rd1190, %rd1258;
	xor.b64  	%rd1260, %rd1204, %rd1259;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1285,%dummy}, %rd1260;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1286}, %rd1260;
	}
	shf.r.wrap.b32 	%r1287, %r1286, %r1285, 24;
	shf.r.wrap.b32 	%r1288, %r1285, %r1286, 24;
	mov.b64 	%rd1261, {%r1288, %r1287};
	add.s64 	%rd1262, %rd1256, %rd1261;
	xor.b64  	%rd1263, %rd1258, %rd1262;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1289,%dummy}, %rd1263;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1290}, %rd1263;
	}
	shf.r.wrap.b32 	%r1291, %r1290, %r1289, 16;
	shf.r.wrap.b32 	%r1292, %r1289, %r1290, 16;
	mov.b64 	%rd1264, {%r1292, %r1291};
	add.s64 	%rd1265, %rd1259, %rd1264;
	xor.b64  	%rd1266, %rd1261, %rd1265;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1293}, %rd1266;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1294,%dummy}, %rd1266;
	}
	shf.l.wrap.b32 	%r1295, %r1294, %r1293, 1;
	shf.l.wrap.b32 	%r1296, %r1293, %r1294, 1;
	mov.b64 	%rd1267, {%r1296, %r1295};
	add.s64 	%rd1268, %rd1216, %rd1223;
	xor.b64  	%rd1269, %rd1189, %rd1268;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1297,%dummy}, %rd1269;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1298}, %rd1269;
	}
	mov.b64 	%rd1270, {%r1298, %r1297};
	add.s64 	%rd1271, %rd1202, %rd1270;
	xor.b64  	%rd1272, %rd1216, %rd1271;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1299,%dummy}, %rd1272;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1300}, %rd1272;
	}
	shf.r.wrap.b32 	%r1301, %r1300, %r1299, 24;
	shf.r.wrap.b32 	%r1302, %r1299, %r1300, 24;
	mov.b64 	%rd1273, {%r1302, %r1301};
	add.s64 	%rd1274, %rd1268, %rd1273;
	add.s64 	%rd1275, %rd1274, %rd1349;
	xor.b64  	%rd1276, %rd1270, %rd1275;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1303,%dummy}, %rd1276;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1304}, %rd1276;
	}
	shf.r.wrap.b32 	%r1305, %r1304, %r1303, 16;
	shf.r.wrap.b32 	%r1306, %r1303, %r1304, 16;
	mov.b64 	%rd1277, {%r1306, %r1305};
	add.s64 	%rd1278, %rd1271, %rd1277;
	xor.b64  	%rd1279, %rd1273, %rd1278;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1307}, %rd1279;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1308,%dummy}, %rd1279;
	}
	shf.l.wrap.b32 	%r1309, %r1308, %r1307, 1;
	shf.l.wrap.b32 	%r1310, %r1307, %r1308, 1;
	mov.b64 	%rd1280, {%r1310, %r1309};
	add.s64 	%rd1281, %rd1254, %rd1235;
	add.s64 	%rd1282, %rd1281, %rd1354;
	xor.b64  	%rd1283, %rd1277, %rd1282;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1311,%dummy}, %rd1283;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1312}, %rd1283;
	}
	mov.b64 	%rd1284, {%r1312, %r1311};
	add.s64 	%rd1285, %rd1265, %rd1284;
	xor.b64  	%rd1286, %rd1254, %rd1285;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1313,%dummy}, %rd1286;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1314}, %rd1286;
	}
	shf.r.wrap.b32 	%r1315, %r1314, %r1313, 24;
	shf.r.wrap.b32 	%r1316, %r1313, %r1314, 24;
	mov.b64 	%rd1287, {%r1316, %r1315};
	add.s64 	%rd1288, %rd1282, %rd1287;
	xor.b64  	%rd1289, %rd1284, %rd1288;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1317,%dummy}, %rd1289;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1318}, %rd1289;
	}
	shf.r.wrap.b32 	%r1319, %r1318, %r1317, 16;
	shf.r.wrap.b32 	%r1320, %r1317, %r1318, 16;
	mov.b64 	%rd1290, {%r1320, %r1319};
	add.s64 	%rd1291, %rd1285, %rd1290;
	xor.b64  	%rd1292, %rd1287, %rd1291;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1321}, %rd1292;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1322,%dummy}, %rd1292;
	}
	shf.l.wrap.b32 	%r1323, %r1322, %r1321, 1;
	shf.l.wrap.b32 	%r1324, %r1321, %r1322, 1;
	mov.b64 	%rd1293, {%r1324, %r1323};
	add.s64 	%rd1294, %rd1267, %rd1249;
	add.s64 	%rd1295, %rd1294, %rd1355;
	xor.b64  	%rd1296, %rd1237, %rd1295;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1325,%dummy}, %rd1296;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1326}, %rd1296;
	}
	mov.b64 	%rd1297, {%r1326, %r1325};
	add.s64 	%rd1298, %rd1278, %rd1297;
	xor.b64  	%rd1299, %rd1267, %rd1298;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1327,%dummy}, %rd1299;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1328}, %rd1299;
	}
	shf.r.wrap.b32 	%r1329, %r1328, %r1327, 24;
	shf.r.wrap.b32 	%r1330, %r1327, %r1328, 24;
	mov.b64 	%rd1300, {%r1330, %r1329};
	add.s64 	%rd1301, %rd1295, %rd1300;
	add.s64 	%rd1302, %rd1301, %rd1353;
	xor.b64  	%rd1303, %rd1297, %rd1302;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1331,%dummy}, %rd1303;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1332}, %rd1303;
	}
	shf.r.wrap.b32 	%r1333, %r1332, %r1331, 16;
	shf.r.wrap.b32 	%r1334, %r1331, %r1332, 16;
	mov.b64 	%rd1304, {%r1334, %r1333};
	add.s64 	%rd1305, %rd1298, %rd1304;
	xor.b64  	%rd1306, %rd1300, %rd1305;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1335}, %rd1306;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1336,%dummy}, %rd1306;
	}
	shf.l.wrap.b32 	%r1337, %r1336, %r1335, 1;
	shf.l.wrap.b32 	%r1338, %r1335, %r1336, 1;
	mov.b64 	%rd1307, {%r1338, %r1337};
	add.s64 	%rd1308, %rd1280, %rd1262;
	xor.b64  	%rd1309, %rd1251, %rd1308;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1339,%dummy}, %rd1309;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1340}, %rd1309;
	}
	mov.b64 	%rd1310, {%r1340, %r1339};
	add.s64 	%rd1311, %rd1238, %rd1310;
	xor.b64  	%rd1312, %rd1280, %rd1311;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1341,%dummy}, %rd1312;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1342}, %rd1312;
	}
	shf.r.wrap.b32 	%r1343, %r1342, %r1341, 24;
	shf.r.wrap.b32 	%r1344, %r1341, %r1342, 24;
	mov.b64 	%rd1313, {%r1344, %r1343};
	add.s64 	%rd1314, %rd1308, %rd1313;
	add.s64 	%rd1315, %rd1314, %rd1348;
	xor.b64  	%rd1316, %rd1310, %rd1315;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1345,%dummy}, %rd1316;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1346}, %rd1316;
	}
	shf.r.wrap.b32 	%r1347, %r1346, %r1345, 16;
	shf.r.wrap.b32 	%r1348, %r1345, %r1346, 16;
	mov.b64 	%rd1317, {%r1348, %r1347};
	add.s64 	%rd1318, %rd1311, %rd1317;
	xor.b64  	%rd1319, %rd1313, %rd1318;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1349}, %rd1319;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1350,%dummy}, %rd1319;
	}
	shf.l.wrap.b32 	%r1351, %r1350, %r1349, 1;
	shf.l.wrap.b32 	%r1352, %r1349, %r1350, 1;
	mov.b64 	%rd1320, {%r1352, %r1351};
	add.s64 	%rd1321, %rd1240, %rd1275;
	add.s64 	%rd1322, %rd1321, %rd1350;
	xor.b64  	%rd1323, %rd1264, %rd1322;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1353,%dummy}, %rd1323;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1354}, %rd1323;
	}
	mov.b64 	%rd1324, {%r1354, %r1353};
	add.s64 	%rd1325, %rd1252, %rd1324;
	xor.b64  	%rd1326, %rd1240, %rd1325;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1355,%dummy}, %rd1326;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1356}, %rd1326;
	}
	shf.r.wrap.b32 	%r1357, %r1356, %r1355, 24;
	shf.r.wrap.b32 	%r1358, %r1355, %r1356, 24;
	mov.b64 	%rd1327, {%r1358, %r1357};
	add.s64 	%rd1328, %rd1322, %rd1327;
	add.s64 	%rd1329, %rd1328, %rd1352;
	xor.b64  	%rd1330, %rd1324, %rd1329;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1359,%dummy}, %rd1330;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1360}, %rd1330;
	}
	shf.r.wrap.b32 	%r1361, %r1360, %r1359, 16;
	shf.r.wrap.b32 	%r1362, %r1359, %r1360, 16;
	mov.b64 	%rd1331, {%r1362, %r1361};
	add.s64 	%rd1332, %rd1325, %rd1331;
	xor.b64  	%rd1333, %rd1327, %rd1332;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1363}, %rd1333;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1364,%dummy}, %rd1333;
	}
	shf.l.wrap.b32 	%r1365, %r1364, %r1363, 1;
	shf.l.wrap.b32 	%r1366, %r1363, %r1364, 1;
	mov.b64 	%rd1334, {%r1366, %r1365};
	xor.b64  	%rd1335, %rd1288, %rd1318;
	xor.b64  	%rd1336, %rd1302, %rd1332;
	xor.b64  	%rd1354, %rd1336, -4942790177534073029;
	xor.b64  	%rd1337, %rd1315, %rd1291;
	xor.b64  	%rd1353, %rd1337, 4354685564936845355;
	xor.b64  	%rd1338, %rd1329, %rd1305;
	xor.b64  	%rd1352, %rd1338, -6534734903238641935;
	xor.b64  	%rd1339, %rd1334, %rd1304;
	xor.b64  	%rd1351, %rd1339, 5840696475078001361;
	xor.b64  	%rd1340, %rd1293, %rd1317;
	xor.b64  	%rd1350, %rd1340, -7276294671716946913;
	xor.b64  	%rd1341, %rd1307, %rd1331;
	xor.b64  	%rd1349, %rd1341, 2270897969802886507;
	xor.b64  	%rd1342, %rd1320, %rd1290;
	xor.b64  	%rd1348, %rd1342, 6620516959819538809;
	xor.b64  	%rd1355, %rd1335, 7640891576939301192;
	mov.u64 	%rd1346, 0;
	st.global.u64 	[%rd1356], %rd1355;
	st.global.u64 	[%rd1356+8], %rd1354;
	st.global.u64 	[%rd1356+16], %rd1353;
	st.global.u64 	[%rd1356+24], %rd1352;
	add.s64 	%rd1356, %rd1356, 32;
	add.s32 	%r1367, %r1367, 1;
	setp.ne.s32 	%p3, %r1367, 31;
	mov.u64 	%rd1347, %rd1346;
	@%p3 bra 	$L__BB0_2;

	shl.b64 	%rd1345, %rd44, 3;
	ld.param.u64 	%rd1344, [pre_processing_param_1];
	add.s64 	%rd1343, %rd1344, %rd1345;
	st.global.u64 	[%rd1343+992], %rd1351;
	st.global.u64 	[%rd1343+1000], %rd1350;
	st.global.u64 	[%rd1343+1008], %rd1349;
	st.global.u64 	[%rd1343+1016], %rd1348;

$L__BB0_4:
	ret;

}
	// .globl	argon2_kernel_segment_0
.entry argon2_kernel_segment_0(
	.param .u64 .ptr .global .align 8 argon2_kernel_segment_0_param_0,
	.param .u32 argon2_kernel_segment_0_param_1,
	.param .u32 argon2_kernel_segment_0_param_2,
	.param .u32 argon2_kernel_segment_0_param_3,
	.param .u32 argon2_kernel_segment_0_param_4,
	.param .u32 argon2_kernel_segment_0_param_5,
	.param .u32 argon2_kernel_segment_0_param_6
)
{
	.reg .pred 	%p<27>;
	.reg .b32 	%r<222>;
	.reg .b64 	%rd<283>;


	ld.param.u64 	%rd51, [argon2_kernel_segment_0_param_0];
	ld.param.u32 	%r29, [argon2_kernel_segment_0_param_2];
	ld.param.u32 	%r30, [argon2_kernel_segment_0_param_3];
	ld.param.u32 	%r31, [argon2_kernel_segment_0_param_4];
	ld.param.u32 	%r32, [argon2_kernel_segment_0_param_5];
	mov.b32 	%r33, %envreg4;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r36, %tid.y;
	add.s32 	%r37, %r36, %r33;
	mad.lo.s32 	%r38, %r35, %r34, %r37;
	mov.u32 	%r39, %ctaid.x;
	mov.u32 	%r40, %ntid.x;
	mov.u32 	%r1, %tid.x;
	mov.b32 	%r41, %envreg3;
	add.s32 	%r42, %r1, %r41;
	mad.lo.s32 	%r43, %r40, %r39, %r42;
	cvt.s64.s32 	%rd52, %r43;
	shr.u64 	%rd1, %rd52, 5;
	and.b32  	%r2, %r1, 31;
	cvt.u64.u32 	%rd2, %r29;
	mul.wide.u32 	%rd53, %r29, %r38;
	shl.b32 	%r3, %r30, 2;
	cvt.u64.u32 	%rd54, %r3;
	mul.lo.s64 	%rd3, %rd53, %rd54;
	mul.lo.s32 	%r44, %r30, %r29;
	mul.lo.s32 	%r45, %r44, %r32;
	cvt.u64.u32 	%rd55, %r45;
	and.b64  	%rd56, %rd1, 4294967295;
	add.s64 	%rd57, %rd3, %rd56;
	add.s64 	%rd4, %rd57, %rd55;
	setp.eq.s32 	%p3, %r31, 0;
	@%p3 bra 	$L__BB1_2;

	setp.eq.s32 	%p4, %r32, 0;
	mov.u32 	%r219, 0;
	mul.lo.s32 	%r47, %r3, %r29;
	selp.b32 	%r48, %r47, 0, %p4;
	cvt.u64.u32 	%rd58, %r48;
	sub.s64 	%rd59, %rd58, %rd2;
	add.s64 	%rd60, %rd59, %rd4;
	shl.b64 	%rd61, %rd60, 10;
	add.s64 	%rd62, %rd51, %rd61;
	mul.wide.u32 	%rd63, %r2, 8;
	add.s64 	%rd268, %rd62, %rd63;
	shl.b64 	%rd64, %rd4, 10;
	add.s64 	%rd65, %rd51, %rd64;
	add.s64 	%rd269, %rd65, %rd63;
	bra.uni 	$L__BB1_5;

$L__BB1_2:
	setp.eq.s32 	%p5, %r32, 0;
	@%p5 bra 	$L__BB1_4;

	sub.s64 	%rd66, %rd4, %rd2;
	shl.b64 	%rd67, %rd66, 10;
	add.s64 	%rd68, %rd51, %rd67;
	mul.wide.u32 	%rd69, %r2, 8;
	add.s64 	%rd268, %rd68, %rd69;
	shl.b64 	%rd70, %rd4, 10;
	add.s64 	%rd71, %rd51, %rd70;
	add.s64 	%rd269, %rd71, %rd69;
	mov.u32 	%r219, 0;
	bra.uni 	$L__BB1_5;

$L__BB1_4:
	add.s64 	%rd72, %rd4, %rd2;
	shl.b64 	%rd73, %rd72, 10;
	add.s64 	%rd74, %rd51, %rd73;
	mul.wide.u32 	%rd75, %r2, 8;
	add.s64 	%rd268, %rd74, %rd75;
	shl.b32 	%r51, %r29, 1;
	cvt.u64.u32 	%rd76, %r51;
	add.s64 	%rd77, %rd4, %rd76;
	shl.b64 	%rd78, %rd77, 10;
	add.s64 	%rd79, %rd51, %rd78;
	add.s64 	%rd269, %rd79, %rd75;
	mov.u32 	%r219, 2;

$L__BB1_5:
	setp.ge.u32 	%p6, %r219, %r30;
	@%p6 bra 	$L__BB1_14;

	cvt.u32.u64 	%r5, %rd1;
	or.b32  	%r52, %r32, %r31;
	ld.global.u64 	%rd274, [%rd268+768];
	ld.global.u64 	%rd273, [%rd268+512];
	ld.global.u64 	%rd272, [%rd268+256];
	ld.global.u64 	%rd271, [%rd268];
	add.s32 	%r53, %r29, -1;
	mov.u32 	%r54, -1;
	and.b32  	%r6, %r53, %r29;
	setp.ne.s32 	%p7, %r31, 0;
	setp.eq.s32 	%p1, %r52, 0;
	selp.b32 	%r55, 3, %r32, %p7;
	mul.lo.s32 	%r7, %r55, %r30;
	add.s32 	%r8, %r7, -1;
	setp.ne.s32 	%p8, %r32, 3;
	and.pred  	%p2, %p7, %p8;
	cvt.u64.u32 	%rd17, %r2;
	and.b32  	%r9, %r1, 12;
	shr.u32 	%r56, %r9, 2;
	add.s32 	%r57, %r1, -1;
	and.b32  	%r58, %r57, 3;
	and.b32  	%r59, %r1, 28;
	or.b32  	%r10, %r58, %r59;
	add.s32 	%r60, %r1, 2;
	and.b32  	%r61, %r60, 3;
	or.b32  	%r11, %r61, %r59;
	add.s32 	%r62, %r1, 1;
	and.b32  	%r63, %r62, 3;
	or.b32  	%r12, %r63, %r59;
	and.b32  	%r64, %r1, 16;
	shr.u32 	%r65, %r64, 3;
	and.b32  	%r66, %r1, 1;
	or.b32  	%r67, %r65, %r66;
	add.s32 	%r68, %r67, 1;
	shl.b32 	%r69, %r68, 3;
	and.b32  	%r70, %r69, 16;
	and.b32  	%r71, %r68, 1;
	and.b32  	%r72, %r1, 14;
	or.b32  	%r73, %r71, %r72;
	or.b32  	%r13, %r73, %r70;
	add.s32 	%r74, %r67, 2;
	shl.b32 	%r75, %r74, 3;
	and.b32  	%r76, %r75, 16;
	and.b32  	%r77, %r74, 1;
	or.b32  	%r78, %r77, %r72;
	or.b32  	%r14, %r78, %r76;
	add.s32 	%r79, %r67, 3;
	shl.b32 	%r80, %r79, 3;
	and.b32  	%r81, %r80, 16;
	and.b32  	%r82, %r79, 1;
	or.b32  	%r83, %r82, %r72;
	or.b32  	%r15, %r83, %r81;
	shl.b32 	%r84, %r29, 7;
	cvt.u64.u32 	%rd18, %r84;
	xor.b32  	%r16, %r56, 1;
	xor.b32  	%r17, %r2, 4;
	xor.b32  	%r18, %r56, 2;
	xor.b32  	%r19, %r2, 8;
	xor.b32  	%r20, %r56, 3;
	xor.b32  	%r21, %r2, 12;
	div.u32 	%r22, %r54, %r29;
	shl.b64 	%rd89, %rd17, 3;
	shl.b64 	%rd266, %rd18, 3;

$L__BB1_7:
	mov.u32 	%r85, 0;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd271;
	shfl.sync.idx.b32  r_lo, v_lo, %r85, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r85, 0x1f, 0xffffffff;
	mov.b64  %rd80, {r_hi,r_lo};
	}
	// end inline asm
	shr.u64 	%rd82, %rd80, 32;
	cvt.u32.u64 	%r24, %rd82;
	setp.eq.s32 	%p9, %r6, 0;
	@%p9 bra 	$L__BB1_9;

	mul.hi.u32 	%r86, %r24, %r22;
	mul.lo.s32 	%r87, %r86, %r29;
	sub.s32 	%r88, %r24, %r87;
	setp.lt.u32 	%p10, %r88, %r29;
	selp.b32 	%r89, 0, %r29, %p10;
	sub.s32 	%r221, %r88, %r89;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	and.b32  	%r221, %r53, %r24;

$L__BB1_10:
	ld.param.u64 	%rd267, [argon2_kernel_segment_0_param_0];
	ld.param.u32 	%r217, [argon2_kernel_segment_0_param_3];
	shl.b32 	%r216, %r217, 2;
	ld.param.u32 	%r215, [argon2_kernel_segment_0_param_5];
	selp.b32 	%r91, %r5, %r221, %p1;
	setp.ne.s32 	%p11, %r91, %r5;
	add.s32 	%r92, %r8, %r219;
	setp.gt.u32 	%p12, %r92, %r7;
	and.pred  	%p13, %p12, %p11;
	selp.b32 	%r93, %r7, %r92, %p13;
	cvt.u32.u64 	%r94, %rd80;
	mul.hi.u32 	%r95, %r94, %r94;
	mul.hi.u32 	%r96, %r93, %r95;
	not.b32 	%r97, %r96;
	add.s32 	%r98, %r93, %r97;
	add.s32 	%r99, %r215, 1;
	mad.lo.s32 	%r100, %r99, %r217, %r98;
	setp.lt.u32 	%p14, %r100, %r216;
	selp.b32 	%r102, 0, %r216, %p14;
	sub.s32 	%r103, %r100, %r102;
	selp.b32 	%r104, %r103, %r98, %p2;
	mul.lo.s32 	%r105, %r104, %r29;
	cvt.u64.u32 	%rd83, %r105;
	cvt.u64.u32 	%rd84, %r91;
	add.s64 	%rd85, %rd3, %rd84;
	add.s64 	%rd86, %rd85, %rd83;
	shl.b64 	%rd87, %rd86, 10;
	add.s64 	%rd88, %rd267, %rd87;
	add.s64 	%rd25, %rd88, %rd89;
	@%p3 bra 	$L__BB1_12;

	ld.global.u64 	%rd90, [%rd269];
	ld.global.u64 	%rd91, [%rd25];
	xor.b64  	%rd278, %rd91, %rd271;
	ld.global.u64 	%rd92, [%rd25+256];
	xor.b64  	%rd277, %rd92, %rd272;
	ld.global.u64 	%rd93, [%rd25+512];
	xor.b64  	%rd276, %rd93, %rd273;
	ld.global.u64 	%rd94, [%rd25+768];
	xor.b64  	%rd275, %rd94, %rd274;
	xor.b64  	%rd282, %rd278, %rd90;
	ld.global.u64 	%rd95, [%rd269+256];
	xor.b64  	%rd281, %rd277, %rd95;
	ld.global.u64 	%rd96, [%rd269+512];
	xor.b64  	%rd280, %rd276, %rd96;
	ld.global.u64 	%rd97, [%rd269+768];
	xor.b64  	%rd279, %rd275, %rd97;
	bra.uni 	$L__BB1_13;

$L__BB1_12:
	ld.global.u64 	%rd98, [%rd25];
	xor.b64  	%rd278, %rd98, %rd271;
	ld.global.u64 	%rd99, [%rd25+256];
	xor.b64  	%rd277, %rd99, %rd272;
	ld.global.u64 	%rd100, [%rd25+512];
	xor.b64  	%rd276, %rd100, %rd273;
	ld.global.u64 	%rd101, [%rd25+768];
	xor.b64  	%rd275, %rd101, %rd274;
	mov.u64 	%rd279, %rd275;
	mov.u64 	%rd280, %rd276;
	mov.u64 	%rd281, %rd277;
	mov.u64 	%rd282, %rd278;

$L__BB1_13:
	ld.param.u32 	%r218, [argon2_kernel_segment_0_param_3];
	setp.eq.s32 	%p16, %r9, 0;
	selp.b64 	%rd190, %rd277, %rd278, %p16;
	setp.eq.s32 	%p17, %r16, 2;
	selp.b64 	%rd191, %rd276, %rd190, %p17;
	setp.eq.s32 	%p18, %r16, 3;
	selp.b64 	%rd103, %rd275, %rd191, %p18;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd103;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd102, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p19, %r16, 0;
	selp.b64 	%rd192, %rd102, %rd278, %p19;
	selp.b64 	%rd193, %rd102, %rd277, %p16;
	selp.b64 	%rd194, %rd102, %rd276, %p17;
	selp.b64 	%rd195, %rd102, %rd275, %p18;
	setp.eq.s32 	%p20, %r18, 1;
	selp.b64 	%rd196, %rd193, %rd192, %p20;
	selp.b64 	%rd197, %rd194, %rd196, %p16;
	setp.eq.s32 	%p21, %r18, 3;
	selp.b64 	%rd105, %rd195, %rd197, %p21;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd105;
	shfl.sync.idx.b32  r_lo, v_lo, %r19, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r19, 0x1f, 0xffffffff;
	mov.b64  %rd104, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p22, %r18, 0;
	selp.b64 	%rd198, %rd104, %rd192, %p22;
	selp.b64 	%rd199, %rd104, %rd193, %p20;
	selp.b64 	%rd200, %rd104, %rd194, %p16;
	selp.b64 	%rd201, %rd104, %rd195, %p21;
	setp.eq.s32 	%p23, %r20, 1;
	selp.b64 	%rd202, %rd199, %rd198, %p23;
	setp.eq.s32 	%p24, %r20, 2;
	selp.b64 	%rd203, %rd200, %rd202, %p24;
	selp.b64 	%rd107, %rd201, %rd203, %p16;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd107;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd106, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p25, %r20, 0;
	selp.b64 	%rd204, %rd106, %rd198, %p25;
	selp.b64 	%rd109, %rd106, %rd199, %p23;
	selp.b64 	%rd205, %rd106, %rd200, %p24;
	selp.b64 	%rd206, %rd106, %rd201, %p16;
	cvt.u32.u64 	%r109, %rd204;
	cvt.u32.u64 	%r110, %rd109;
	// begin inline asm
	mul.wide.u32 %rd108, %r109, %r110;
	mad.lo.u64   %rd108, %rd108, 2, %rd109;
	// end inline asm
	add.s64 	%rd208, %rd108, %rd204;
	xor.b64  	%rd209, %rd208, %rd206;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r158,%dummy}, %rd209;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r159}, %rd209;
	}
	mov.b64 	%rd112, {%r159, %r158};
	cvt.u32.u64 	%r111, %rd205;
	cvt.u32.u64 	%r112, %rd112;
	// begin inline asm
	mul.wide.u32 %rd111, %r111, %r112;
	mad.lo.u64   %rd111, %rd111, 2, %rd112;
	// end inline asm
	add.s64 	%rd211, %rd111, %rd205;
	xor.b64  	%rd212, %rd211, %rd109;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r160,%dummy}, %rd212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r161}, %rd212;
	}
	shf.r.wrap.b32 	%r162, %r161, %r160, 24;
	shf.r.wrap.b32 	%r163, %r160, %r161, 24;
	mov.b64 	%rd115, {%r163, %r162};
	cvt.u32.u64 	%r113, %rd208;
	cvt.u32.u64 	%r114, %rd115;
	// begin inline asm
	mul.wide.u32 %rd114, %r113, %r114;
	mad.lo.u64   %rd114, %rd114, 2, %rd115;
	// end inline asm
	add.s64 	%rd214, %rd114, %rd208;
	xor.b64  	%rd215, %rd112, %rd214;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r164,%dummy}, %rd215;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r165}, %rd215;
	}
	shf.r.wrap.b32 	%r166, %r165, %r164, 16;
	shf.r.wrap.b32 	%r167, %r164, %r165, 16;
	mov.b64 	%rd125, {%r167, %r166};
	cvt.u32.u64 	%r115, %rd211;
	cvt.u32.u64 	%r116, %rd125;
	// begin inline asm
	mul.wide.u32 %rd117, %r115, %r116;
	mad.lo.u64   %rd117, %rd117, 2, %rd125;
	// end inline asm
	add.s64 	%rd123, %rd117, %rd211;
	xor.b64  	%rd217, %rd115, %rd123;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r168}, %rd217;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r169,%dummy}, %rd217;
	}
	shf.l.wrap.b32 	%r170, %r169, %r168, 1;
	shf.l.wrap.b32 	%r171, %r168, %r169, 1;
	mov.b64 	%rd121, {%r171, %r170};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd121;
	shfl.sync.idx.b32  r_lo, v_lo, %r12, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r12, 0x1f, 0xffffffff;
	mov.b64  %rd120, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd123;
	shfl.sync.idx.b32  r_lo, v_lo, %r11, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r11, 0x1f, 0xffffffff;
	mov.b64  %rd122, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd125;
	shfl.sync.idx.b32  r_lo, v_lo, %r10, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r10, 0x1f, 0xffffffff;
	mov.b64  %rd124, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r120, %rd214;
	cvt.u32.u64 	%r121, %rd120;
	// begin inline asm
	mul.wide.u32 %rd126, %r120, %r121;
	mad.lo.u64   %rd126, %rd126, 2, %rd120;
	// end inline asm
	add.s64 	%rd219, %rd126, %rd214;
	xor.b64  	%rd220, %rd219, %rd124;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r172,%dummy}, %rd220;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r173}, %rd220;
	}
	mov.b64 	%rd130, {%r173, %r172};
	cvt.u32.u64 	%r122, %rd122;
	cvt.u32.u64 	%r123, %rd130;
	// begin inline asm
	mul.wide.u32 %rd129, %r122, %r123;
	mad.lo.u64   %rd129, %rd129, 2, %rd130;
	// end inline asm
	add.s64 	%rd222, %rd129, %rd122;
	xor.b64  	%rd223, %rd222, %rd120;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r174,%dummy}, %rd223;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r175}, %rd223;
	}
	shf.r.wrap.b32 	%r176, %r175, %r174, 24;
	shf.r.wrap.b32 	%r177, %r174, %r175, 24;
	mov.b64 	%rd133, {%r177, %r176};
	cvt.u32.u64 	%r124, %rd219;
	cvt.u32.u64 	%r125, %rd133;
	// begin inline asm
	mul.wide.u32 %rd132, %r124, %r125;
	mad.lo.u64   %rd132, %rd132, 2, %rd133;
	// end inline asm
	add.s64 	%rd225, %rd132, %rd219;
	xor.b64  	%rd226, %rd130, %rd225;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r178,%dummy}, %rd226;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r179}, %rd226;
	}
	shf.r.wrap.b32 	%r180, %r179, %r178, 16;
	shf.r.wrap.b32 	%r181, %r178, %r179, 16;
	mov.b64 	%rd143, {%r181, %r180};
	cvt.u32.u64 	%r126, %rd222;
	cvt.u32.u64 	%r127, %rd143;
	// begin inline asm
	mul.wide.u32 %rd135, %r126, %r127;
	mad.lo.u64   %rd135, %rd135, 2, %rd143;
	// end inline asm
	add.s64 	%rd141, %rd135, %rd222;
	xor.b64  	%rd228, %rd133, %rd141;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r182}, %rd228;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r183,%dummy}, %rd228;
	}
	shf.l.wrap.b32 	%r184, %r183, %r182, 1;
	shf.l.wrap.b32 	%r185, %r182, %r183, 1;
	mov.b64 	%rd139, {%r185, %r184};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd139;
	shfl.sync.idx.b32  r_lo, v_lo, %r10, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r10, 0x1f, 0xffffffff;
	mov.b64  %rd138, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd141;
	shfl.sync.idx.b32  r_lo, v_lo, %r11, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r11, 0x1f, 0xffffffff;
	mov.b64  %rd140, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd143;
	shfl.sync.idx.b32  r_lo, v_lo, %r12, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r12, 0x1f, 0xffffffff;
	mov.b64  %rd142, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd229, %rd138, %rd225, %p16;
	selp.b64 	%rd230, %rd140, %rd229, %p17;
	selp.b64 	%rd145, %rd142, %rd230, %p18;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd145;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd144, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd231, %rd144, %rd225, %p19;
	selp.b64 	%rd232, %rd144, %rd138, %p16;
	selp.b64 	%rd233, %rd144, %rd140, %p17;
	selp.b64 	%rd234, %rd144, %rd142, %p18;
	selp.b64 	%rd235, %rd232, %rd231, %p20;
	selp.b64 	%rd236, %rd233, %rd235, %p16;
	selp.b64 	%rd147, %rd234, %rd236, %p21;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd147;
	shfl.sync.idx.b32  r_lo, v_lo, %r19, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r19, 0x1f, 0xffffffff;
	mov.b64  %rd146, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd237, %rd146, %rd231, %p22;
	selp.b64 	%rd238, %rd146, %rd232, %p20;
	selp.b64 	%rd239, %rd146, %rd233, %p16;
	selp.b64 	%rd240, %rd146, %rd234, %p21;
	selp.b64 	%rd241, %rd238, %rd237, %p23;
	selp.b64 	%rd242, %rd239, %rd241, %p24;
	selp.b64 	%rd149, %rd240, %rd242, %p16;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd149;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd148, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd243, %rd148, %rd237, %p25;
	selp.b64 	%rd151, %rd148, %rd238, %p23;
	selp.b64 	%rd244, %rd148, %rd239, %p24;
	selp.b64 	%rd245, %rd148, %rd240, %p16;
	cvt.u32.u64 	%r134, %rd243;
	cvt.u32.u64 	%r135, %rd151;
	// begin inline asm
	mul.wide.u32 %rd150, %r134, %r135;
	mad.lo.u64   %rd150, %rd150, 2, %rd151;
	// end inline asm
	add.s64 	%rd247, %rd150, %rd243;
	xor.b64  	%rd248, %rd247, %rd245;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r186,%dummy}, %rd248;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r187}, %rd248;
	}
	mov.b64 	%rd154, {%r187, %r186};
	cvt.u32.u64 	%r136, %rd244;
	cvt.u32.u64 	%r137, %rd154;
	// begin inline asm
	mul.wide.u32 %rd153, %r136, %r137;
	mad.lo.u64   %rd153, %rd153, 2, %rd154;
	// end inline asm
	add.s64 	%rd250, %rd153, %rd244;
	xor.b64  	%rd251, %rd250, %rd151;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r188,%dummy}, %rd251;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r189}, %rd251;
	}
	shf.r.wrap.b32 	%r190, %r189, %r188, 24;
	shf.r.wrap.b32 	%r191, %r188, %r189, 24;
	mov.b64 	%rd157, {%r191, %r190};
	cvt.u32.u64 	%r138, %rd247;
	cvt.u32.u64 	%r139, %rd157;
	// begin inline asm
	mul.wide.u32 %rd156, %r138, %r139;
	mad.lo.u64   %rd156, %rd156, 2, %rd157;
	// end inline asm
	add.s64 	%rd163, %rd156, %rd247;
	xor.b64  	%rd253, %rd154, %rd163;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r192,%dummy}, %rd253;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r193}, %rd253;
	}
	shf.r.wrap.b32 	%r194, %r193, %r192, 16;
	shf.r.wrap.b32 	%r195, %r192, %r193, 16;
	mov.b64 	%rd169, {%r195, %r194};
	cvt.u32.u64 	%r140, %rd250;
	cvt.u32.u64 	%r141, %rd169;
	// begin inline asm
	mul.wide.u32 %rd159, %r140, %r141;
	mad.lo.u64   %rd159, %rd159, 2, %rd169;
	// end inline asm
	add.s64 	%rd167, %rd159, %rd250;
	xor.b64  	%rd255, %rd157, %rd167;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r196}, %rd255;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r197,%dummy}, %rd255;
	}
	shf.l.wrap.b32 	%r198, %r197, %r196, 1;
	shf.l.wrap.b32 	%r199, %r196, %r197, 1;
	mov.b64 	%rd165, {%r199, %r198};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd163;
	shfl.sync.idx.b32  r_lo, v_lo, %r2, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r2, 0x1f, 0xffffffff;
	mov.b64  %rd162, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd165;
	shfl.sync.idx.b32  r_lo, v_lo, %r13, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r13, 0x1f, 0xffffffff;
	mov.b64  %rd164, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd167;
	shfl.sync.idx.b32  r_lo, v_lo, %r14, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r14, 0x1f, 0xffffffff;
	mov.b64  %rd166, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd169;
	shfl.sync.idx.b32  r_lo, v_lo, %r15, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r15, 0x1f, 0xffffffff;
	mov.b64  %rd168, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r146, %rd162;
	cvt.u32.u64 	%r147, %rd164;
	// begin inline asm
	mul.wide.u32 %rd170, %r146, %r147;
	mad.lo.u64   %rd170, %rd170, 2, %rd164;
	// end inline asm
	add.s64 	%rd257, %rd170, %rd162;
	xor.b64  	%rd258, %rd257, %rd168;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r201,%dummy}, %rd258;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r202}, %rd258;
	}
	mov.b64 	%rd174, {%r202, %r201};
	cvt.u32.u64 	%r148, %rd166;
	cvt.u32.u64 	%r149, %rd174;
	// begin inline asm
	mul.wide.u32 %rd173, %r148, %r149;
	mad.lo.u64   %rd173, %rd173, 2, %rd174;
	// end inline asm
	add.s64 	%rd260, %rd173, %rd166;
	xor.b64  	%rd261, %rd260, %rd164;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r203,%dummy}, %rd261;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r204}, %rd261;
	}
	shf.r.wrap.b32 	%r205, %r204, %r203, 24;
	shf.r.wrap.b32 	%r206, %r203, %r204, 24;
	mov.b64 	%rd177, {%r206, %r205};
	cvt.u32.u64 	%r150, %rd257;
	cvt.u32.u64 	%r151, %rd177;
	// begin inline asm
	mul.wide.u32 %rd176, %r150, %r151;
	mad.lo.u64   %rd176, %rd176, 2, %rd177;
	// end inline asm
	add.s64 	%rd183, %rd176, %rd257;
	xor.b64  	%rd263, %rd174, %rd183;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r207,%dummy}, %rd263;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r208}, %rd263;
	}
	shf.r.wrap.b32 	%r209, %r208, %r207, 16;
	shf.r.wrap.b32 	%r210, %r207, %r208, 16;
	mov.b64 	%rd189, {%r210, %r209};
	cvt.u32.u64 	%r152, %rd260;
	cvt.u32.u64 	%r153, %rd189;
	// begin inline asm
	mul.wide.u32 %rd179, %r152, %r153;
	mad.lo.u64   %rd179, %rd179, 2, %rd189;
	// end inline asm
	add.s64 	%rd187, %rd179, %rd260;
	xor.b64  	%rd265, %rd177, %rd187;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r211}, %rd265;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r212,%dummy}, %rd265;
	}
	shf.l.wrap.b32 	%r213, %r212, %r211, 1;
	shf.l.wrap.b32 	%r214, %r211, %r212, 1;
	mov.b64 	%rd185, {%r214, %r213};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd183;
	shfl.sync.idx.b32  r_lo, v_lo, %r2, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r2, 0x1f, 0xffffffff;
	mov.b64  %rd182, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd185;
	shfl.sync.idx.b32  r_lo, v_lo, %r15, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r15, 0x1f, 0xffffffff;
	mov.b64  %rd184, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd187;
	shfl.sync.idx.b32  r_lo, v_lo, %r14, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r14, 0x1f, 0xffffffff;
	mov.b64  %rd186, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd189;
	shfl.sync.idx.b32  r_lo, v_lo, %r13, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r13, 0x1f, 0xffffffff;
	mov.b64  %rd188, {r_hi,r_lo};
	}
	// end inline asm
	xor.b64  	%rd271, %rd182, %rd282;
	xor.b64  	%rd272, %rd184, %rd281;
	xor.b64  	%rd273, %rd186, %rd280;
	xor.b64  	%rd274, %rd188, %rd279;
	st.global.u64 	[%rd269], %rd271;
	st.global.u64 	[%rd269+256], %rd272;
	st.global.u64 	[%rd269+512], %rd273;
	st.global.u64 	[%rd269+768], %rd274;
	add.s64 	%rd269, %rd269, %rd266;
	add.s32 	%r219, %r219, 1;
	setp.lt.u32 	%p26, %r219, %r218;
	@%p26 bra 	$L__BB1_7;

$L__BB1_14:
	ret;

}
	// .globl	argon2_kernel_segment_1
.entry argon2_kernel_segment_1(
	.param .u64 .ptr .global .align 8 argon2_kernel_segment_1_param_0,
	.param .u32 argon2_kernel_segment_1_param_1,
	.param .u32 argon2_kernel_segment_1_param_2,
	.param .u32 argon2_kernel_segment_1_param_3,
	.param .u32 argon2_kernel_segment_1_param_4,
	.param .u32 argon2_kernel_segment_1_param_5,
	.param .u32 argon2_kernel_segment_1_param_6
)
{
	.reg .pred 	%p<68>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<703>;
	.reg .b64 	%rd<974>;


	ld.param.u64 	%rd71, [argon2_kernel_segment_1_param_0];
	ld.param.u32 	%r696, [argon2_kernel_segment_1_param_1];
	ld.param.u32 	%r41, [argon2_kernel_segment_1_param_2];
	ld.param.u32 	%r42, [argon2_kernel_segment_1_param_3];
	ld.param.u32 	%r43, [argon2_kernel_segment_1_param_4];
	ld.param.u32 	%r44, [argon2_kernel_segment_1_param_5];
	ld.param.u32 	%r45, [argon2_kernel_segment_1_param_6];
	mov.b32 	%r46, %envreg4;
	mov.u32 	%r47, %ctaid.y;
	mov.u32 	%r48, %ntid.y;
	mov.u32 	%r49, %tid.y;
	add.s32 	%r50, %r49, %r46;
	mad.lo.s32 	%r51, %r48, %r47, %r50;
	mov.u32 	%r52, %ctaid.x;
	mov.u32 	%r53, %ntid.x;
	mov.u32 	%r1, %tid.x;
	mov.b32 	%r54, %envreg3;
	add.s32 	%r55, %r1, %r54;
	mad.lo.s32 	%r56, %r53, %r52, %r55;
	cvt.s64.s32 	%rd72, %r56;
	shr.u64 	%rd1, %rd72, 5;
	cvt.u32.u64 	%r2, %rd1;
	and.b32  	%r3, %r1, 31;
	cvt.u64.u32 	%rd2, %r41;
	mul.wide.u32 	%rd73, %r41, %r51;
	shl.b32 	%r4, %r42, 2;
	cvt.u64.u32 	%rd74, %r4;
	mul.lo.s64 	%rd3, %rd73, %rd74;
	cvt.u16.u32 	%rs1, %r3;
	setp.gt.s16 	%p4, %rs1, 2;
	@%p4 bra 	$L__BB2_4;

	setp.eq.s16 	%p8, %rs1, 0;
	mov.u32 	%r696, %r43;
	@%p8 bra 	$L__BB2_8;

	setp.eq.s16 	%p9, %rs1, 1;
	mov.u32 	%r696, %r2;
	@%p9 bra 	$L__BB2_8;

	setp.eq.s16 	%p10, %rs1, 2;
	mov.u32 	%r696, %r44;
	@%p10 bra 	$L__BB2_8;
	bra.uni 	$L__BB2_7;

$L__BB2_4:
	setp.eq.s16 	%p5, %rs1, 3;
	@%p5 bra 	$L__BB2_27;

	setp.eq.s16 	%p6, %rs1, 4;
	@%p6 bra 	$L__BB2_8;

	setp.ne.s16 	%p7, %rs1, 5;
	mov.u32 	%r696, %r45;
	@%p7 bra 	$L__BB2_7;
	bra.uni 	$L__BB2_8;

$L__BB2_7:
	mov.u32 	%r696, 0;
	bra.uni 	$L__BB2_8;

$L__BB2_27:
	mul.lo.s32 	%r696, %r4, %r41;

$L__BB2_8:
	or.b32  	%r58, %r44, %r43;
	setp.ne.s32 	%p11, %r58, 0;
	setp.lt.u32 	%p12, %r42, 3;
	or.pred  	%p13, %p12, %p11;
	@%p13 bra 	$L__BB2_10;

	setp.eq.s32 	%p14, %r3, 6;
	selp.u32 	%r163, 1, 0, %p14;
	add.s32 	%r696, %r696, %r163;
	cvt.u64.u32 	%rd252, %r696;
	and.b32  	%r164, %r1, 12;
	shr.u32 	%r165, %r164, 2;
	xor.b32  	%r166, %r165, 1;
	setp.eq.s32 	%p15, %r164, 0;
	setp.eq.s32 	%p16, %r166, 2;
	setp.eq.s32 	%p17, %r166, 3;
	or.b32  	%r167, %r165, 1;
	setp.eq.s32 	%p18, %r167, 3;
	or.pred  	%p19, %p18, %p15;
	selp.b64 	%rd77, 0, %rd252, %p19;
	xor.b32  	%r136, %r3, 4;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd77;
	shfl.sync.idx.b32  r_lo, v_lo, %r136, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r136, 0x1f, 0xffffffff;
	mov.b64  %rd76, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p20, %r166, 0;
	selp.b64 	%rd253, %rd76, %rd252, %p20;
	selp.b64 	%rd254, %rd76, 0, %p15;
	selp.b64 	%rd255, %rd76, 0, %p16;
	selp.b64 	%rd256, %rd76, 0, %p17;
	xor.b32  	%r168, %r165, 2;
	setp.eq.s32 	%p21, %r168, 1;
	selp.b64 	%rd257, %rd254, %rd253, %p21;
	selp.b64 	%rd258, %rd255, %rd257, %p15;
	setp.eq.s32 	%p22, %r168, 3;
	selp.b64 	%rd79, %rd256, %rd258, %p22;
	xor.b32  	%r137, %r3, 8;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd79;
	shfl.sync.idx.b32  r_lo, v_lo, %r137, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r137, 0x1f, 0xffffffff;
	mov.b64  %rd78, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p23, %r168, 0;
	selp.b64 	%rd259, %rd78, %rd253, %p23;
	selp.b64 	%rd260, %rd78, %rd254, %p21;
	selp.b64 	%rd261, %rd78, %rd255, %p15;
	selp.b64 	%rd262, %rd78, %rd256, %p22;
	setp.eq.s32 	%p24, %r165, 2;
	selp.b64 	%rd263, %rd260, %rd259, %p24;
	setp.eq.s32 	%p25, %r165, 1;
	selp.b64 	%rd264, %rd261, %rd263, %p25;
	selp.b64 	%rd81, %rd262, %rd264, %p15;
	xor.b32  	%r138, %r3, 12;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd81;
	shfl.sync.idx.b32  r_lo, v_lo, %r138, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r138, 0x1f, 0xffffffff;
	mov.b64  %rd80, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p26, %r165, 3;
	selp.b64 	%rd265, %rd80, %rd259, %p26;
	selp.b64 	%rd83, %rd80, %rd260, %p24;
	selp.b64 	%rd266, %rd80, %rd261, %p25;
	selp.b64 	%rd267, %rd80, %rd262, %p15;
	cvt.u32.u64 	%r62, %rd265;
	cvt.u32.u64 	%r63, %rd83;
	// begin inline asm
	mul.wide.u32 %rd82, %r62, %r63;
	mad.lo.u64   %rd82, %rd82, 2, %rd83;
	// end inline asm
	add.s64 	%rd269, %rd82, %rd265;
	xor.b64  	%rd270, %rd269, %rd267;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r169,%dummy}, %rd270;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r170}, %rd270;
	}
	mov.b64 	%rd86, {%r170, %r169};
	cvt.u32.u64 	%r64, %rd266;
	cvt.u32.u64 	%r65, %rd86;
	// begin inline asm
	mul.wide.u32 %rd85, %r64, %r65;
	mad.lo.u64   %rd85, %rd85, 2, %rd86;
	// end inline asm
	add.s64 	%rd272, %rd85, %rd266;
	xor.b64  	%rd273, %rd272, %rd83;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r171,%dummy}, %rd273;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r172}, %rd273;
	}
	shf.r.wrap.b32 	%r173, %r172, %r171, 24;
	shf.r.wrap.b32 	%r174, %r171, %r172, 24;
	mov.b64 	%rd89, {%r174, %r173};
	cvt.u32.u64 	%r66, %rd269;
	cvt.u32.u64 	%r67, %rd89;
	// begin inline asm
	mul.wide.u32 %rd88, %r66, %r67;
	mad.lo.u64   %rd88, %rd88, 2, %rd89;
	// end inline asm
	add.s64 	%rd275, %rd88, %rd269;
	xor.b64  	%rd276, %rd86, %rd275;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r175,%dummy}, %rd276;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r176}, %rd276;
	}
	shf.r.wrap.b32 	%r177, %r176, %r175, 16;
	shf.r.wrap.b32 	%r178, %r175, %r176, 16;
	mov.b64 	%rd99, {%r178, %r177};
	cvt.u32.u64 	%r68, %rd272;
	cvt.u32.u64 	%r69, %rd99;
	// begin inline asm
	mul.wide.u32 %rd91, %r68, %r69;
	mad.lo.u64   %rd91, %rd91, 2, %rd99;
	// end inline asm
	add.s64 	%rd97, %rd91, %rd272;
	xor.b64  	%rd278, %rd89, %rd97;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r179}, %rd278;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r180,%dummy}, %rd278;
	}
	shf.l.wrap.b32 	%r181, %r180, %r179, 1;
	shf.l.wrap.b32 	%r182, %r179, %r180, 1;
	mov.b64 	%rd95, {%r182, %r181};
	add.s32 	%r183, %r1, -1;
	and.b32  	%r184, %r183, 3;
	and.b32  	%r185, %r1, 28;
	or.b32  	%r133, %r184, %r185;
	add.s32 	%r186, %r1, 2;
	and.b32  	%r187, %r186, 3;
	or.b32  	%r134, %r187, %r185;
	add.s32 	%r188, %r1, 1;
	and.b32  	%r189, %r188, 3;
	or.b32  	%r135, %r189, %r185;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd95;
	shfl.sync.idx.b32  r_lo, v_lo, %r135, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r135, 0x1f, 0xffffffff;
	mov.b64  %rd94, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd97;
	shfl.sync.idx.b32  r_lo, v_lo, %r134, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r134, 0x1f, 0xffffffff;
	mov.b64  %rd96, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd99;
	shfl.sync.idx.b32  r_lo, v_lo, %r133, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r133, 0x1f, 0xffffffff;
	mov.b64  %rd98, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r73, %rd275;
	cvt.u32.u64 	%r74, %rd94;
	// begin inline asm
	mul.wide.u32 %rd100, %r73, %r74;
	mad.lo.u64   %rd100, %rd100, 2, %rd94;
	// end inline asm
	add.s64 	%rd280, %rd100, %rd275;
	xor.b64  	%rd281, %rd280, %rd98;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r190,%dummy}, %rd281;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r191}, %rd281;
	}
	mov.b64 	%rd104, {%r191, %r190};
	cvt.u32.u64 	%r75, %rd96;
	cvt.u32.u64 	%r76, %rd104;
	// begin inline asm
	mul.wide.u32 %rd103, %r75, %r76;
	mad.lo.u64   %rd103, %rd103, 2, %rd104;
	// end inline asm
	add.s64 	%rd283, %rd103, %rd96;
	xor.b64  	%rd284, %rd283, %rd94;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r192,%dummy}, %rd284;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r193}, %rd284;
	}
	shf.r.wrap.b32 	%r194, %r193, %r192, 24;
	shf.r.wrap.b32 	%r195, %r192, %r193, 24;
	mov.b64 	%rd107, {%r195, %r194};
	cvt.u32.u64 	%r77, %rd280;
	cvt.u32.u64 	%r78, %rd107;
	// begin inline asm
	mul.wide.u32 %rd106, %r77, %r78;
	mad.lo.u64   %rd106, %rd106, 2, %rd107;
	// end inline asm
	add.s64 	%rd286, %rd106, %rd280;
	xor.b64  	%rd287, %rd104, %rd286;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r196,%dummy}, %rd287;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r197}, %rd287;
	}
	shf.r.wrap.b32 	%r198, %r197, %r196, 16;
	shf.r.wrap.b32 	%r199, %r196, %r197, 16;
	mov.b64 	%rd117, {%r199, %r198};
	cvt.u32.u64 	%r79, %rd283;
	cvt.u32.u64 	%r80, %rd117;
	// begin inline asm
	mul.wide.u32 %rd109, %r79, %r80;
	mad.lo.u64   %rd109, %rd109, 2, %rd117;
	// end inline asm
	add.s64 	%rd115, %rd109, %rd283;
	xor.b64  	%rd289, %rd107, %rd115;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r200}, %rd289;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r201,%dummy}, %rd289;
	}
	shf.l.wrap.b32 	%r202, %r201, %r200, 1;
	shf.l.wrap.b32 	%r203, %r200, %r201, 1;
	mov.b64 	%rd113, {%r203, %r202};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd113;
	shfl.sync.idx.b32  r_lo, v_lo, %r133, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r133, 0x1f, 0xffffffff;
	mov.b64  %rd112, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd115;
	shfl.sync.idx.b32  r_lo, v_lo, %r134, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r134, 0x1f, 0xffffffff;
	mov.b64  %rd114, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd117;
	shfl.sync.idx.b32  r_lo, v_lo, %r135, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r135, 0x1f, 0xffffffff;
	mov.b64  %rd116, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd290, %rd112, %rd286, %p15;
	selp.b64 	%rd291, %rd114, %rd290, %p16;
	selp.b64 	%rd119, %rd116, %rd291, %p17;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd119;
	shfl.sync.idx.b32  r_lo, v_lo, %r136, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r136, 0x1f, 0xffffffff;
	mov.b64  %rd118, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd292, %rd118, %rd286, %p20;
	selp.b64 	%rd293, %rd118, %rd112, %p15;
	selp.b64 	%rd294, %rd118, %rd114, %p16;
	selp.b64 	%rd295, %rd118, %rd116, %p17;
	selp.b64 	%rd296, %rd293, %rd292, %p21;
	selp.b64 	%rd297, %rd294, %rd296, %p15;
	selp.b64 	%rd121, %rd295, %rd297, %p22;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd121;
	shfl.sync.idx.b32  r_lo, v_lo, %r137, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r137, 0x1f, 0xffffffff;
	mov.b64  %rd120, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd298, %rd120, %rd292, %p23;
	selp.b64 	%rd299, %rd120, %rd293, %p21;
	selp.b64 	%rd300, %rd120, %rd294, %p15;
	selp.b64 	%rd301, %rd120, %rd295, %p22;
	selp.b64 	%rd302, %rd299, %rd298, %p24;
	selp.b64 	%rd303, %rd300, %rd302, %p25;
	selp.b64 	%rd123, %rd301, %rd303, %p15;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd123;
	shfl.sync.idx.b32  r_lo, v_lo, %r138, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r138, 0x1f, 0xffffffff;
	mov.b64  %rd122, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd304, %rd122, %rd298, %p26;
	selp.b64 	%rd125, %rd122, %rd299, %p24;
	selp.b64 	%rd305, %rd122, %rd300, %p25;
	selp.b64 	%rd306, %rd122, %rd301, %p15;
	cvt.u32.u64 	%r87, %rd304;
	cvt.u32.u64 	%r88, %rd125;
	// begin inline asm
	mul.wide.u32 %rd124, %r87, %r88;
	mad.lo.u64   %rd124, %rd124, 2, %rd125;
	// end inline asm
	add.s64 	%rd308, %rd124, %rd304;
	xor.b64  	%rd309, %rd308, %rd306;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r204,%dummy}, %rd309;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r205}, %rd309;
	}
	mov.b64 	%rd128, {%r205, %r204};
	cvt.u32.u64 	%r89, %rd305;
	cvt.u32.u64 	%r90, %rd128;
	// begin inline asm
	mul.wide.u32 %rd127, %r89, %r90;
	mad.lo.u64   %rd127, %rd127, 2, %rd128;
	// end inline asm
	add.s64 	%rd311, %rd127, %rd305;
	xor.b64  	%rd312, %rd311, %rd125;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r206,%dummy}, %rd312;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r207}, %rd312;
	}
	shf.r.wrap.b32 	%r208, %r207, %r206, 24;
	shf.r.wrap.b32 	%r209, %r206, %r207, 24;
	mov.b64 	%rd131, {%r209, %r208};
	cvt.u32.u64 	%r91, %rd308;
	cvt.u32.u64 	%r92, %rd131;
	// begin inline asm
	mul.wide.u32 %rd130, %r91, %r92;
	mad.lo.u64   %rd130, %rd130, 2, %rd131;
	// end inline asm
	add.s64 	%rd137, %rd130, %rd308;
	xor.b64  	%rd314, %rd128, %rd137;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r210,%dummy}, %rd314;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r211}, %rd314;
	}
	shf.r.wrap.b32 	%r212, %r211, %r210, 16;
	shf.r.wrap.b32 	%r213, %r210, %r211, 16;
	mov.b64 	%rd143, {%r213, %r212};
	cvt.u32.u64 	%r93, %rd311;
	cvt.u32.u64 	%r94, %rd143;
	// begin inline asm
	mul.wide.u32 %rd133, %r93, %r94;
	mad.lo.u64   %rd133, %rd133, 2, %rd143;
	// end inline asm
	add.s64 	%rd141, %rd133, %rd311;
	xor.b64  	%rd316, %rd131, %rd141;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r214}, %rd316;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r215,%dummy}, %rd316;
	}
	shf.l.wrap.b32 	%r216, %r215, %r214, 1;
	shf.l.wrap.b32 	%r217, %r214, %r215, 1;
	mov.b64 	%rd139, {%r217, %r216};
	and.b32  	%r218, %r1, 16;
	shr.u32 	%r219, %r218, 3;
	and.b32  	%r220, %r1, 1;
	or.b32  	%r221, %r219, %r220;
	add.s32 	%r222, %r221, 1;
	shl.b32 	%r223, %r222, 3;
	and.b32  	%r224, %r223, 16;
	and.b32  	%r225, %r222, 1;
	and.b32  	%r226, %r1, 14;
	or.b32  	%r227, %r225, %r226;
	or.b32  	%r162, %r227, %r224;
	add.s32 	%r228, %r221, 2;
	shl.b32 	%r229, %r228, 3;
	and.b32  	%r230, %r229, 16;
	and.b32  	%r231, %r228, 1;
	or.b32  	%r232, %r231, %r226;
	or.b32  	%r161, %r232, %r230;
	add.s32 	%r233, %r221, 3;
	shl.b32 	%r234, %r233, 3;
	and.b32  	%r235, %r234, 16;
	and.b32  	%r236, %r233, 1;
	or.b32  	%r237, %r236, %r226;
	or.b32  	%r160, %r237, %r235;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd137;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd136, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd139;
	shfl.sync.idx.b32  r_lo, v_lo, %r162, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r162, 0x1f, 0xffffffff;
	mov.b64  %rd138, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd141;
	shfl.sync.idx.b32  r_lo, v_lo, %r161, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r161, 0x1f, 0xffffffff;
	mov.b64  %rd140, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd143;
	shfl.sync.idx.b32  r_lo, v_lo, %r160, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r160, 0x1f, 0xffffffff;
	mov.b64  %rd142, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r99, %rd136;
	cvt.u32.u64 	%r100, %rd138;
	// begin inline asm
	mul.wide.u32 %rd144, %r99, %r100;
	mad.lo.u64   %rd144, %rd144, 2, %rd138;
	// end inline asm
	add.s64 	%rd318, %rd144, %rd136;
	xor.b64  	%rd319, %rd318, %rd142;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r238,%dummy}, %rd319;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r239}, %rd319;
	}
	mov.b64 	%rd148, {%r239, %r238};
	cvt.u32.u64 	%r101, %rd140;
	cvt.u32.u64 	%r102, %rd148;
	// begin inline asm
	mul.wide.u32 %rd147, %r101, %r102;
	mad.lo.u64   %rd147, %rd147, 2, %rd148;
	// end inline asm
	add.s64 	%rd321, %rd147, %rd140;
	xor.b64  	%rd322, %rd321, %rd138;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r240,%dummy}, %rd322;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r241}, %rd322;
	}
	shf.r.wrap.b32 	%r242, %r241, %r240, 24;
	shf.r.wrap.b32 	%r243, %r240, %r241, 24;
	mov.b64 	%rd151, {%r243, %r242};
	cvt.u32.u64 	%r103, %rd318;
	cvt.u32.u64 	%r104, %rd151;
	// begin inline asm
	mul.wide.u32 %rd150, %r103, %r104;
	mad.lo.u64   %rd150, %rd150, 2, %rd151;
	// end inline asm
	add.s64 	%rd157, %rd150, %rd318;
	xor.b64  	%rd324, %rd148, %rd157;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r244,%dummy}, %rd324;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r245}, %rd324;
	}
	shf.r.wrap.b32 	%r246, %r245, %r244, 16;
	shf.r.wrap.b32 	%r247, %r244, %r245, 16;
	mov.b64 	%rd163, {%r247, %r246};
	cvt.u32.u64 	%r105, %rd321;
	cvt.u32.u64 	%r106, %rd163;
	// begin inline asm
	mul.wide.u32 %rd153, %r105, %r106;
	mad.lo.u64   %rd153, %rd153, 2, %rd163;
	// end inline asm
	add.s64 	%rd161, %rd153, %rd321;
	xor.b64  	%rd326, %rd151, %rd161;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r248}, %rd326;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r249,%dummy}, %rd326;
	}
	shf.l.wrap.b32 	%r250, %r249, %r248, 1;
	shf.l.wrap.b32 	%r251, %r248, %r249, 1;
	mov.b64 	%rd159, {%r251, %r250};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd157;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd156, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd159;
	shfl.sync.idx.b32  r_lo, v_lo, %r160, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r160, 0x1f, 0xffffffff;
	mov.b64  %rd158, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd161;
	shfl.sync.idx.b32  r_lo, v_lo, %r161, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r161, 0x1f, 0xffffffff;
	mov.b64  %rd160, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd163;
	shfl.sync.idx.b32  r_lo, v_lo, %r162, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r162, 0x1f, 0xffffffff;
	mov.b64  %rd162, {r_hi,r_lo};
	}
	// end inline asm
	xor.b64  	%rd327, %rd156, %rd252;
	selp.b64 	%rd328, %rd158, %rd327, %p15;
	selp.b64 	%rd329, %rd160, %rd328, %p16;
	selp.b64 	%rd165, %rd162, %rd329, %p17;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd165;
	shfl.sync.idx.b32  r_lo, v_lo, %r136, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r136, 0x1f, 0xffffffff;
	mov.b64  %rd164, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd330, %rd164, %rd327, %p20;
	selp.b64 	%rd331, %rd164, %rd158, %p15;
	selp.b64 	%rd332, %rd164, %rd160, %p16;
	selp.b64 	%rd333, %rd164, %rd162, %p17;
	selp.b64 	%rd334, %rd331, %rd330, %p21;
	selp.b64 	%rd335, %rd332, %rd334, %p15;
	selp.b64 	%rd167, %rd333, %rd335, %p22;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd167;
	shfl.sync.idx.b32  r_lo, v_lo, %r137, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r137, 0x1f, 0xffffffff;
	mov.b64  %rd166, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd336, %rd166, %rd330, %p23;
	selp.b64 	%rd337, %rd166, %rd331, %p21;
	selp.b64 	%rd338, %rd166, %rd332, %p15;
	selp.b64 	%rd339, %rd166, %rd333, %p22;
	selp.b64 	%rd340, %rd337, %rd336, %p24;
	selp.b64 	%rd341, %rd338, %rd340, %p25;
	selp.b64 	%rd169, %rd339, %rd341, %p15;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd169;
	shfl.sync.idx.b32  r_lo, v_lo, %r138, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r138, 0x1f, 0xffffffff;
	mov.b64  %rd168, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd342, %rd168, %rd336, %p26;
	selp.b64 	%rd171, %rd168, %rd337, %p24;
	selp.b64 	%rd343, %rd168, %rd338, %p25;
	selp.b64 	%rd344, %rd168, %rd339, %p15;
	cvt.u32.u64 	%r114, %rd342;
	cvt.u32.u64 	%r115, %rd171;
	// begin inline asm
	mul.wide.u32 %rd170, %r114, %r115;
	mad.lo.u64   %rd170, %rd170, 2, %rd171;
	// end inline asm
	add.s64 	%rd346, %rd170, %rd342;
	xor.b64  	%rd347, %rd346, %rd344;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r252,%dummy}, %rd347;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r253}, %rd347;
	}
	mov.b64 	%rd174, {%r253, %r252};
	cvt.u32.u64 	%r116, %rd343;
	cvt.u32.u64 	%r117, %rd174;
	// begin inline asm
	mul.wide.u32 %rd173, %r116, %r117;
	mad.lo.u64   %rd173, %rd173, 2, %rd174;
	// end inline asm
	add.s64 	%rd349, %rd173, %rd343;
	xor.b64  	%rd350, %rd349, %rd171;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r254,%dummy}, %rd350;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r255}, %rd350;
	}
	shf.r.wrap.b32 	%r256, %r255, %r254, 24;
	shf.r.wrap.b32 	%r257, %r254, %r255, 24;
	mov.b64 	%rd177, {%r257, %r256};
	cvt.u32.u64 	%r118, %rd346;
	cvt.u32.u64 	%r119, %rd177;
	// begin inline asm
	mul.wide.u32 %rd176, %r118, %r119;
	mad.lo.u64   %rd176, %rd176, 2, %rd177;
	// end inline asm
	add.s64 	%rd352, %rd176, %rd346;
	xor.b64  	%rd353, %rd174, %rd352;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r258,%dummy}, %rd353;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r259}, %rd353;
	}
	shf.r.wrap.b32 	%r260, %r259, %r258, 16;
	shf.r.wrap.b32 	%r261, %r258, %r259, 16;
	mov.b64 	%rd187, {%r261, %r260};
	cvt.u32.u64 	%r120, %rd349;
	cvt.u32.u64 	%r121, %rd187;
	// begin inline asm
	mul.wide.u32 %rd179, %r120, %r121;
	mad.lo.u64   %rd179, %rd179, 2, %rd187;
	// end inline asm
	add.s64 	%rd185, %rd179, %rd349;
	xor.b64  	%rd355, %rd177, %rd185;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r262}, %rd355;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r263,%dummy}, %rd355;
	}
	shf.l.wrap.b32 	%r264, %r263, %r262, 1;
	shf.l.wrap.b32 	%r265, %r262, %r263, 1;
	mov.b64 	%rd183, {%r265, %r264};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd183;
	shfl.sync.idx.b32  r_lo, v_lo, %r135, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r135, 0x1f, 0xffffffff;
	mov.b64  %rd182, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd185;
	shfl.sync.idx.b32  r_lo, v_lo, %r134, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r134, 0x1f, 0xffffffff;
	mov.b64  %rd184, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd187;
	shfl.sync.idx.b32  r_lo, v_lo, %r133, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r133, 0x1f, 0xffffffff;
	mov.b64  %rd186, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r125, %rd352;
	cvt.u32.u64 	%r126, %rd182;
	// begin inline asm
	mul.wide.u32 %rd188, %r125, %r126;
	mad.lo.u64   %rd188, %rd188, 2, %rd182;
	// end inline asm
	add.s64 	%rd357, %rd188, %rd352;
	xor.b64  	%rd358, %rd357, %rd186;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r266,%dummy}, %rd358;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r267}, %rd358;
	}
	mov.b64 	%rd192, {%r267, %r266};
	cvt.u32.u64 	%r127, %rd184;
	cvt.u32.u64 	%r128, %rd192;
	// begin inline asm
	mul.wide.u32 %rd191, %r127, %r128;
	mad.lo.u64   %rd191, %rd191, 2, %rd192;
	// end inline asm
	add.s64 	%rd360, %rd191, %rd184;
	xor.b64  	%rd361, %rd360, %rd182;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r268,%dummy}, %rd361;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r269}, %rd361;
	}
	shf.r.wrap.b32 	%r270, %r269, %r268, 24;
	shf.r.wrap.b32 	%r271, %r268, %r269, 24;
	mov.b64 	%rd195, {%r271, %r270};
	cvt.u32.u64 	%r129, %rd357;
	cvt.u32.u64 	%r130, %rd195;
	// begin inline asm
	mul.wide.u32 %rd194, %r129, %r130;
	mad.lo.u64   %rd194, %rd194, 2, %rd195;
	// end inline asm
	add.s64 	%rd363, %rd194, %rd357;
	xor.b64  	%rd364, %rd192, %rd363;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r272,%dummy}, %rd364;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r273}, %rd364;
	}
	shf.r.wrap.b32 	%r274, %r273, %r272, 16;
	shf.r.wrap.b32 	%r275, %r272, %r273, 16;
	mov.b64 	%rd205, {%r275, %r274};
	cvt.u32.u64 	%r131, %rd360;
	cvt.u32.u64 	%r132, %rd205;
	// begin inline asm
	mul.wide.u32 %rd197, %r131, %r132;
	mad.lo.u64   %rd197, %rd197, 2, %rd205;
	// end inline asm
	add.s64 	%rd203, %rd197, %rd360;
	xor.b64  	%rd366, %rd195, %rd203;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r276}, %rd366;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r277,%dummy}, %rd366;
	}
	shf.l.wrap.b32 	%r278, %r277, %r276, 1;
	shf.l.wrap.b32 	%r279, %r276, %r277, 1;
	mov.b64 	%rd201, {%r279, %r278};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd201;
	shfl.sync.idx.b32  r_lo, v_lo, %r133, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r133, 0x1f, 0xffffffff;
	mov.b64  %rd200, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd203;
	shfl.sync.idx.b32  r_lo, v_lo, %r134, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r134, 0x1f, 0xffffffff;
	mov.b64  %rd202, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd205;
	shfl.sync.idx.b32  r_lo, v_lo, %r135, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r135, 0x1f, 0xffffffff;
	mov.b64  %rd204, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd367, %rd200, %rd363, %p15;
	selp.b64 	%rd368, %rd202, %rd367, %p16;
	selp.b64 	%rd207, %rd204, %rd368, %p17;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd207;
	shfl.sync.idx.b32  r_lo, v_lo, %r136, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r136, 0x1f, 0xffffffff;
	mov.b64  %rd206, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd369, %rd206, %rd363, %p20;
	selp.b64 	%rd370, %rd206, %rd200, %p15;
	selp.b64 	%rd371, %rd206, %rd202, %p16;
	selp.b64 	%rd372, %rd206, %rd204, %p17;
	selp.b64 	%rd373, %rd370, %rd369, %p21;
	selp.b64 	%rd374, %rd371, %rd373, %p15;
	selp.b64 	%rd209, %rd372, %rd374, %p22;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd209;
	shfl.sync.idx.b32  r_lo, v_lo, %r137, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r137, 0x1f, 0xffffffff;
	mov.b64  %rd208, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd375, %rd208, %rd369, %p23;
	selp.b64 	%rd376, %rd208, %rd370, %p21;
	selp.b64 	%rd377, %rd208, %rd371, %p15;
	selp.b64 	%rd378, %rd208, %rd372, %p22;
	selp.b64 	%rd379, %rd376, %rd375, %p24;
	selp.b64 	%rd380, %rd377, %rd379, %p25;
	selp.b64 	%rd211, %rd378, %rd380, %p15;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd211;
	shfl.sync.idx.b32  r_lo, v_lo, %r138, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r138, 0x1f, 0xffffffff;
	mov.b64  %rd210, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd381, %rd210, %rd375, %p26;
	selp.b64 	%rd213, %rd210, %rd376, %p24;
	selp.b64 	%rd382, %rd210, %rd377, %p25;
	selp.b64 	%rd383, %rd210, %rd378, %p15;
	cvt.u32.u64 	%r139, %rd381;
	cvt.u32.u64 	%r140, %rd213;
	// begin inline asm
	mul.wide.u32 %rd212, %r139, %r140;
	mad.lo.u64   %rd212, %rd212, 2, %rd213;
	// end inline asm
	add.s64 	%rd385, %rd212, %rd381;
	xor.b64  	%rd386, %rd385, %rd383;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r280,%dummy}, %rd386;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r281}, %rd386;
	}
	mov.b64 	%rd216, {%r281, %r280};
	cvt.u32.u64 	%r141, %rd382;
	cvt.u32.u64 	%r142, %rd216;
	// begin inline asm
	mul.wide.u32 %rd215, %r141, %r142;
	mad.lo.u64   %rd215, %rd215, 2, %rd216;
	// end inline asm
	add.s64 	%rd388, %rd215, %rd382;
	xor.b64  	%rd389, %rd388, %rd213;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r282,%dummy}, %rd389;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r283}, %rd389;
	}
	shf.r.wrap.b32 	%r284, %r283, %r282, 24;
	shf.r.wrap.b32 	%r285, %r282, %r283, 24;
	mov.b64 	%rd219, {%r285, %r284};
	cvt.u32.u64 	%r143, %rd385;
	cvt.u32.u64 	%r144, %rd219;
	// begin inline asm
	mul.wide.u32 %rd218, %r143, %r144;
	mad.lo.u64   %rd218, %rd218, 2, %rd219;
	// end inline asm
	add.s64 	%rd225, %rd218, %rd385;
	xor.b64  	%rd391, %rd216, %rd225;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r286,%dummy}, %rd391;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r287}, %rd391;
	}
	shf.r.wrap.b32 	%r288, %r287, %r286, 16;
	shf.r.wrap.b32 	%r289, %r286, %r287, 16;
	mov.b64 	%rd231, {%r289, %r288};
	cvt.u32.u64 	%r145, %rd388;
	cvt.u32.u64 	%r146, %rd231;
	// begin inline asm
	mul.wide.u32 %rd221, %r145, %r146;
	mad.lo.u64   %rd221, %rd221, 2, %rd231;
	// end inline asm
	add.s64 	%rd229, %rd221, %rd388;
	xor.b64  	%rd393, %rd219, %rd229;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r290}, %rd393;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r291,%dummy}, %rd393;
	}
	shf.l.wrap.b32 	%r292, %r291, %r290, 1;
	shf.l.wrap.b32 	%r293, %r290, %r291, 1;
	mov.b64 	%rd227, {%r293, %r292};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd225;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd224, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd227;
	shfl.sync.idx.b32  r_lo, v_lo, %r162, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r162, 0x1f, 0xffffffff;
	mov.b64  %rd226, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd229;
	shfl.sync.idx.b32  r_lo, v_lo, %r161, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r161, 0x1f, 0xffffffff;
	mov.b64  %rd228, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd231;
	shfl.sync.idx.b32  r_lo, v_lo, %r160, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r160, 0x1f, 0xffffffff;
	mov.b64  %rd230, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r151, %rd224;
	cvt.u32.u64 	%r152, %rd226;
	// begin inline asm
	mul.wide.u32 %rd232, %r151, %r152;
	mad.lo.u64   %rd232, %rd232, 2, %rd226;
	// end inline asm
	add.s64 	%rd395, %rd232, %rd224;
	xor.b64  	%rd396, %rd395, %rd230;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r294,%dummy}, %rd396;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r295}, %rd396;
	}
	mov.b64 	%rd236, {%r295, %r294};
	cvt.u32.u64 	%r153, %rd228;
	cvt.u32.u64 	%r154, %rd236;
	// begin inline asm
	mul.wide.u32 %rd235, %r153, %r154;
	mad.lo.u64   %rd235, %rd235, 2, %rd236;
	// end inline asm
	add.s64 	%rd398, %rd235, %rd228;
	xor.b64  	%rd399, %rd398, %rd226;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r296,%dummy}, %rd399;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r297}, %rd399;
	}
	shf.r.wrap.b32 	%r298, %r297, %r296, 24;
	shf.r.wrap.b32 	%r299, %r296, %r297, 24;
	mov.b64 	%rd239, {%r299, %r298};
	cvt.u32.u64 	%r155, %rd395;
	cvt.u32.u64 	%r156, %rd239;
	// begin inline asm
	mul.wide.u32 %rd238, %r155, %r156;
	mad.lo.u64   %rd238, %rd238, 2, %rd239;
	// end inline asm
	add.s64 	%rd245, %rd238, %rd395;
	xor.b64  	%rd401, %rd236, %rd245;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r300,%dummy}, %rd401;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r301}, %rd401;
	}
	shf.r.wrap.b32 	%r302, %r301, %r300, 16;
	shf.r.wrap.b32 	%r303, %r300, %r301, 16;
	mov.b64 	%rd251, {%r303, %r302};
	cvt.u32.u64 	%r157, %rd398;
	cvt.u32.u64 	%r158, %rd251;
	// begin inline asm
	mul.wide.u32 %rd241, %r157, %r158;
	mad.lo.u64   %rd241, %rd241, 2, %rd251;
	// end inline asm
	add.s64 	%rd249, %rd241, %rd398;
	xor.b64  	%rd403, %rd239, %rd249;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r304}, %rd403;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r305,%dummy}, %rd403;
	}
	shf.l.wrap.b32 	%r306, %r305, %r304, 1;
	shf.l.wrap.b32 	%r307, %r304, %r305, 1;
	mov.b64 	%rd247, {%r307, %r306};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd245;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd244, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd247;
	shfl.sync.idx.b32  r_lo, v_lo, %r160, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r160, 0x1f, 0xffffffff;
	mov.b64  %rd246, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd249;
	shfl.sync.idx.b32  r_lo, v_lo, %r161, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r161, 0x1f, 0xffffffff;
	mov.b64  %rd248, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd251;
	shfl.sync.idx.b32  r_lo, v_lo, %r162, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r162, 0x1f, 0xffffffff;
	mov.b64  %rd250, {r_hi,r_lo};
	}
	// end inline asm
	xor.b64  	%rd965, %rd244, %rd327;
	xor.b64  	%rd964, %rd246, %rd158;
	xor.b64  	%rd963, %rd248, %rd160;
	xor.b64  	%rd962, %rd250, %rd162;

$L__BB2_10:
	mul.lo.s32 	%r308, %r42, %r41;
	mul.lo.s32 	%r309, %r308, %r44;
	cvt.u64.u32 	%rd404, %r309;
	and.b64  	%rd405, %rd1, 4294967295;
	add.s64 	%rd406, %rd3, %rd405;
	add.s64 	%rd12, %rd406, %rd404;
	setp.eq.s32 	%p27, %r43, 0;
	@%p27 bra 	$L__BB2_12;

	setp.eq.s32 	%p28, %r44, 0;
	mov.u32 	%r698, 0;
	mul.lo.s32 	%r311, %r4, %r41;
	selp.b32 	%r312, %r311, 0, %p28;
	cvt.u64.u32 	%rd407, %r312;
	sub.s64 	%rd408, %rd407, %rd2;
	add.s64 	%rd409, %rd408, %rd12;
	shl.b64 	%rd410, %rd409, 10;
	add.s64 	%rd411, %rd71, %rd410;
	mul.wide.u32 	%rd412, %r3, 8;
	add.s64 	%rd951, %rd411, %rd412;
	shl.b64 	%rd413, %rd12, 10;
	add.s64 	%rd414, %rd71, %rd413;
	add.s64 	%rd952, %rd414, %rd412;
	bra.uni 	$L__BB2_15;

$L__BB2_12:
	setp.eq.s32 	%p29, %r44, 0;
	@%p29 bra 	$L__BB2_14;

	sub.s64 	%rd415, %rd12, %rd2;
	shl.b64 	%rd416, %rd415, 10;
	add.s64 	%rd417, %rd71, %rd416;
	mul.wide.u32 	%rd418, %r3, 8;
	add.s64 	%rd951, %rd417, %rd418;
	shl.b64 	%rd419, %rd12, 10;
	add.s64 	%rd420, %rd71, %rd419;
	add.s64 	%rd952, %rd420, %rd418;
	mov.u32 	%r698, 0;
	bra.uni 	$L__BB2_15;

$L__BB2_14:
	add.s64 	%rd421, %rd12, %rd2;
	shl.b64 	%rd422, %rd421, 10;
	add.s64 	%rd423, %rd71, %rd422;
	mul.wide.u32 	%rd424, %r3, 8;
	add.s64 	%rd951, %rd423, %rd424;
	shl.b32 	%r315, %r41, 1;
	cvt.u64.u32 	%rd425, %r315;
	add.s64 	%rd426, %rd12, %rd425;
	shl.b64 	%rd427, %rd426, 10;
	add.s64 	%rd428, %rd71, %rd427;
	add.s64 	%rd952, %rd428, %rd424;
	mov.u32 	%r698, 2;

$L__BB2_15:
	setp.ge.u32 	%p30, %r698, %r42;
	@%p30 bra 	$L__BB2_26;

	ld.global.u64 	%rd957, [%rd951+768];
	ld.global.u64 	%rd956, [%rd951+512];
	ld.global.u64 	%rd955, [%rd951+256];
	ld.global.u64 	%rd954, [%rd951];
	setp.eq.s32 	%p31, %r3, 6;
	selp.u32 	%r10, 1, 0, %p31;
	and.b32  	%r11, %r1, 12;
	shr.u32 	%r317, %r11, 2;
	add.s32 	%r12, %r41, -1;
	mov.u32 	%r318, -1;
	and.b32  	%r13, %r12, %r41;
	setp.ne.s32 	%p32, %r43, 0;
	setp.eq.s32 	%p1, %r58, 0;
	selp.b32 	%r319, 3, %r44, %p32;
	mul.lo.s32 	%r14, %r319, %r42;
	add.s32 	%r15, %r14, -1;
	setp.ne.s32 	%p33, %r44, 3;
	and.pred  	%p2, %p32, %p33;
	add.s32 	%r320, %r1, -1;
	and.b32  	%r321, %r320, 3;
	and.b32  	%r322, %r1, 28;
	or.b32  	%r16, %r321, %r322;
	add.s32 	%r323, %r1, 2;
	and.b32  	%r324, %r323, 3;
	or.b32  	%r17, %r324, %r322;
	add.s32 	%r325, %r1, 1;
	and.b32  	%r326, %r325, 3;
	or.b32  	%r18, %r326, %r322;
	add.s32 	%r327, %r44, 1;
	mul.lo.s32 	%r19, %r327, %r42;
	cvt.u64.u32 	%rd25, %r3;
	and.b32  	%r328, %r1, 16;
	shr.u32 	%r329, %r328, 3;
	and.b32  	%r330, %r1, 1;
	or.b32  	%r331, %r329, %r330;
	add.s32 	%r332, %r331, 1;
	shl.b32 	%r333, %r332, 3;
	and.b32  	%r334, %r333, 16;
	and.b32  	%r335, %r332, 1;
	and.b32  	%r336, %r1, 14;
	or.b32  	%r337, %r335, %r336;
	or.b32  	%r20, %r337, %r334;
	add.s32 	%r338, %r331, 2;
	shl.b32 	%r339, %r338, 3;
	and.b32  	%r340, %r339, 16;
	and.b32  	%r341, %r338, 1;
	or.b32  	%r342, %r341, %r336;
	or.b32  	%r21, %r342, %r340;
	add.s32 	%r343, %r331, 3;
	shl.b32 	%r344, %r343, 3;
	and.b32  	%r345, %r344, 16;
	and.b32  	%r346, %r343, 1;
	or.b32  	%r347, %r346, %r336;
	or.b32  	%r22, %r347, %r345;
	shl.b32 	%r348, %r41, 7;
	cvt.u64.u32 	%rd26, %r348;
	xor.b32  	%r23, %r317, 1;
	setp.eq.s32 	%p34, %r11, 0;
	or.b32  	%r349, %r317, 1;
	setp.eq.s32 	%p35, %r349, 3;
	or.pred  	%p3, %p35, %p34;
	xor.b32  	%r24, %r3, 4;
	xor.b32  	%r25, %r317, 2;
	xor.b32  	%r26, %r3, 8;
	xor.b32  	%r27, %r317, 3;
	xor.b32  	%r28, %r3, 12;
	div.u32 	%r29, %r318, %r41;
	shl.b64 	%rd768, %rd25, 3;
	shl.b64 	%rd945, %rd26, 3;

$L__BB2_17:
	and.b32  	%r32, %r698, 127;
	setp.ne.s32 	%p36, %r32, 0;
	@%p36 bra 	$L__BB2_19;

	setp.eq.s32 	%p37, %r27, 0;
	add.s32 	%r696, %r696, %r10;
	cvt.u64.u32 	%rd605, %r696;
	selp.b64 	%rd430, 0, %rd605, %p3;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd430;
	shfl.sync.idx.b32  r_lo, v_lo, %r24, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r24, 0x1f, 0xffffffff;
	mov.b64  %rd429, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p38, %r23, 0;
	selp.b64 	%rd606, %rd429, %rd605, %p38;
	selp.b64 	%rd607, %rd429, 0, %p34;
	setp.eq.s32 	%p40, %r23, 2;
	selp.b64 	%rd608, %rd429, 0, %p40;
	setp.eq.s32 	%p41, %r23, 3;
	selp.b64 	%rd609, %rd429, 0, %p41;
	setp.eq.s32 	%p42, %r25, 1;
	selp.b64 	%rd610, %rd607, %rd606, %p42;
	selp.b64 	%rd611, %rd608, %rd610, %p34;
	setp.eq.s32 	%p43, %r25, 3;
	selp.b64 	%rd432, %rd609, %rd611, %p43;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd432;
	shfl.sync.idx.b32  r_lo, v_lo, %r26, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r26, 0x1f, 0xffffffff;
	mov.b64  %rd431, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p44, %r25, 0;
	selp.b64 	%rd612, %rd431, %rd606, %p44;
	selp.b64 	%rd613, %rd431, %rd607, %p42;
	selp.b64 	%rd614, %rd431, %rd608, %p34;
	selp.b64 	%rd615, %rd431, %rd609, %p43;
	setp.eq.s32 	%p45, %r27, 1;
	selp.b64 	%rd616, %rd613, %rd612, %p45;
	setp.eq.s32 	%p46, %r27, 2;
	selp.b64 	%rd617, %rd614, %rd616, %p46;
	selp.b64 	%rd434, %rd615, %rd617, %p34;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd434;
	shfl.sync.idx.b32  r_lo, v_lo, %r28, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r28, 0x1f, 0xffffffff;
	mov.b64  %rd433, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd618, %rd433, %rd612, %p37;
	selp.b64 	%rd436, %rd433, %rd613, %p45;
	selp.b64 	%rd619, %rd433, %rd614, %p46;
	selp.b64 	%rd620, %rd433, %rd615, %p34;
	cvt.u32.u64 	%r353, %rd618;
	cvt.u32.u64 	%r354, %rd436;
	// begin inline asm
	mul.wide.u32 %rd435, %r353, %r354;
	mad.lo.u64   %rd435, %rd435, 2, %rd436;
	// end inline asm
	add.s64 	%rd622, %rd435, %rd618;
	xor.b64  	%rd623, %rd622, %rd620;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r454,%dummy}, %rd623;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r455}, %rd623;
	}
	mov.b64 	%rd439, {%r455, %r454};
	cvt.u32.u64 	%r355, %rd619;
	cvt.u32.u64 	%r356, %rd439;
	// begin inline asm
	mul.wide.u32 %rd438, %r355, %r356;
	mad.lo.u64   %rd438, %rd438, 2, %rd439;
	// end inline asm
	add.s64 	%rd625, %rd438, %rd619;
	xor.b64  	%rd626, %rd625, %rd436;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r456,%dummy}, %rd626;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r457}, %rd626;
	}
	shf.r.wrap.b32 	%r458, %r457, %r456, 24;
	shf.r.wrap.b32 	%r459, %r456, %r457, 24;
	mov.b64 	%rd442, {%r459, %r458};
	cvt.u32.u64 	%r357, %rd622;
	cvt.u32.u64 	%r358, %rd442;
	// begin inline asm
	mul.wide.u32 %rd441, %r357, %r358;
	mad.lo.u64   %rd441, %rd441, 2, %rd442;
	// end inline asm
	add.s64 	%rd628, %rd441, %rd622;
	xor.b64  	%rd629, %rd439, %rd628;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r460,%dummy}, %rd629;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r461}, %rd629;
	}
	shf.r.wrap.b32 	%r462, %r461, %r460, 16;
	shf.r.wrap.b32 	%r463, %r460, %r461, 16;
	mov.b64 	%rd452, {%r463, %r462};
	cvt.u32.u64 	%r359, %rd625;
	cvt.u32.u64 	%r360, %rd452;
	// begin inline asm
	mul.wide.u32 %rd444, %r359, %r360;
	mad.lo.u64   %rd444, %rd444, 2, %rd452;
	// end inline asm
	add.s64 	%rd450, %rd444, %rd625;
	xor.b64  	%rd631, %rd442, %rd450;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r464}, %rd631;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r465,%dummy}, %rd631;
	}
	shf.l.wrap.b32 	%r466, %r465, %r464, 1;
	shf.l.wrap.b32 	%r467, %r464, %r465, 1;
	mov.b64 	%rd448, {%r467, %r466};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd448;
	shfl.sync.idx.b32  r_lo, v_lo, %r18, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r18, 0x1f, 0xffffffff;
	mov.b64  %rd447, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd450;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd449, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd452;
	shfl.sync.idx.b32  r_lo, v_lo, %r16, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r16, 0x1f, 0xffffffff;
	mov.b64  %rd451, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r364, %rd628;
	cvt.u32.u64 	%r365, %rd447;
	// begin inline asm
	mul.wide.u32 %rd453, %r364, %r365;
	mad.lo.u64   %rd453, %rd453, 2, %rd447;
	// end inline asm
	add.s64 	%rd633, %rd453, %rd628;
	xor.b64  	%rd634, %rd633, %rd451;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r468,%dummy}, %rd634;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r469}, %rd634;
	}
	mov.b64 	%rd457, {%r469, %r468};
	cvt.u32.u64 	%r366, %rd449;
	cvt.u32.u64 	%r367, %rd457;
	// begin inline asm
	mul.wide.u32 %rd456, %r366, %r367;
	mad.lo.u64   %rd456, %rd456, 2, %rd457;
	// end inline asm
	add.s64 	%rd636, %rd456, %rd449;
	xor.b64  	%rd637, %rd636, %rd447;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r470,%dummy}, %rd637;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r471}, %rd637;
	}
	shf.r.wrap.b32 	%r472, %r471, %r470, 24;
	shf.r.wrap.b32 	%r473, %r470, %r471, 24;
	mov.b64 	%rd460, {%r473, %r472};
	cvt.u32.u64 	%r368, %rd633;
	cvt.u32.u64 	%r369, %rd460;
	// begin inline asm
	mul.wide.u32 %rd459, %r368, %r369;
	mad.lo.u64   %rd459, %rd459, 2, %rd460;
	// end inline asm
	add.s64 	%rd639, %rd459, %rd633;
	xor.b64  	%rd640, %rd457, %rd639;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r474,%dummy}, %rd640;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r475}, %rd640;
	}
	shf.r.wrap.b32 	%r476, %r475, %r474, 16;
	shf.r.wrap.b32 	%r477, %r474, %r475, 16;
	mov.b64 	%rd470, {%r477, %r476};
	cvt.u32.u64 	%r370, %rd636;
	cvt.u32.u64 	%r371, %rd470;
	// begin inline asm
	mul.wide.u32 %rd462, %r370, %r371;
	mad.lo.u64   %rd462, %rd462, 2, %rd470;
	// end inline asm
	add.s64 	%rd468, %rd462, %rd636;
	xor.b64  	%rd642, %rd460, %rd468;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r478}, %rd642;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r479,%dummy}, %rd642;
	}
	shf.l.wrap.b32 	%r480, %r479, %r478, 1;
	shf.l.wrap.b32 	%r481, %r478, %r479, 1;
	mov.b64 	%rd466, {%r481, %r480};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd466;
	shfl.sync.idx.b32  r_lo, v_lo, %r16, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r16, 0x1f, 0xffffffff;
	mov.b64  %rd465, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd468;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd467, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd470;
	shfl.sync.idx.b32  r_lo, v_lo, %r18, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r18, 0x1f, 0xffffffff;
	mov.b64  %rd469, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd643, %rd465, %rd639, %p34;
	selp.b64 	%rd644, %rd467, %rd643, %p40;
	selp.b64 	%rd472, %rd469, %rd644, %p41;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd472;
	shfl.sync.idx.b32  r_lo, v_lo, %r24, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r24, 0x1f, 0xffffffff;
	mov.b64  %rd471, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd645, %rd471, %rd639, %p38;
	selp.b64 	%rd646, %rd471, %rd465, %p34;
	selp.b64 	%rd647, %rd471, %rd467, %p40;
	selp.b64 	%rd648, %rd471, %rd469, %p41;
	selp.b64 	%rd649, %rd646, %rd645, %p42;
	selp.b64 	%rd650, %rd647, %rd649, %p34;
	selp.b64 	%rd474, %rd648, %rd650, %p43;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd474;
	shfl.sync.idx.b32  r_lo, v_lo, %r26, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r26, 0x1f, 0xffffffff;
	mov.b64  %rd473, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd651, %rd473, %rd645, %p44;
	selp.b64 	%rd652, %rd473, %rd646, %p42;
	selp.b64 	%rd653, %rd473, %rd647, %p34;
	selp.b64 	%rd654, %rd473, %rd648, %p43;
	selp.b64 	%rd655, %rd652, %rd651, %p45;
	selp.b64 	%rd656, %rd653, %rd655, %p46;
	selp.b64 	%rd476, %rd654, %rd656, %p34;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd476;
	shfl.sync.idx.b32  r_lo, v_lo, %r28, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r28, 0x1f, 0xffffffff;
	mov.b64  %rd475, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd657, %rd475, %rd651, %p37;
	selp.b64 	%rd478, %rd475, %rd652, %p45;
	selp.b64 	%rd658, %rd475, %rd653, %p46;
	selp.b64 	%rd659, %rd475, %rd654, %p34;
	cvt.u32.u64 	%r378, %rd657;
	cvt.u32.u64 	%r379, %rd478;
	// begin inline asm
	mul.wide.u32 %rd477, %r378, %r379;
	mad.lo.u64   %rd477, %rd477, 2, %rd478;
	// end inline asm
	add.s64 	%rd661, %rd477, %rd657;
	xor.b64  	%rd662, %rd661, %rd659;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r482,%dummy}, %rd662;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r483}, %rd662;
	}
	mov.b64 	%rd481, {%r483, %r482};
	cvt.u32.u64 	%r380, %rd658;
	cvt.u32.u64 	%r381, %rd481;
	// begin inline asm
	mul.wide.u32 %rd480, %r380, %r381;
	mad.lo.u64   %rd480, %rd480, 2, %rd481;
	// end inline asm
	add.s64 	%rd664, %rd480, %rd658;
	xor.b64  	%rd665, %rd664, %rd478;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r484,%dummy}, %rd665;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r485}, %rd665;
	}
	shf.r.wrap.b32 	%r486, %r485, %r484, 24;
	shf.r.wrap.b32 	%r487, %r484, %r485, 24;
	mov.b64 	%rd484, {%r487, %r486};
	cvt.u32.u64 	%r382, %rd661;
	cvt.u32.u64 	%r383, %rd484;
	// begin inline asm
	mul.wide.u32 %rd483, %r382, %r383;
	mad.lo.u64   %rd483, %rd483, 2, %rd484;
	// end inline asm
	add.s64 	%rd490, %rd483, %rd661;
	xor.b64  	%rd667, %rd481, %rd490;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r488,%dummy}, %rd667;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r489}, %rd667;
	}
	shf.r.wrap.b32 	%r490, %r489, %r488, 16;
	shf.r.wrap.b32 	%r491, %r488, %r489, 16;
	mov.b64 	%rd496, {%r491, %r490};
	cvt.u32.u64 	%r384, %rd664;
	cvt.u32.u64 	%r385, %rd496;
	// begin inline asm
	mul.wide.u32 %rd486, %r384, %r385;
	mad.lo.u64   %rd486, %rd486, 2, %rd496;
	// end inline asm
	add.s64 	%rd494, %rd486, %rd664;
	xor.b64  	%rd669, %rd484, %rd494;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r492}, %rd669;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r493,%dummy}, %rd669;
	}
	shf.l.wrap.b32 	%r494, %r493, %r492, 1;
	shf.l.wrap.b32 	%r495, %r492, %r493, 1;
	mov.b64 	%rd492, {%r495, %r494};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd490;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd489, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd492;
	shfl.sync.idx.b32  r_lo, v_lo, %r20, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r20, 0x1f, 0xffffffff;
	mov.b64  %rd491, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd494;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd493, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd496;
	shfl.sync.idx.b32  r_lo, v_lo, %r22, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r22, 0x1f, 0xffffffff;
	mov.b64  %rd495, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r390, %rd489;
	cvt.u32.u64 	%r391, %rd491;
	// begin inline asm
	mul.wide.u32 %rd497, %r390, %r391;
	mad.lo.u64   %rd497, %rd497, 2, %rd491;
	// end inline asm
	add.s64 	%rd671, %rd497, %rd489;
	xor.b64  	%rd672, %rd671, %rd495;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r496,%dummy}, %rd672;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r497}, %rd672;
	}
	mov.b64 	%rd501, {%r497, %r496};
	cvt.u32.u64 	%r392, %rd493;
	cvt.u32.u64 	%r393, %rd501;
	// begin inline asm
	mul.wide.u32 %rd500, %r392, %r393;
	mad.lo.u64   %rd500, %rd500, 2, %rd501;
	// end inline asm
	add.s64 	%rd674, %rd500, %rd493;
	xor.b64  	%rd675, %rd674, %rd491;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r498,%dummy}, %rd675;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r499}, %rd675;
	}
	shf.r.wrap.b32 	%r500, %r499, %r498, 24;
	shf.r.wrap.b32 	%r501, %r498, %r499, 24;
	mov.b64 	%rd504, {%r501, %r500};
	cvt.u32.u64 	%r394, %rd671;
	cvt.u32.u64 	%r395, %rd504;
	// begin inline asm
	mul.wide.u32 %rd503, %r394, %r395;
	mad.lo.u64   %rd503, %rd503, 2, %rd504;
	// end inline asm
	add.s64 	%rd510, %rd503, %rd671;
	xor.b64  	%rd677, %rd501, %rd510;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r502,%dummy}, %rd677;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r503}, %rd677;
	}
	shf.r.wrap.b32 	%r504, %r503, %r502, 16;
	shf.r.wrap.b32 	%r505, %r502, %r503, 16;
	mov.b64 	%rd516, {%r505, %r504};
	cvt.u32.u64 	%r396, %rd674;
	cvt.u32.u64 	%r397, %rd516;
	// begin inline asm
	mul.wide.u32 %rd506, %r396, %r397;
	mad.lo.u64   %rd506, %rd506, 2, %rd516;
	// end inline asm
	add.s64 	%rd514, %rd506, %rd674;
	xor.b64  	%rd679, %rd504, %rd514;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r506}, %rd679;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r507,%dummy}, %rd679;
	}
	shf.l.wrap.b32 	%r508, %r507, %r506, 1;
	shf.l.wrap.b32 	%r509, %r506, %r507, 1;
	mov.b64 	%rd512, {%r509, %r508};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd510;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd509, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd512;
	shfl.sync.idx.b32  r_lo, v_lo, %r22, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r22, 0x1f, 0xffffffff;
	mov.b64  %rd511, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd514;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd513, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd516;
	shfl.sync.idx.b32  r_lo, v_lo, %r20, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r20, 0x1f, 0xffffffff;
	mov.b64  %rd515, {r_hi,r_lo};
	}
	// end inline asm
	xor.b64  	%rd680, %rd509, %rd605;
	selp.b64 	%rd681, %rd511, %rd680, %p34;
	selp.b64 	%rd682, %rd513, %rd681, %p40;
	selp.b64 	%rd518, %rd515, %rd682, %p41;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd518;
	shfl.sync.idx.b32  r_lo, v_lo, %r24, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r24, 0x1f, 0xffffffff;
	mov.b64  %rd517, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd683, %rd517, %rd680, %p38;
	selp.b64 	%rd684, %rd517, %rd511, %p34;
	selp.b64 	%rd685, %rd517, %rd513, %p40;
	selp.b64 	%rd686, %rd517, %rd515, %p41;
	selp.b64 	%rd687, %rd684, %rd683, %p42;
	selp.b64 	%rd688, %rd685, %rd687, %p34;
	selp.b64 	%rd520, %rd686, %rd688, %p43;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd520;
	shfl.sync.idx.b32  r_lo, v_lo, %r26, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r26, 0x1f, 0xffffffff;
	mov.b64  %rd519, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd689, %rd519, %rd683, %p44;
	selp.b64 	%rd690, %rd519, %rd684, %p42;
	selp.b64 	%rd691, %rd519, %rd685, %p34;
	selp.b64 	%rd692, %rd519, %rd686, %p43;
	selp.b64 	%rd693, %rd690, %rd689, %p45;
	selp.b64 	%rd694, %rd691, %rd693, %p46;
	selp.b64 	%rd522, %rd692, %rd694, %p34;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd522;
	shfl.sync.idx.b32  r_lo, v_lo, %r28, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r28, 0x1f, 0xffffffff;
	mov.b64  %rd521, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd695, %rd521, %rd689, %p37;
	selp.b64 	%rd524, %rd521, %rd690, %p45;
	selp.b64 	%rd696, %rd521, %rd691, %p46;
	selp.b64 	%rd697, %rd521, %rd692, %p34;
	cvt.u32.u64 	%r405, %rd695;
	cvt.u32.u64 	%r406, %rd524;
	// begin inline asm
	mul.wide.u32 %rd523, %r405, %r406;
	mad.lo.u64   %rd523, %rd523, 2, %rd524;
	// end inline asm
	add.s64 	%rd699, %rd523, %rd695;
	xor.b64  	%rd700, %rd699, %rd697;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r510,%dummy}, %rd700;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r511}, %rd700;
	}
	mov.b64 	%rd527, {%r511, %r510};
	cvt.u32.u64 	%r407, %rd696;
	cvt.u32.u64 	%r408, %rd527;
	// begin inline asm
	mul.wide.u32 %rd526, %r407, %r408;
	mad.lo.u64   %rd526, %rd526, 2, %rd527;
	// end inline asm
	add.s64 	%rd702, %rd526, %rd696;
	xor.b64  	%rd703, %rd702, %rd524;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r512,%dummy}, %rd703;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r513}, %rd703;
	}
	shf.r.wrap.b32 	%r514, %r513, %r512, 24;
	shf.r.wrap.b32 	%r515, %r512, %r513, 24;
	mov.b64 	%rd530, {%r515, %r514};
	cvt.u32.u64 	%r409, %rd699;
	cvt.u32.u64 	%r410, %rd530;
	// begin inline asm
	mul.wide.u32 %rd529, %r409, %r410;
	mad.lo.u64   %rd529, %rd529, 2, %rd530;
	// end inline asm
	add.s64 	%rd705, %rd529, %rd699;
	xor.b64  	%rd706, %rd527, %rd705;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r516,%dummy}, %rd706;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r517}, %rd706;
	}
	shf.r.wrap.b32 	%r518, %r517, %r516, 16;
	shf.r.wrap.b32 	%r519, %r516, %r517, 16;
	mov.b64 	%rd540, {%r519, %r518};
	cvt.u32.u64 	%r411, %rd702;
	cvt.u32.u64 	%r412, %rd540;
	// begin inline asm
	mul.wide.u32 %rd532, %r411, %r412;
	mad.lo.u64   %rd532, %rd532, 2, %rd540;
	// end inline asm
	add.s64 	%rd538, %rd532, %rd702;
	xor.b64  	%rd708, %rd530, %rd538;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r520}, %rd708;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r521,%dummy}, %rd708;
	}
	shf.l.wrap.b32 	%r522, %r521, %r520, 1;
	shf.l.wrap.b32 	%r523, %r520, %r521, 1;
	mov.b64 	%rd536, {%r523, %r522};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd536;
	shfl.sync.idx.b32  r_lo, v_lo, %r18, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r18, 0x1f, 0xffffffff;
	mov.b64  %rd535, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd538;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd537, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd540;
	shfl.sync.idx.b32  r_lo, v_lo, %r16, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r16, 0x1f, 0xffffffff;
	mov.b64  %rd539, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r416, %rd705;
	cvt.u32.u64 	%r417, %rd535;
	// begin inline asm
	mul.wide.u32 %rd541, %r416, %r417;
	mad.lo.u64   %rd541, %rd541, 2, %rd535;
	// end inline asm
	add.s64 	%rd710, %rd541, %rd705;
	xor.b64  	%rd711, %rd710, %rd539;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r524,%dummy}, %rd711;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r525}, %rd711;
	}
	mov.b64 	%rd545, {%r525, %r524};
	cvt.u32.u64 	%r418, %rd537;
	cvt.u32.u64 	%r419, %rd545;
	// begin inline asm
	mul.wide.u32 %rd544, %r418, %r419;
	mad.lo.u64   %rd544, %rd544, 2, %rd545;
	// end inline asm
	add.s64 	%rd713, %rd544, %rd537;
	xor.b64  	%rd714, %rd713, %rd535;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r526,%dummy}, %rd714;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r527}, %rd714;
	}
	shf.r.wrap.b32 	%r528, %r527, %r526, 24;
	shf.r.wrap.b32 	%r529, %r526, %r527, 24;
	mov.b64 	%rd548, {%r529, %r528};
	cvt.u32.u64 	%r420, %rd710;
	cvt.u32.u64 	%r421, %rd548;
	// begin inline asm
	mul.wide.u32 %rd547, %r420, %r421;
	mad.lo.u64   %rd547, %rd547, 2, %rd548;
	// end inline asm
	add.s64 	%rd716, %rd547, %rd710;
	xor.b64  	%rd717, %rd545, %rd716;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r530,%dummy}, %rd717;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r531}, %rd717;
	}
	shf.r.wrap.b32 	%r532, %r531, %r530, 16;
	shf.r.wrap.b32 	%r533, %r530, %r531, 16;
	mov.b64 	%rd558, {%r533, %r532};
	cvt.u32.u64 	%r422, %rd713;
	cvt.u32.u64 	%r423, %rd558;
	// begin inline asm
	mul.wide.u32 %rd550, %r422, %r423;
	mad.lo.u64   %rd550, %rd550, 2, %rd558;
	// end inline asm
	add.s64 	%rd556, %rd550, %rd713;
	xor.b64  	%rd719, %rd548, %rd556;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r534}, %rd719;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r535,%dummy}, %rd719;
	}
	shf.l.wrap.b32 	%r536, %r535, %r534, 1;
	shf.l.wrap.b32 	%r537, %r534, %r535, 1;
	mov.b64 	%rd554, {%r537, %r536};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd554;
	shfl.sync.idx.b32  r_lo, v_lo, %r16, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r16, 0x1f, 0xffffffff;
	mov.b64  %rd553, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd556;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd555, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd558;
	shfl.sync.idx.b32  r_lo, v_lo, %r18, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r18, 0x1f, 0xffffffff;
	mov.b64  %rd557, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd720, %rd553, %rd716, %p34;
	selp.b64 	%rd721, %rd555, %rd720, %p40;
	selp.b64 	%rd560, %rd557, %rd721, %p41;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd560;
	shfl.sync.idx.b32  r_lo, v_lo, %r24, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r24, 0x1f, 0xffffffff;
	mov.b64  %rd559, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd722, %rd559, %rd716, %p38;
	selp.b64 	%rd723, %rd559, %rd553, %p34;
	selp.b64 	%rd724, %rd559, %rd555, %p40;
	selp.b64 	%rd725, %rd559, %rd557, %p41;
	selp.b64 	%rd726, %rd723, %rd722, %p42;
	selp.b64 	%rd727, %rd724, %rd726, %p34;
	selp.b64 	%rd562, %rd725, %rd727, %p43;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd562;
	shfl.sync.idx.b32  r_lo, v_lo, %r26, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r26, 0x1f, 0xffffffff;
	mov.b64  %rd561, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd728, %rd561, %rd722, %p44;
	selp.b64 	%rd729, %rd561, %rd723, %p42;
	selp.b64 	%rd730, %rd561, %rd724, %p34;
	selp.b64 	%rd731, %rd561, %rd725, %p43;
	selp.b64 	%rd732, %rd729, %rd728, %p45;
	selp.b64 	%rd733, %rd730, %rd732, %p46;
	selp.b64 	%rd564, %rd731, %rd733, %p34;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd564;
	shfl.sync.idx.b32  r_lo, v_lo, %r28, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r28, 0x1f, 0xffffffff;
	mov.b64  %rd563, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd734, %rd563, %rd728, %p37;
	selp.b64 	%rd566, %rd563, %rd729, %p45;
	selp.b64 	%rd735, %rd563, %rd730, %p46;
	selp.b64 	%rd736, %rd563, %rd731, %p34;
	cvt.u32.u64 	%r430, %rd734;
	cvt.u32.u64 	%r431, %rd566;
	// begin inline asm
	mul.wide.u32 %rd565, %r430, %r431;
	mad.lo.u64   %rd565, %rd565, 2, %rd566;
	// end inline asm
	add.s64 	%rd738, %rd565, %rd734;
	xor.b64  	%rd739, %rd738, %rd736;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r538,%dummy}, %rd739;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r539}, %rd739;
	}
	mov.b64 	%rd569, {%r539, %r538};
	cvt.u32.u64 	%r432, %rd735;
	cvt.u32.u64 	%r433, %rd569;
	// begin inline asm
	mul.wide.u32 %rd568, %r432, %r433;
	mad.lo.u64   %rd568, %rd568, 2, %rd569;
	// end inline asm
	add.s64 	%rd741, %rd568, %rd735;
	xor.b64  	%rd742, %rd741, %rd566;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r540,%dummy}, %rd742;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r541}, %rd742;
	}
	shf.r.wrap.b32 	%r542, %r541, %r540, 24;
	shf.r.wrap.b32 	%r543, %r540, %r541, 24;
	mov.b64 	%rd572, {%r543, %r542};
	cvt.u32.u64 	%r434, %rd738;
	cvt.u32.u64 	%r435, %rd572;
	// begin inline asm
	mul.wide.u32 %rd571, %r434, %r435;
	mad.lo.u64   %rd571, %rd571, 2, %rd572;
	// end inline asm
	add.s64 	%rd578, %rd571, %rd738;
	xor.b64  	%rd744, %rd569, %rd578;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r544,%dummy}, %rd744;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r545}, %rd744;
	}
	shf.r.wrap.b32 	%r546, %r545, %r544, 16;
	shf.r.wrap.b32 	%r547, %r544, %r545, 16;
	mov.b64 	%rd584, {%r547, %r546};
	cvt.u32.u64 	%r436, %rd741;
	cvt.u32.u64 	%r437, %rd584;
	// begin inline asm
	mul.wide.u32 %rd574, %r436, %r437;
	mad.lo.u64   %rd574, %rd574, 2, %rd584;
	// end inline asm
	add.s64 	%rd582, %rd574, %rd741;
	xor.b64  	%rd746, %rd572, %rd582;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r548}, %rd746;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r549,%dummy}, %rd746;
	}
	shf.l.wrap.b32 	%r550, %r549, %r548, 1;
	shf.l.wrap.b32 	%r551, %r548, %r549, 1;
	mov.b64 	%rd580, {%r551, %r550};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd578;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd577, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd580;
	shfl.sync.idx.b32  r_lo, v_lo, %r20, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r20, 0x1f, 0xffffffff;
	mov.b64  %rd579, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd582;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd581, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd584;
	shfl.sync.idx.b32  r_lo, v_lo, %r22, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r22, 0x1f, 0xffffffff;
	mov.b64  %rd583, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r442, %rd577;
	cvt.u32.u64 	%r443, %rd579;
	// begin inline asm
	mul.wide.u32 %rd585, %r442, %r443;
	mad.lo.u64   %rd585, %rd585, 2, %rd579;
	// end inline asm
	add.s64 	%rd748, %rd585, %rd577;
	xor.b64  	%rd749, %rd748, %rd583;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r552,%dummy}, %rd749;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r553}, %rd749;
	}
	mov.b64 	%rd589, {%r553, %r552};
	cvt.u32.u64 	%r444, %rd581;
	cvt.u32.u64 	%r445, %rd589;
	// begin inline asm
	mul.wide.u32 %rd588, %r444, %r445;
	mad.lo.u64   %rd588, %rd588, 2, %rd589;
	// end inline asm
	add.s64 	%rd751, %rd588, %rd581;
	xor.b64  	%rd752, %rd751, %rd579;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r554,%dummy}, %rd752;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r555}, %rd752;
	}
	shf.r.wrap.b32 	%r556, %r555, %r554, 24;
	shf.r.wrap.b32 	%r557, %r554, %r555, 24;
	mov.b64 	%rd592, {%r557, %r556};
	cvt.u32.u64 	%r446, %rd748;
	cvt.u32.u64 	%r447, %rd592;
	// begin inline asm
	mul.wide.u32 %rd591, %r446, %r447;
	mad.lo.u64   %rd591, %rd591, 2, %rd592;
	// end inline asm
	add.s64 	%rd598, %rd591, %rd748;
	xor.b64  	%rd754, %rd589, %rd598;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r558,%dummy}, %rd754;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r559}, %rd754;
	}
	shf.r.wrap.b32 	%r560, %r559, %r558, 16;
	shf.r.wrap.b32 	%r561, %r558, %r559, 16;
	mov.b64 	%rd604, {%r561, %r560};
	cvt.u32.u64 	%r448, %rd751;
	cvt.u32.u64 	%r449, %rd604;
	// begin inline asm
	mul.wide.u32 %rd594, %r448, %r449;
	mad.lo.u64   %rd594, %rd594, 2, %rd604;
	// end inline asm
	add.s64 	%rd602, %rd594, %rd751;
	xor.b64  	%rd756, %rd592, %rd602;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r562}, %rd756;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r563,%dummy}, %rd756;
	}
	shf.l.wrap.b32 	%r564, %r563, %r562, 1;
	shf.l.wrap.b32 	%r565, %r562, %r563, 1;
	mov.b64 	%rd600, {%r565, %r564};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd598;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd597, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd600;
	shfl.sync.idx.b32  r_lo, v_lo, %r22, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r22, 0x1f, 0xffffffff;
	mov.b64  %rd599, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd602;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd601, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd604;
	shfl.sync.idx.b32  r_lo, v_lo, %r20, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r20, 0x1f, 0xffffffff;
	mov.b64  %rd603, {r_hi,r_lo};
	}
	// end inline asm
	xor.b64  	%rd965, %rd597, %rd680;
	xor.b64  	%rd964, %rd599, %rd511;
	xor.b64  	%rd963, %rd601, %rd513;
	xor.b64  	%rd962, %rd603, %rd515;

$L__BB2_19:
	shr.u32 	%r567, %r32, 5;
	setp.eq.s32 	%p47, %r567, 1;
	selp.b64 	%rd759, %rd964, %rd965, %p47;
	setp.eq.s32 	%p48, %r567, 2;
	selp.b64 	%rd760, %rd963, %rd759, %p48;
	setp.eq.s32 	%p49, %r567, 3;
	selp.b64 	%rd758, %rd962, %rd760, %p49;
	and.b32  	%r566, %r698, 31;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd758;
	shfl.sync.idx.b32  r_lo, v_lo, %r566, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r566, 0x1f, 0xffffffff;
	mov.b64  %rd757, {r_hi,r_lo};
	}
	// end inline asm
	shr.u64 	%rd761, %rd757, 32;
	cvt.u32.u64 	%r35, %rd761;
	setp.eq.s32 	%p50, %r13, 0;
	@%p50 bra 	$L__BB2_21;

	mul.hi.u32 	%r568, %r35, %r29;
	mul.lo.s32 	%r569, %r568, %r41;
	sub.s32 	%r570, %r35, %r569;
	setp.lt.u32 	%p51, %r570, %r41;
	selp.b32 	%r571, 0, %r41, %p51;
	sub.s32 	%r702, %r570, %r571;
	bra.uni 	$L__BB2_22;

$L__BB2_21:
	and.b32  	%r702, %r12, %r35;

$L__BB2_22:
	ld.param.u64 	%rd946, [argon2_kernel_segment_1_param_0];
	ld.param.u32 	%r694, [argon2_kernel_segment_1_param_3];
	shl.b32 	%r693, %r694, 2;
	cvt.u32.u64 	%r572, %rd757;
	selp.b32 	%r573, %r2, %r702, %p1;
	setp.ne.s32 	%p52, %r573, %r2;
	add.s32 	%r574, %r15, %r698;
	setp.gt.u32 	%p53, %r574, %r14;
	and.pred  	%p54, %p53, %p52;
	selp.b32 	%r575, %r14, %r574, %p54;
	mul.hi.u32 	%r576, %r572, %r572;
	mul.hi.u32 	%r577, %r575, %r576;
	not.b32 	%r578, %r577;
	add.s32 	%r579, %r575, %r578;
	add.s32 	%r580, %r579, %r19;
	setp.lt.u32 	%p55, %r580, %r693;
	selp.b32 	%r581, 0, %r693, %p55;
	sub.s32 	%r582, %r580, %r581;
	selp.b32 	%r583, %r582, %r579, %p2;
	mul.lo.s32 	%r584, %r583, %r41;
	cvt.u64.u32 	%rd762, %r584;
	cvt.u64.u32 	%rd763, %r573;
	add.s64 	%rd764, %rd3, %rd763;
	add.s64 	%rd765, %rd764, %rd762;
	shl.b64 	%rd766, %rd765, 10;
	add.s64 	%rd767, %rd946, %rd766;
	add.s64 	%rd45, %rd767, %rd768;
	@%p27 bra 	$L__BB2_24;

	ld.global.u64 	%rd769, [%rd952];
	ld.global.u64 	%rd770, [%rd45];
	xor.b64  	%rd969, %rd770, %rd954;
	ld.global.u64 	%rd771, [%rd45+256];
	xor.b64  	%rd968, %rd771, %rd955;
	ld.global.u64 	%rd772, [%rd45+512];
	xor.b64  	%rd967, %rd772, %rd956;
	ld.global.u64 	%rd773, [%rd45+768];
	xor.b64  	%rd966, %rd773, %rd957;
	xor.b64  	%rd973, %rd969, %rd769;
	ld.global.u64 	%rd774, [%rd952+256];
	xor.b64  	%rd972, %rd968, %rd774;
	ld.global.u64 	%rd775, [%rd952+512];
	xor.b64  	%rd971, %rd967, %rd775;
	ld.global.u64 	%rd776, [%rd952+768];
	xor.b64  	%rd970, %rd966, %rd776;
	bra.uni 	$L__BB2_25;

$L__BB2_24:
	ld.global.u64 	%rd777, [%rd45];
	xor.b64  	%rd969, %rd777, %rd954;
	ld.global.u64 	%rd778, [%rd45+256];
	xor.b64  	%rd968, %rd778, %rd955;
	ld.global.u64 	%rd779, [%rd45+512];
	xor.b64  	%rd967, %rd779, %rd956;
	ld.global.u64 	%rd780, [%rd45+768];
	xor.b64  	%rd966, %rd780, %rd957;
	mov.u64 	%rd970, %rd966;
	mov.u64 	%rd971, %rd967;
	mov.u64 	%rd972, %rd968;
	mov.u64 	%rd973, %rd969;

$L__BB2_25:
	ld.param.u32 	%r695, [argon2_kernel_segment_1_param_3];
	selp.b64 	%rd869, %rd968, %rd969, %p34;
	setp.eq.s32 	%p58, %r23, 2;
	selp.b64 	%rd870, %rd967, %rd869, %p58;
	setp.eq.s32 	%p59, %r23, 3;
	selp.b64 	%rd782, %rd966, %rd870, %p59;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd782;
	shfl.sync.idx.b32  r_lo, v_lo, %r24, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r24, 0x1f, 0xffffffff;
	mov.b64  %rd781, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p60, %r23, 0;
	selp.b64 	%rd871, %rd781, %rd969, %p60;
	selp.b64 	%rd872, %rd781, %rd968, %p34;
	selp.b64 	%rd873, %rd781, %rd967, %p58;
	selp.b64 	%rd874, %rd781, %rd966, %p59;
	setp.eq.s32 	%p61, %r25, 1;
	selp.b64 	%rd875, %rd872, %rd871, %p61;
	selp.b64 	%rd876, %rd873, %rd875, %p34;
	setp.eq.s32 	%p62, %r25, 3;
	selp.b64 	%rd784, %rd874, %rd876, %p62;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd784;
	shfl.sync.idx.b32  r_lo, v_lo, %r26, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r26, 0x1f, 0xffffffff;
	mov.b64  %rd783, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p63, %r25, 0;
	selp.b64 	%rd877, %rd783, %rd871, %p63;
	selp.b64 	%rd878, %rd783, %rd872, %p61;
	selp.b64 	%rd879, %rd783, %rd873, %p34;
	selp.b64 	%rd880, %rd783, %rd874, %p62;
	setp.eq.s32 	%p64, %r27, 1;
	selp.b64 	%rd881, %rd878, %rd877, %p64;
	setp.eq.s32 	%p65, %r27, 2;
	selp.b64 	%rd882, %rd879, %rd881, %p65;
	selp.b64 	%rd786, %rd880, %rd882, %p34;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd786;
	shfl.sync.idx.b32  r_lo, v_lo, %r28, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r28, 0x1f, 0xffffffff;
	mov.b64  %rd785, {r_hi,r_lo};
	}
	// end inline asm
	setp.eq.s32 	%p66, %r27, 0;
	selp.b64 	%rd883, %rd785, %rd877, %p66;
	selp.b64 	%rd788, %rd785, %rd878, %p64;
	selp.b64 	%rd884, %rd785, %rd879, %p65;
	selp.b64 	%rd885, %rd785, %rd880, %p34;
	cvt.u32.u64 	%r588, %rd883;
	cvt.u32.u64 	%r589, %rd788;
	// begin inline asm
	mul.wide.u32 %rd787, %r588, %r589;
	mad.lo.u64   %rd787, %rd787, 2, %rd788;
	// end inline asm
	add.s64 	%rd887, %rd787, %rd883;
	xor.b64  	%rd888, %rd887, %rd885;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r637,%dummy}, %rd888;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r638}, %rd888;
	}
	mov.b64 	%rd791, {%r638, %r637};
	cvt.u32.u64 	%r590, %rd884;
	cvt.u32.u64 	%r591, %rd791;
	// begin inline asm
	mul.wide.u32 %rd790, %r590, %r591;
	mad.lo.u64   %rd790, %rd790, 2, %rd791;
	// end inline asm
	add.s64 	%rd890, %rd790, %rd884;
	xor.b64  	%rd891, %rd890, %rd788;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r639,%dummy}, %rd891;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r640}, %rd891;
	}
	shf.r.wrap.b32 	%r641, %r640, %r639, 24;
	shf.r.wrap.b32 	%r642, %r639, %r640, 24;
	mov.b64 	%rd794, {%r642, %r641};
	cvt.u32.u64 	%r592, %rd887;
	cvt.u32.u64 	%r593, %rd794;
	// begin inline asm
	mul.wide.u32 %rd793, %r592, %r593;
	mad.lo.u64   %rd793, %rd793, 2, %rd794;
	// end inline asm
	add.s64 	%rd893, %rd793, %rd887;
	xor.b64  	%rd894, %rd791, %rd893;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r643,%dummy}, %rd894;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r644}, %rd894;
	}
	shf.r.wrap.b32 	%r645, %r644, %r643, 16;
	shf.r.wrap.b32 	%r646, %r643, %r644, 16;
	mov.b64 	%rd804, {%r646, %r645};
	cvt.u32.u64 	%r594, %rd890;
	cvt.u32.u64 	%r595, %rd804;
	// begin inline asm
	mul.wide.u32 %rd796, %r594, %r595;
	mad.lo.u64   %rd796, %rd796, 2, %rd804;
	// end inline asm
	add.s64 	%rd802, %rd796, %rd890;
	xor.b64  	%rd896, %rd794, %rd802;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r647}, %rd896;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r648,%dummy}, %rd896;
	}
	shf.l.wrap.b32 	%r649, %r648, %r647, 1;
	shf.l.wrap.b32 	%r650, %r647, %r648, 1;
	mov.b64 	%rd800, {%r650, %r649};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd800;
	shfl.sync.idx.b32  r_lo, v_lo, %r18, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r18, 0x1f, 0xffffffff;
	mov.b64  %rd799, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd802;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd801, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd804;
	shfl.sync.idx.b32  r_lo, v_lo, %r16, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r16, 0x1f, 0xffffffff;
	mov.b64  %rd803, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r599, %rd893;
	cvt.u32.u64 	%r600, %rd799;
	// begin inline asm
	mul.wide.u32 %rd805, %r599, %r600;
	mad.lo.u64   %rd805, %rd805, 2, %rd799;
	// end inline asm
	add.s64 	%rd898, %rd805, %rd893;
	xor.b64  	%rd899, %rd898, %rd803;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r651,%dummy}, %rd899;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r652}, %rd899;
	}
	mov.b64 	%rd809, {%r652, %r651};
	cvt.u32.u64 	%r601, %rd801;
	cvt.u32.u64 	%r602, %rd809;
	// begin inline asm
	mul.wide.u32 %rd808, %r601, %r602;
	mad.lo.u64   %rd808, %rd808, 2, %rd809;
	// end inline asm
	add.s64 	%rd901, %rd808, %rd801;
	xor.b64  	%rd902, %rd901, %rd799;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r653,%dummy}, %rd902;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r654}, %rd902;
	}
	shf.r.wrap.b32 	%r655, %r654, %r653, 24;
	shf.r.wrap.b32 	%r656, %r653, %r654, 24;
	mov.b64 	%rd812, {%r656, %r655};
	cvt.u32.u64 	%r603, %rd898;
	cvt.u32.u64 	%r604, %rd812;
	// begin inline asm
	mul.wide.u32 %rd811, %r603, %r604;
	mad.lo.u64   %rd811, %rd811, 2, %rd812;
	// end inline asm
	add.s64 	%rd904, %rd811, %rd898;
	xor.b64  	%rd905, %rd809, %rd904;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r657,%dummy}, %rd905;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r658}, %rd905;
	}
	shf.r.wrap.b32 	%r659, %r658, %r657, 16;
	shf.r.wrap.b32 	%r660, %r657, %r658, 16;
	mov.b64 	%rd822, {%r660, %r659};
	cvt.u32.u64 	%r605, %rd901;
	cvt.u32.u64 	%r606, %rd822;
	// begin inline asm
	mul.wide.u32 %rd814, %r605, %r606;
	mad.lo.u64   %rd814, %rd814, 2, %rd822;
	// end inline asm
	add.s64 	%rd820, %rd814, %rd901;
	xor.b64  	%rd907, %rd812, %rd820;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r661}, %rd907;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r662,%dummy}, %rd907;
	}
	shf.l.wrap.b32 	%r663, %r662, %r661, 1;
	shf.l.wrap.b32 	%r664, %r661, %r662, 1;
	mov.b64 	%rd818, {%r664, %r663};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd818;
	shfl.sync.idx.b32  r_lo, v_lo, %r16, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r16, 0x1f, 0xffffffff;
	mov.b64  %rd817, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd820;
	shfl.sync.idx.b32  r_lo, v_lo, %r17, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r17, 0x1f, 0xffffffff;
	mov.b64  %rd819, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd822;
	shfl.sync.idx.b32  r_lo, v_lo, %r18, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r18, 0x1f, 0xffffffff;
	mov.b64  %rd821, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd908, %rd817, %rd904, %p34;
	selp.b64 	%rd909, %rd819, %rd908, %p58;
	selp.b64 	%rd824, %rd821, %rd909, %p59;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd824;
	shfl.sync.idx.b32  r_lo, v_lo, %r24, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r24, 0x1f, 0xffffffff;
	mov.b64  %rd823, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd910, %rd823, %rd904, %p60;
	selp.b64 	%rd911, %rd823, %rd817, %p34;
	selp.b64 	%rd912, %rd823, %rd819, %p58;
	selp.b64 	%rd913, %rd823, %rd821, %p59;
	selp.b64 	%rd914, %rd911, %rd910, %p61;
	selp.b64 	%rd915, %rd912, %rd914, %p34;
	selp.b64 	%rd826, %rd913, %rd915, %p62;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd826;
	shfl.sync.idx.b32  r_lo, v_lo, %r26, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r26, 0x1f, 0xffffffff;
	mov.b64  %rd825, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd916, %rd825, %rd910, %p63;
	selp.b64 	%rd917, %rd825, %rd911, %p61;
	selp.b64 	%rd918, %rd825, %rd912, %p34;
	selp.b64 	%rd919, %rd825, %rd913, %p62;
	selp.b64 	%rd920, %rd917, %rd916, %p64;
	selp.b64 	%rd921, %rd918, %rd920, %p65;
	selp.b64 	%rd828, %rd919, %rd921, %p34;
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd828;
	shfl.sync.idx.b32  r_lo, v_lo, %r28, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r28, 0x1f, 0xffffffff;
	mov.b64  %rd827, {r_hi,r_lo};
	}
	// end inline asm
	selp.b64 	%rd922, %rd827, %rd916, %p66;
	selp.b64 	%rd830, %rd827, %rd917, %p64;
	selp.b64 	%rd923, %rd827, %rd918, %p65;
	selp.b64 	%rd924, %rd827, %rd919, %p34;
	cvt.u32.u64 	%r613, %rd922;
	cvt.u32.u64 	%r614, %rd830;
	// begin inline asm
	mul.wide.u32 %rd829, %r613, %r614;
	mad.lo.u64   %rd829, %rd829, 2, %rd830;
	// end inline asm
	add.s64 	%rd926, %rd829, %rd922;
	xor.b64  	%rd927, %rd926, %rd924;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r665,%dummy}, %rd927;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r666}, %rd927;
	}
	mov.b64 	%rd833, {%r666, %r665};
	cvt.u32.u64 	%r615, %rd923;
	cvt.u32.u64 	%r616, %rd833;
	// begin inline asm
	mul.wide.u32 %rd832, %r615, %r616;
	mad.lo.u64   %rd832, %rd832, 2, %rd833;
	// end inline asm
	add.s64 	%rd929, %rd832, %rd923;
	xor.b64  	%rd930, %rd929, %rd830;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r667,%dummy}, %rd930;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r668}, %rd930;
	}
	shf.r.wrap.b32 	%r669, %r668, %r667, 24;
	shf.r.wrap.b32 	%r670, %r667, %r668, 24;
	mov.b64 	%rd836, {%r670, %r669};
	cvt.u32.u64 	%r617, %rd926;
	cvt.u32.u64 	%r618, %rd836;
	// begin inline asm
	mul.wide.u32 %rd835, %r617, %r618;
	mad.lo.u64   %rd835, %rd835, 2, %rd836;
	// end inline asm
	add.s64 	%rd842, %rd835, %rd926;
	xor.b64  	%rd932, %rd833, %rd842;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r671,%dummy}, %rd932;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r672}, %rd932;
	}
	shf.r.wrap.b32 	%r673, %r672, %r671, 16;
	shf.r.wrap.b32 	%r674, %r671, %r672, 16;
	mov.b64 	%rd848, {%r674, %r673};
	cvt.u32.u64 	%r619, %rd929;
	cvt.u32.u64 	%r620, %rd848;
	// begin inline asm
	mul.wide.u32 %rd838, %r619, %r620;
	mad.lo.u64   %rd838, %rd838, 2, %rd848;
	// end inline asm
	add.s64 	%rd846, %rd838, %rd929;
	xor.b64  	%rd934, %rd836, %rd846;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r675}, %rd934;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r676,%dummy}, %rd934;
	}
	shf.l.wrap.b32 	%r677, %r676, %r675, 1;
	shf.l.wrap.b32 	%r678, %r675, %r676, 1;
	mov.b64 	%rd844, {%r678, %r677};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd842;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd841, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd844;
	shfl.sync.idx.b32  r_lo, v_lo, %r20, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r20, 0x1f, 0xffffffff;
	mov.b64  %rd843, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd846;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd845, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd848;
	shfl.sync.idx.b32  r_lo, v_lo, %r22, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r22, 0x1f, 0xffffffff;
	mov.b64  %rd847, {r_hi,r_lo};
	}
	// end inline asm
	cvt.u32.u64 	%r625, %rd841;
	cvt.u32.u64 	%r626, %rd843;
	// begin inline asm
	mul.wide.u32 %rd849, %r625, %r626;
	mad.lo.u64   %rd849, %rd849, 2, %rd843;
	// end inline asm
	add.s64 	%rd936, %rd849, %rd841;
	xor.b64  	%rd937, %rd936, %rd847;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r679,%dummy}, %rd937;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r680}, %rd937;
	}
	mov.b64 	%rd853, {%r680, %r679};
	cvt.u32.u64 	%r627, %rd845;
	cvt.u32.u64 	%r628, %rd853;
	// begin inline asm
	mul.wide.u32 %rd852, %r627, %r628;
	mad.lo.u64   %rd852, %rd852, 2, %rd853;
	// end inline asm
	add.s64 	%rd939, %rd852, %rd845;
	xor.b64  	%rd940, %rd939, %rd843;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r681,%dummy}, %rd940;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r682}, %rd940;
	}
	shf.r.wrap.b32 	%r683, %r682, %r681, 24;
	shf.r.wrap.b32 	%r684, %r681, %r682, 24;
	mov.b64 	%rd856, {%r684, %r683};
	cvt.u32.u64 	%r629, %rd936;
	cvt.u32.u64 	%r630, %rd856;
	// begin inline asm
	mul.wide.u32 %rd855, %r629, %r630;
	mad.lo.u64   %rd855, %rd855, 2, %rd856;
	// end inline asm
	add.s64 	%rd862, %rd855, %rd936;
	xor.b64  	%rd942, %rd853, %rd862;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r685,%dummy}, %rd942;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r686}, %rd942;
	}
	shf.r.wrap.b32 	%r687, %r686, %r685, 16;
	shf.r.wrap.b32 	%r688, %r685, %r686, 16;
	mov.b64 	%rd868, {%r688, %r687};
	cvt.u32.u64 	%r631, %rd939;
	cvt.u32.u64 	%r632, %rd868;
	// begin inline asm
	mul.wide.u32 %rd858, %r631, %r632;
	mad.lo.u64   %rd858, %rd858, 2, %rd868;
	// end inline asm
	add.s64 	%rd866, %rd858, %rd939;
	xor.b64  	%rd944, %rd856, %rd866;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r689}, %rd944;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r690,%dummy}, %rd944;
	}
	shf.l.wrap.b32 	%r691, %r690, %r689, 1;
	shf.l.wrap.b32 	%r692, %r689, %r690, 1;
	mov.b64 	%rd864, {%r692, %r691};
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd862;
	shfl.sync.idx.b32  r_lo, v_lo, %r3, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r3, 0x1f, 0xffffffff;
	mov.b64  %rd861, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd864;
	shfl.sync.idx.b32  r_lo, v_lo, %r22, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r22, 0x1f, 0xffffffff;
	mov.b64  %rd863, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd866;
	shfl.sync.idx.b32  r_lo, v_lo, %r21, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r21, 0x1f, 0xffffffff;
	mov.b64  %rd865, {r_hi,r_lo};
	}
	// end inline asm
	// begin inline asm
	{
	.reg .b32 v_lo;
	.reg .b32 v_hi;
	.reg .b32 r_lo;
	.reg .b32 r_hi;
	mov.b64  {v_hi,v_lo}, %rd868;
	shfl.sync.idx.b32  r_lo, v_lo, %r20, 0x1f, 0xffffffff;
	shfl.sync.idx.b32  r_hi, v_hi, %r20, 0x1f, 0xffffffff;
	mov.b64  %rd867, {r_hi,r_lo};
	}
	// end inline asm
	xor.b64  	%rd954, %rd861, %rd973;
	xor.b64  	%rd955, %rd863, %rd972;
	xor.b64  	%rd956, %rd865, %rd971;
	xor.b64  	%rd957, %rd867, %rd970;
	st.global.u64 	[%rd952], %rd954;
	st.global.u64 	[%rd952+256], %rd955;
	st.global.u64 	[%rd952+512], %rd956;
	st.global.u64 	[%rd952+768], %rd957;
	add.s64 	%rd952, %rd952, %rd945;
	add.s32 	%r698, %r698, 1;
	setp.lt.u32 	%p67, %r698, %r695;
	@%p67 bra 	$L__BB2_17;

$L__BB2_26:
	ret;

}

  