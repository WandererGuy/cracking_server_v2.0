//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.6
.target sm_86, texmode_independent
.address_size 64

	// .globl	nt
.const .align 4 .u32 halfShift = 10;
.const .align 4 .u32 halfBase = 65536;
.const .align 4 .u32 halfMask = 1023;
.const .align 1 .b8 opt_trailingBytesUTF8[64] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5};
.const .align 4 .b8 offsetsFromUTF8[24] = {0, 0, 0, 0, 128, 48, 0, 0, 128, 32, 14, 0, 128, 32, 200, 3, 128, 32, 8, 250, 128, 32, 8, 130};
// nt_$_s_bitmaps has been demoted

.entry nt(
	.param .u64 .ptr .global .align 4 nt_param_0,
	.param .u64 .ptr .global .align 4 nt_param_1,
	.param .u64 .ptr .global .align 4 nt_param_2,
	.param .u64 .ptr .const .align 4 nt_param_3,
	.param .u64 .ptr .global .align 4 nt_param_4,
	.param .u64 .ptr .global .align 4 nt_param_5,
	.param .u64 .ptr .global .align 4 nt_param_6,
	.param .u64 .ptr .global .align 4 nt_param_7,
	.param .u64 .ptr .global .align 4 nt_param_8
)
{
	.local .align 16 .b8 	__local_depot0[320];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b32 	%r<1114>;
	.reg .b64 	%rd<77>;
	// demoted variable
	.shared .align 4 .b8 nt_$_s_bitmaps[16384];

	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd11, [nt_param_0];
	ld.param.u64 	%rd17, [nt_param_1];
	ld.param.u64 	%rd12, [nt_param_4];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r45, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r1102, %tid.x;
	mov.b32 	%r46, %envreg3;
	add.s32 	%r47, %r1102, %r46;
	mad.lo.s32 	%r3, %r1, %r45, %r47;
	mul.wide.u32 	%rd19, %r3, 4;
	add.s64 	%rd20, %rd17, %rd19;
	ld.global.u32 	%r4, [%rd20];
	mov.u32 	%r44, 0;
	mov.u64 	%rd75, %rd1;
	mov.u32 	%r1101, %r44;

$L__BB0_1:
	st.local.u32 	[%rd75], %r44;
	add.s64 	%rd75, %rd75, 4;
	add.s32 	%r1101, %r1101, 1;
	setp.lt.u32 	%p1, %r1101, 80;
	@%p1 bra 	$L__BB0_1;

	and.b32  	%r7, %r4, 127;
	setp.gt.u32 	%p2, %r1102, 4095;
	@%p2 bra 	$L__BB0_5;

	mov.u64 	%rd23, nt_$_s_bitmaps;

$L__BB0_4:
	mul.wide.u32 	%rd21, %r1102, 4;
	add.s64 	%rd22, %rd12, %rd21;
	ld.global.u32 	%r49, [%rd22];
	add.s64 	%rd24, %rd23, %rd21;
	st.shared.u32 	[%rd24], %r49;
	add.s32 	%r1102, %r1102, %r1;
	setp.lt.u32 	%p3, %r1102, 4096;
	@%p3 bra 	$L__BB0_4;

$L__BB0_5:
	bar.sync 	0;
	add.s32 	%r10, %r7, 3;
	and.b32  	%r50, %r10, 252;
	setp.eq.s32 	%p4, %r50, 0;
	@%p4 bra 	$L__BB0_38;

	shr.u32 	%r51, %r4, 7;
	mul.wide.u32 	%rd25, %r51, 4;
	add.s64 	%rd5, %rd11, %rd25;
	ld.global.u32 	%r52, [%rd5];
	and.b32  	%r53, %r52, 255;
	and.b32  	%r54, %r52, 65280;
	and.b32  	%r55, %r52, 16711680;
	shr.u32 	%r56, %r55, 16;
	shr.u32 	%r57, %r52, 8;
	and.b32  	%r58, %r57, 16711680;
	prmt.b32 	%r59, %r54, %r53, 8452;
	or.b32  	%r60, %r56, %r58;
	st.local.v2.u32 	[%rd1], {%r59, %r60};
	setp.lt.u32 	%p5, %r10, 8;
	@%p5 bra 	$L__BB0_38;

	ld.global.u32 	%r61, [%rd5+4];
	and.b32  	%r62, %r61, 255;
	and.b32  	%r63, %r61, 65280;
	and.b32  	%r64, %r61, 16711680;
	shr.u32 	%r65, %r64, 16;
	shr.u32 	%r66, %r61, 8;
	and.b32  	%r67, %r66, 16711680;
	prmt.b32 	%r68, %r63, %r62, 8452;
	or.b32  	%r69, %r65, %r67;
	st.local.v2.u32 	[%rd1+8], {%r68, %r69};
	setp.lt.u32 	%p6, %r10, 12;
	@%p6 bra 	$L__BB0_38;

	ld.global.u32 	%r70, [%rd5+8];
	and.b32  	%r71, %r70, 255;
	and.b32  	%r72, %r70, 65280;
	and.b32  	%r73, %r70, 16711680;
	shr.u32 	%r74, %r73, 16;
	shr.u32 	%r75, %r70, 8;
	and.b32  	%r76, %r75, 16711680;
	prmt.b32 	%r77, %r72, %r71, 8452;
	or.b32  	%r78, %r74, %r76;
	st.local.v2.u32 	[%rd1+16], {%r77, %r78};
	setp.lt.u32 	%p7, %r10, 16;
	@%p7 bra 	$L__BB0_38;

	ld.global.u32 	%r79, [%rd5+12];
	and.b32  	%r80, %r79, 255;
	and.b32  	%r81, %r79, 65280;
	and.b32  	%r82, %r79, 16711680;
	shr.u32 	%r83, %r82, 16;
	shr.u32 	%r84, %r79, 8;
	and.b32  	%r85, %r84, 16711680;
	prmt.b32 	%r86, %r81, %r80, 8452;
	or.b32  	%r87, %r83, %r85;
	st.local.v2.u32 	[%rd1+24], {%r86, %r87};
	setp.lt.u32 	%p8, %r10, 20;
	@%p8 bra 	$L__BB0_38;

	ld.global.u32 	%r88, [%rd5+16];
	and.b32  	%r89, %r88, 255;
	and.b32  	%r90, %r88, 65280;
	and.b32  	%r91, %r88, 16711680;
	shr.u32 	%r92, %r91, 16;
	shr.u32 	%r93, %r88, 8;
	and.b32  	%r94, %r93, 16711680;
	prmt.b32 	%r95, %r90, %r89, 8452;
	or.b32  	%r96, %r92, %r94;
	st.local.v2.u32 	[%rd1+32], {%r95, %r96};
	setp.lt.u32 	%p9, %r10, 24;
	@%p9 bra 	$L__BB0_38;

	ld.global.u32 	%r97, [%rd5+20];
	and.b32  	%r98, %r97, 255;
	and.b32  	%r99, %r97, 65280;
	and.b32  	%r100, %r97, 16711680;
	shr.u32 	%r101, %r100, 16;
	shr.u32 	%r102, %r97, 8;
	and.b32  	%r103, %r102, 16711680;
	prmt.b32 	%r104, %r99, %r98, 8452;
	or.b32  	%r105, %r101, %r103;
	st.local.v2.u32 	[%rd1+40], {%r104, %r105};
	setp.lt.u32 	%p10, %r10, 28;
	@%p10 bra 	$L__BB0_38;

	ld.global.u32 	%r106, [%rd5+24];
	and.b32  	%r107, %r106, 255;
	and.b32  	%r108, %r106, 65280;
	and.b32  	%r109, %r106, 16711680;
	shr.u32 	%r110, %r109, 16;
	shr.u32 	%r111, %r106, 8;
	and.b32  	%r112, %r111, 16711680;
	prmt.b32 	%r113, %r108, %r107, 8452;
	or.b32  	%r114, %r110, %r112;
	st.local.v2.u32 	[%rd1+48], {%r113, %r114};
	setp.lt.u32 	%p11, %r10, 32;
	@%p11 bra 	$L__BB0_38;

	ld.global.u32 	%r115, [%rd5+28];
	and.b32  	%r116, %r115, 255;
	and.b32  	%r117, %r115, 65280;
	and.b32  	%r118, %r115, 16711680;
	shr.u32 	%r119, %r118, 16;
	shr.u32 	%r120, %r115, 8;
	and.b32  	%r121, %r120, 16711680;
	prmt.b32 	%r122, %r117, %r116, 8452;
	or.b32  	%r123, %r119, %r121;
	st.local.v2.u32 	[%rd1+56], {%r122, %r123};
	setp.lt.u32 	%p12, %r10, 36;
	@%p12 bra 	$L__BB0_38;

	ld.global.u32 	%r124, [%rd5+32];
	and.b32  	%r125, %r124, 255;
	and.b32  	%r126, %r124, 65280;
	and.b32  	%r127, %r124, 16711680;
	shr.u32 	%r128, %r127, 16;
	shr.u32 	%r129, %r124, 8;
	and.b32  	%r130, %r129, 16711680;
	prmt.b32 	%r131, %r126, %r125, 8452;
	or.b32  	%r132, %r128, %r130;
	st.local.v2.u32 	[%rd1+64], {%r131, %r132};
	setp.lt.u32 	%p13, %r10, 40;
	@%p13 bra 	$L__BB0_38;

	ld.global.u32 	%r133, [%rd5+36];
	and.b32  	%r134, %r133, 255;
	and.b32  	%r135, %r133, 65280;
	and.b32  	%r136, %r133, 16711680;
	shr.u32 	%r137, %r136, 16;
	shr.u32 	%r138, %r133, 8;
	and.b32  	%r139, %r138, 16711680;
	prmt.b32 	%r140, %r135, %r134, 8452;
	or.b32  	%r141, %r137, %r139;
	st.local.v2.u32 	[%rd1+72], {%r140, %r141};
	setp.lt.u32 	%p14, %r10, 44;
	@%p14 bra 	$L__BB0_38;

	ld.global.u32 	%r142, [%rd5+40];
	and.b32  	%r143, %r142, 255;
	and.b32  	%r144, %r142, 65280;
	and.b32  	%r145, %r142, 16711680;
	shr.u32 	%r146, %r145, 16;
	shr.u32 	%r147, %r142, 8;
	and.b32  	%r148, %r147, 16711680;
	prmt.b32 	%r149, %r144, %r143, 8452;
	or.b32  	%r150, %r146, %r148;
	st.local.v2.u32 	[%rd1+80], {%r149, %r150};
	setp.lt.u32 	%p15, %r10, 48;
	@%p15 bra 	$L__BB0_38;

	ld.global.u32 	%r151, [%rd5+44];
	and.b32  	%r152, %r151, 255;
	and.b32  	%r153, %r151, 65280;
	and.b32  	%r154, %r151, 16711680;
	shr.u32 	%r155, %r154, 16;
	shr.u32 	%r156, %r151, 8;
	and.b32  	%r157, %r156, 16711680;
	prmt.b32 	%r158, %r153, %r152, 8452;
	or.b32  	%r159, %r155, %r157;
	st.local.v2.u32 	[%rd1+88], {%r158, %r159};
	setp.lt.u32 	%p16, %r10, 52;
	@%p16 bra 	$L__BB0_38;

	ld.global.u32 	%r160, [%rd5+48];
	and.b32  	%r161, %r160, 255;
	and.b32  	%r162, %r160, 65280;
	and.b32  	%r163, %r160, 16711680;
	shr.u32 	%r164, %r163, 16;
	shr.u32 	%r165, %r160, 8;
	and.b32  	%r166, %r165, 16711680;
	prmt.b32 	%r167, %r162, %r161, 8452;
	or.b32  	%r168, %r164, %r166;
	st.local.v2.u32 	[%rd1+96], {%r167, %r168};
	setp.lt.u32 	%p17, %r10, 56;
	@%p17 bra 	$L__BB0_38;

	ld.global.u32 	%r169, [%rd5+52];
	and.b32  	%r170, %r169, 255;
	and.b32  	%r171, %r169, 65280;
	and.b32  	%r172, %r169, 16711680;
	shr.u32 	%r173, %r172, 16;
	shr.u32 	%r174, %r169, 8;
	and.b32  	%r175, %r174, 16711680;
	prmt.b32 	%r176, %r171, %r170, 8452;
	or.b32  	%r177, %r173, %r175;
	st.local.v2.u32 	[%rd1+104], {%r176, %r177};
	setp.lt.u32 	%p18, %r10, 60;
	@%p18 bra 	$L__BB0_38;

	ld.global.u32 	%r178, [%rd5+56];
	and.b32  	%r179, %r178, 255;
	and.b32  	%r180, %r178, 65280;
	and.b32  	%r181, %r178, 16711680;
	shr.u32 	%r182, %r181, 16;
	shr.u32 	%r183, %r178, 8;
	and.b32  	%r184, %r183, 16711680;
	prmt.b32 	%r185, %r180, %r179, 8452;
	or.b32  	%r186, %r182, %r184;
	st.local.v2.u32 	[%rd1+112], {%r185, %r186};
	setp.lt.u32 	%p19, %r10, 64;
	@%p19 bra 	$L__BB0_38;

	ld.global.u32 	%r187, [%rd5+60];
	and.b32  	%r188, %r187, 255;
	and.b32  	%r189, %r187, 65280;
	and.b32  	%r190, %r187, 16711680;
	shr.u32 	%r191, %r190, 16;
	shr.u32 	%r192, %r187, 8;
	and.b32  	%r193, %r192, 16711680;
	prmt.b32 	%r194, %r189, %r188, 8452;
	or.b32  	%r195, %r191, %r193;
	st.local.v2.u32 	[%rd1+120], {%r194, %r195};
	setp.lt.u32 	%p20, %r10, 68;
	@%p20 bra 	$L__BB0_38;

	ld.global.u32 	%r196, [%rd5+64];
	and.b32  	%r197, %r196, 255;
	and.b32  	%r198, %r196, 65280;
	and.b32  	%r199, %r196, 16711680;
	shr.u32 	%r200, %r199, 16;
	shr.u32 	%r201, %r196, 8;
	and.b32  	%r202, %r201, 16711680;
	prmt.b32 	%r203, %r198, %r197, 8452;
	or.b32  	%r204, %r200, %r202;
	st.local.v2.u32 	[%rd1+128], {%r203, %r204};
	setp.lt.u32 	%p21, %r10, 72;
	@%p21 bra 	$L__BB0_38;

	ld.global.u32 	%r205, [%rd5+68];
	and.b32  	%r206, %r205, 255;
	and.b32  	%r207, %r205, 65280;
	and.b32  	%r208, %r205, 16711680;
	shr.u32 	%r209, %r208, 16;
	shr.u32 	%r210, %r205, 8;
	and.b32  	%r211, %r210, 16711680;
	prmt.b32 	%r212, %r207, %r206, 8452;
	or.b32  	%r213, %r209, %r211;
	st.local.v2.u32 	[%rd1+136], {%r212, %r213};
	setp.lt.u32 	%p22, %r10, 76;
	@%p22 bra 	$L__BB0_38;

	ld.global.u32 	%r214, [%rd5+72];
	and.b32  	%r215, %r214, 255;
	and.b32  	%r216, %r214, 65280;
	and.b32  	%r217, %r214, 16711680;
	shr.u32 	%r218, %r217, 16;
	shr.u32 	%r219, %r214, 8;
	and.b32  	%r220, %r219, 16711680;
	prmt.b32 	%r221, %r216, %r215, 8452;
	or.b32  	%r222, %r218, %r220;
	st.local.v2.u32 	[%rd1+144], {%r221, %r222};
	setp.lt.u32 	%p23, %r10, 80;
	@%p23 bra 	$L__BB0_38;

	ld.global.u32 	%r223, [%rd5+76];
	and.b32  	%r224, %r223, 255;
	and.b32  	%r225, %r223, 65280;
	and.b32  	%r226, %r223, 16711680;
	shr.u32 	%r227, %r226, 16;
	shr.u32 	%r228, %r223, 8;
	and.b32  	%r229, %r228, 16711680;
	prmt.b32 	%r230, %r225, %r224, 8452;
	or.b32  	%r231, %r227, %r229;
	st.local.v2.u32 	[%rd1+152], {%r230, %r231};
	setp.lt.u32 	%p24, %r10, 84;
	@%p24 bra 	$L__BB0_38;

	ld.global.u32 	%r232, [%rd5+80];
	and.b32  	%r233, %r232, 255;
	and.b32  	%r234, %r232, 65280;
	and.b32  	%r235, %r232, 16711680;
	shr.u32 	%r236, %r235, 16;
	shr.u32 	%r237, %r232, 8;
	and.b32  	%r238, %r237, 16711680;
	prmt.b32 	%r239, %r234, %r233, 8452;
	or.b32  	%r240, %r236, %r238;
	st.local.v2.u32 	[%rd1+160], {%r239, %r240};
	setp.lt.u32 	%p25, %r10, 88;
	@%p25 bra 	$L__BB0_38;

	ld.global.u32 	%r241, [%rd5+84];
	and.b32  	%r242, %r241, 255;
	and.b32  	%r243, %r241, 65280;
	and.b32  	%r244, %r241, 16711680;
	shr.u32 	%r245, %r244, 16;
	shr.u32 	%r246, %r241, 8;
	and.b32  	%r247, %r246, 16711680;
	prmt.b32 	%r248, %r243, %r242, 8452;
	or.b32  	%r249, %r245, %r247;
	st.local.v2.u32 	[%rd1+168], {%r248, %r249};
	setp.lt.u32 	%p26, %r10, 92;
	@%p26 bra 	$L__BB0_38;

	ld.global.u32 	%r250, [%rd5+88];
	and.b32  	%r251, %r250, 255;
	and.b32  	%r252, %r250, 65280;
	and.b32  	%r253, %r250, 16711680;
	shr.u32 	%r254, %r253, 16;
	shr.u32 	%r255, %r250, 8;
	and.b32  	%r256, %r255, 16711680;
	prmt.b32 	%r257, %r252, %r251, 8452;
	or.b32  	%r258, %r254, %r256;
	st.local.v2.u32 	[%rd1+176], {%r257, %r258};
	setp.lt.u32 	%p27, %r10, 96;
	@%p27 bra 	$L__BB0_38;

	ld.global.u32 	%r259, [%rd5+92];
	and.b32  	%r260, %r259, 255;
	and.b32  	%r261, %r259, 65280;
	and.b32  	%r262, %r259, 16711680;
	shr.u32 	%r263, %r262, 16;
	shr.u32 	%r264, %r259, 8;
	and.b32  	%r265, %r264, 16711680;
	prmt.b32 	%r266, %r261, %r260, 8452;
	or.b32  	%r267, %r263, %r265;
	st.local.v2.u32 	[%rd1+184], {%r266, %r267};
	setp.lt.u32 	%p28, %r10, 100;
	@%p28 bra 	$L__BB0_38;

	ld.global.u32 	%r268, [%rd5+96];
	and.b32  	%r269, %r268, 255;
	and.b32  	%r270, %r268, 65280;
	and.b32  	%r271, %r268, 16711680;
	shr.u32 	%r272, %r271, 16;
	shr.u32 	%r273, %r268, 8;
	and.b32  	%r274, %r273, 16711680;
	prmt.b32 	%r275, %r270, %r269, 8452;
	or.b32  	%r276, %r272, %r274;
	st.local.v2.u32 	[%rd1+192], {%r275, %r276};
	setp.lt.u32 	%p29, %r10, 104;
	@%p29 bra 	$L__BB0_38;

	ld.global.u32 	%r277, [%rd5+100];
	and.b32  	%r278, %r277, 255;
	and.b32  	%r279, %r277, 65280;
	and.b32  	%r280, %r277, 16711680;
	shr.u32 	%r281, %r280, 16;
	shr.u32 	%r282, %r277, 8;
	and.b32  	%r283, %r282, 16711680;
	prmt.b32 	%r284, %r279, %r278, 8452;
	or.b32  	%r285, %r281, %r283;
	st.local.v2.u32 	[%rd1+200], {%r284, %r285};
	setp.lt.u32 	%p30, %r10, 108;
	@%p30 bra 	$L__BB0_38;

	ld.global.u32 	%r286, [%rd5+104];
	and.b32  	%r287, %r286, 255;
	and.b32  	%r288, %r286, 65280;
	and.b32  	%r289, %r286, 16711680;
	shr.u32 	%r290, %r289, 16;
	shr.u32 	%r291, %r286, 8;
	and.b32  	%r292, %r291, 16711680;
	prmt.b32 	%r293, %r288, %r287, 8452;
	or.b32  	%r294, %r290, %r292;
	st.local.v2.u32 	[%rd1+208], {%r293, %r294};
	setp.lt.u32 	%p31, %r10, 112;
	@%p31 bra 	$L__BB0_38;

	ld.global.u32 	%r295, [%rd5+108];
	and.b32  	%r296, %r295, 255;
	and.b32  	%r297, %r295, 65280;
	and.b32  	%r298, %r295, 16711680;
	shr.u32 	%r299, %r298, 16;
	shr.u32 	%r300, %r295, 8;
	and.b32  	%r301, %r300, 16711680;
	prmt.b32 	%r302, %r297, %r296, 8452;
	or.b32  	%r303, %r299, %r301;
	st.local.v2.u32 	[%rd1+216], {%r302, %r303};
	setp.lt.u32 	%p32, %r10, 116;
	@%p32 bra 	$L__BB0_38;

	ld.global.u32 	%r304, [%rd5+112];
	and.b32  	%r305, %r304, 255;
	and.b32  	%r306, %r304, 65280;
	and.b32  	%r307, %r304, 16711680;
	shr.u32 	%r308, %r307, 16;
	shr.u32 	%r309, %r304, 8;
	and.b32  	%r310, %r309, 16711680;
	prmt.b32 	%r311, %r306, %r305, 8452;
	or.b32  	%r312, %r308, %r310;
	st.local.v2.u32 	[%rd1+224], {%r311, %r312};
	setp.lt.u32 	%p33, %r10, 120;
	@%p33 bra 	$L__BB0_38;

	ld.global.u32 	%r313, [%rd5+116];
	and.b32  	%r314, %r313, 255;
	and.b32  	%r315, %r313, 65280;
	and.b32  	%r316, %r313, 16711680;
	shr.u32 	%r317, %r316, 16;
	shr.u32 	%r318, %r313, 8;
	and.b32  	%r319, %r318, 16711680;
	prmt.b32 	%r320, %r315, %r314, 8452;
	or.b32  	%r321, %r317, %r319;
	st.local.v2.u32 	[%rd1+232], {%r320, %r321};
	setp.lt.u32 	%p34, %r10, 124;
	@%p34 bra 	$L__BB0_38;

	ld.global.u32 	%r322, [%rd5+120];
	and.b32  	%r323, %r322, 255;
	and.b32  	%r324, %r322, 65280;
	and.b32  	%r325, %r322, 16711680;
	shr.u32 	%r326, %r325, 16;
	shr.u32 	%r327, %r322, 8;
	and.b32  	%r328, %r327, 16711680;
	prmt.b32 	%r329, %r324, %r323, 8452;
	or.b32  	%r330, %r326, %r328;
	st.local.v2.u32 	[%rd1+240], {%r329, %r330};
	setp.lt.u32 	%p35, %r10, 128;
	@%p35 bra 	$L__BB0_38;

	ld.global.u32 	%r331, [%rd5+124];
	and.b32  	%r332, %r331, 255;
	and.b32  	%r333, %r331, 65280;
	and.b32  	%r334, %r331, 16711680;
	shr.u32 	%r335, %r334, 16;
	shr.u32 	%r336, %r331, 8;
	and.b32  	%r337, %r336, 16711680;
	prmt.b32 	%r338, %r333, %r332, 8452;
	or.b32  	%r339, %r335, %r337;
	st.local.v2.u32 	[%rd1+248], {%r338, %r339};

$L__BB0_38:
	shr.u32 	%r512, %r7, 1;
	mul.wide.u32 	%rd26, %r512, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.local.u16 	%r513, [%rd27];
	shl.b32 	%r514, %r4, 4;
	and.b32  	%r515, %r514, 16;
	mov.u32 	%r516, 128;
	shl.b32 	%r517, %r516, %r515;
	or.b32  	%r518, %r513, %r517;
	st.local.u32 	[%rd27], %r518;
	add.s32 	%r519, %r7, 36;
	shr.u32 	%r11, %r519, 5;
	shl.b32 	%r520, %r11, 4;
	add.s32 	%r521, %r520, -2;
	mul.wide.u32 	%rd28, %r521, 4;
	add.s64 	%rd29, %rd1, %rd28;
	shl.b32 	%r522, %r7, 4;
	st.local.u32 	[%rd29], %r522;
	ld.local.v4.u32 	{%r523, %r524, %r525, %r526}, [%rd1];
	add.s32 	%r531, %r523, -1;
	shf.l.wrap.b32 	%r347, %r531, %r531, 3;
	and.b32  	%r532, %r347, 2004318071;
	xor.b32  	%r533, %r532, -1732584194;
	add.s32 	%r534, %r524, %r533;
	add.s32 	%r535, %r534, 271733878;
	shf.l.wrap.b32 	%r351, %r535, %r535, 7;
	mov.u32 	%r343, -271733879;
	// begin inline asm
	lop3.b32 %r340, %r351, %r347, %r343, 202;
	// end inline asm
	add.s32 	%r536, %r340, %r525;
	add.s32 	%r537, %r536, -1732584194;
	shf.l.wrap.b32 	%r355, %r537, %r537, 11;
	// begin inline asm
	lop3.b32 %r344, %r355, %r351, %r347, 202;
	// end inline asm
	add.s32 	%r538, %r344, %r526;
	add.s32 	%r539, %r538, -271733879;
	shf.l.wrap.b32 	%r359, %r539, %r539, 19;
	// begin inline asm
	lop3.b32 %r348, %r359, %r355, %r351, 202;
	// end inline asm
	ld.local.v4.u32 	{%r540, %r541, %r542, %r543}, [%rd1+16];
	add.s32 	%r547, %r348, %r347;
	add.s32 	%r548, %r547, %r540;
	shf.l.wrap.b32 	%r363, %r548, %r548, 3;
	// begin inline asm
	lop3.b32 %r352, %r363, %r359, %r355, 202;
	// end inline asm
	add.s32 	%r549, %r352, %r351;
	add.s32 	%r550, %r549, %r541;
	shf.l.wrap.b32 	%r367, %r550, %r550, 7;
	// begin inline asm
	lop3.b32 %r356, %r367, %r363, %r359, 202;
	// end inline asm
	add.s32 	%r551, %r356, %r355;
	add.s32 	%r552, %r551, %r542;
	shf.l.wrap.b32 	%r371, %r552, %r552, 11;
	// begin inline asm
	lop3.b32 %r360, %r371, %r367, %r363, 202;
	// end inline asm
	add.s32 	%r553, %r360, %r359;
	add.s32 	%r554, %r553, %r543;
	shf.l.wrap.b32 	%r375, %r554, %r554, 19;
	// begin inline asm
	lop3.b32 %r364, %r375, %r371, %r367, 202;
	// end inline asm
	ld.local.v4.u32 	{%r555, %r556, %r557, %r558}, [%rd1+32];
	add.s32 	%r562, %r364, %r363;
	add.s32 	%r563, %r562, %r555;
	shf.l.wrap.b32 	%r379, %r563, %r563, 3;
	// begin inline asm
	lop3.b32 %r368, %r379, %r375, %r371, 202;
	// end inline asm
	add.s32 	%r564, %r368, %r367;
	add.s32 	%r565, %r564, %r556;
	shf.l.wrap.b32 	%r383, %r565, %r565, 7;
	// begin inline asm
	lop3.b32 %r372, %r383, %r379, %r375, 202;
	// end inline asm
	add.s32 	%r566, %r372, %r371;
	add.s32 	%r567, %r566, %r557;
	shf.l.wrap.b32 	%r387, %r567, %r567, 11;
	// begin inline asm
	lop3.b32 %r376, %r387, %r383, %r379, 202;
	// end inline asm
	add.s32 	%r568, %r376, %r375;
	add.s32 	%r569, %r568, %r558;
	shf.l.wrap.b32 	%r391, %r569, %r569, 19;
	// begin inline asm
	lop3.b32 %r380, %r391, %r387, %r383, 202;
	// end inline asm
	ld.local.v4.u32 	{%r570, %r571, %r572, %r573}, [%rd1+48];
	add.s32 	%r577, %r380, %r379;
	add.s32 	%r578, %r577, %r570;
	shf.l.wrap.b32 	%r395, %r578, %r578, 3;
	// begin inline asm
	lop3.b32 %r384, %r395, %r391, %r387, 202;
	// end inline asm
	add.s32 	%r579, %r384, %r383;
	add.s32 	%r580, %r579, %r571;
	shf.l.wrap.b32 	%r399, %r580, %r580, 7;
	// begin inline asm
	lop3.b32 %r388, %r399, %r395, %r391, 202;
	// end inline asm
	add.s32 	%r581, %r388, %r387;
	add.s32 	%r582, %r581, %r572;
	shf.l.wrap.b32 	%r403, %r582, %r582, 11;
	// begin inline asm
	lop3.b32 %r392, %r403, %r399, %r395, 202;
	// end inline asm
	add.s32 	%r583, %r392, %r391;
	add.s32 	%r584, %r583, %r573;
	shf.l.wrap.b32 	%r407, %r584, %r584, 19;
	// begin inline asm
	lop3.b32 %r396, %r407, %r403, %r399, 232;
	// end inline asm
	add.s32 	%r585, %r395, %r396;
	add.s32 	%r586, %r585, %r523;
	add.s32 	%r587, %r586, 1518500249;
	shf.l.wrap.b32 	%r411, %r587, %r587, 3;
	// begin inline asm
	lop3.b32 %r400, %r411, %r407, %r403, 232;
	// end inline asm
	add.s32 	%r588, %r399, %r400;
	add.s32 	%r589, %r588, %r540;
	add.s32 	%r590, %r589, 1518500249;
	shf.l.wrap.b32 	%r415, %r590, %r590, 5;
	// begin inline asm
	lop3.b32 %r404, %r415, %r411, %r407, 232;
	// end inline asm
	add.s32 	%r591, %r403, %r404;
	add.s32 	%r592, %r591, %r555;
	add.s32 	%r593, %r592, 1518500249;
	shf.l.wrap.b32 	%r419, %r593, %r593, 9;
	// begin inline asm
	lop3.b32 %r408, %r419, %r415, %r411, 232;
	// end inline asm
	add.s32 	%r594, %r407, %r408;
	add.s32 	%r595, %r594, %r570;
	add.s32 	%r596, %r595, 1518500249;
	shf.l.wrap.b32 	%r423, %r596, %r596, 13;
	// begin inline asm
	lop3.b32 %r412, %r423, %r419, %r415, 232;
	// end inline asm
	add.s32 	%r597, %r411, %r412;
	add.s32 	%r598, %r597, %r524;
	add.s32 	%r599, %r598, 1518500249;
	shf.l.wrap.b32 	%r427, %r599, %r599, 3;
	// begin inline asm
	lop3.b32 %r416, %r427, %r423, %r419, 232;
	// end inline asm
	add.s32 	%r600, %r415, %r416;
	add.s32 	%r601, %r600, %r541;
	add.s32 	%r602, %r601, 1518500249;
	shf.l.wrap.b32 	%r431, %r602, %r602, 5;
	// begin inline asm
	lop3.b32 %r420, %r431, %r427, %r423, 232;
	// end inline asm
	add.s32 	%r603, %r419, %r420;
	add.s32 	%r604, %r603, %r556;
	add.s32 	%r605, %r604, 1518500249;
	shf.l.wrap.b32 	%r435, %r605, %r605, 9;
	// begin inline asm
	lop3.b32 %r424, %r435, %r431, %r427, 232;
	// end inline asm
	add.s32 	%r606, %r423, %r424;
	add.s32 	%r607, %r606, %r571;
	add.s32 	%r608, %r607, 1518500249;
	shf.l.wrap.b32 	%r439, %r608, %r608, 13;
	// begin inline asm
	lop3.b32 %r428, %r439, %r435, %r431, 232;
	// end inline asm
	add.s32 	%r609, %r427, %r428;
	add.s32 	%r610, %r609, %r525;
	add.s32 	%r611, %r610, 1518500249;
	shf.l.wrap.b32 	%r443, %r611, %r611, 3;
	// begin inline asm
	lop3.b32 %r432, %r443, %r439, %r435, 232;
	// end inline asm
	add.s32 	%r612, %r431, %r432;
	add.s32 	%r613, %r612, %r542;
	add.s32 	%r614, %r613, 1518500249;
	shf.l.wrap.b32 	%r447, %r614, %r614, 5;
	// begin inline asm
	lop3.b32 %r436, %r447, %r443, %r439, 232;
	// end inline asm
	add.s32 	%r615, %r435, %r436;
	add.s32 	%r616, %r615, %r557;
	add.s32 	%r617, %r616, 1518500249;
	shf.l.wrap.b32 	%r451, %r617, %r617, 9;
	// begin inline asm
	lop3.b32 %r440, %r451, %r447, %r443, 232;
	// end inline asm
	add.s32 	%r618, %r439, %r440;
	add.s32 	%r619, %r618, %r572;
	add.s32 	%r620, %r619, 1518500249;
	shf.l.wrap.b32 	%r455, %r620, %r620, 13;
	// begin inline asm
	lop3.b32 %r444, %r455, %r451, %r447, 232;
	// end inline asm
	add.s32 	%r621, %r443, %r444;
	add.s32 	%r622, %r621, %r526;
	add.s32 	%r623, %r622, 1518500249;
	shf.l.wrap.b32 	%r459, %r623, %r623, 3;
	// begin inline asm
	lop3.b32 %r448, %r459, %r455, %r451, 232;
	// end inline asm
	add.s32 	%r624, %r447, %r448;
	add.s32 	%r625, %r624, %r543;
	add.s32 	%r626, %r625, 1518500249;
	shf.l.wrap.b32 	%r463, %r626, %r626, 5;
	// begin inline asm
	lop3.b32 %r452, %r463, %r459, %r455, 232;
	// end inline asm
	add.s32 	%r627, %r451, %r452;
	add.s32 	%r628, %r627, %r558;
	add.s32 	%r629, %r628, 1518500249;
	shf.l.wrap.b32 	%r467, %r629, %r629, 9;
	// begin inline asm
	lop3.b32 %r456, %r467, %r463, %r459, 232;
	// end inline asm
	add.s32 	%r630, %r455, %r456;
	add.s32 	%r631, %r630, %r573;
	add.s32 	%r632, %r631, 1518500249;
	shf.l.wrap.b32 	%r471, %r632, %r632, 13;
	// begin inline asm
	lop3.b32 %r460, %r471, %r467, %r463, 150;
	// end inline asm
	add.s32 	%r633, %r459, %r460;
	add.s32 	%r634, %r633, %r523;
	add.s32 	%r635, %r634, 1859775393;
	shf.l.wrap.b32 	%r475, %r635, %r635, 3;
	// begin inline asm
	lop3.b32 %r464, %r475, %r471, %r467, 150;
	// end inline asm
	add.s32 	%r636, %r463, %r464;
	add.s32 	%r637, %r636, %r555;
	add.s32 	%r638, %r637, 1859775393;
	shf.l.wrap.b32 	%r479, %r638, %r638, 9;
	// begin inline asm
	lop3.b32 %r468, %r479, %r475, %r471, 150;
	// end inline asm
	add.s32 	%r639, %r467, %r468;
	add.s32 	%r640, %r639, %r540;
	add.s32 	%r641, %r640, 1859775393;
	shf.l.wrap.b32 	%r483, %r641, %r641, 11;
	// begin inline asm
	lop3.b32 %r472, %r483, %r479, %r475, 150;
	// end inline asm
	add.s32 	%r642, %r471, %r472;
	add.s32 	%r643, %r642, %r570;
	add.s32 	%r644, %r643, 1859775393;
	shf.l.wrap.b32 	%r487, %r644, %r644, 15;
	// begin inline asm
	lop3.b32 %r476, %r487, %r483, %r479, 150;
	// end inline asm
	add.s32 	%r645, %r475, %r476;
	add.s32 	%r646, %r645, %r525;
	add.s32 	%r647, %r646, 1859775393;
	shf.l.wrap.b32 	%r491, %r647, %r647, 3;
	// begin inline asm
	lop3.b32 %r480, %r491, %r487, %r483, 150;
	// end inline asm
	add.s32 	%r648, %r479, %r480;
	add.s32 	%r649, %r648, %r557;
	add.s32 	%r650, %r649, 1859775393;
	shf.l.wrap.b32 	%r495, %r650, %r650, 9;
	// begin inline asm
	lop3.b32 %r484, %r495, %r491, %r487, 150;
	// end inline asm
	add.s32 	%r651, %r483, %r484;
	add.s32 	%r652, %r651, %r542;
	add.s32 	%r653, %r652, 1859775393;
	shf.l.wrap.b32 	%r499, %r653, %r653, 11;
	// begin inline asm
	lop3.b32 %r488, %r499, %r495, %r491, 150;
	// end inline asm
	add.s32 	%r654, %r487, %r488;
	add.s32 	%r655, %r654, %r572;
	add.s32 	%r656, %r655, 1859775393;
	shf.l.wrap.b32 	%r503, %r656, %r656, 15;
	// begin inline asm
	lop3.b32 %r492, %r503, %r499, %r495, 150;
	// end inline asm
	add.s32 	%r657, %r491, %r492;
	add.s32 	%r658, %r657, %r524;
	add.s32 	%r659, %r658, 1859775393;
	shf.l.wrap.b32 	%r507, %r659, %r659, 3;
	// begin inline asm
	lop3.b32 %r496, %r507, %r503, %r499, 150;
	// end inline asm
	add.s32 	%r660, %r495, %r496;
	add.s32 	%r661, %r660, %r556;
	add.s32 	%r662, %r661, 1859775393;
	shf.l.wrap.b32 	%r509, %r662, %r662, 9;
	// begin inline asm
	lop3.b32 %r500, %r509, %r507, %r503, 150;
	// end inline asm
	add.s32 	%r663, %r499, %r500;
	add.s32 	%r664, %r663, %r541;
	add.s32 	%r665, %r664, 1859775393;
	shf.l.wrap.b32 	%r510, %r665, %r665, 11;
	// begin inline asm
	lop3.b32 %r504, %r510, %r509, %r507, 150;
	// end inline asm
	add.s32 	%r666, %r504, %r503;
	add.s32 	%r1112, %r666, %r571;
	add.s32 	%r667, %r1112, 1859775393;
	shf.l.wrap.b32 	%r511, %r667, %r667, 15;
	// begin inline asm
	lop3.b32 %r508, %r509, %r510, %r511, 150;
	// end inline asm
	add.s32 	%r668, %r507, %r508;
	add.s32 	%r669, %r668, %r526;
	add.s32 	%r670, %r669, 1859775393;
	shf.l.wrap.b32 	%r1113, %r670, %r670, 3;
	setp.lt.u32 	%p36, %r7, 28;
	@%p36 bra 	$L__BB0_43;
	bra.uni 	$L__BB0_39;

$L__BB0_43:
	and.b32  	%r1055, %r1112, 32736;
	shr.u32 	%r1056, %r1055, 5;
	mul.wide.u32 	%rd30, %r1056, 4;
	mov.u64 	%rd31, nt_$_s_bitmaps;
	add.s64 	%rd32, %rd31, %rd30;
	and.b32  	%r1057, %r1112, 31;
	ld.shared.u32 	%r1058, [%rd32];
	shr.u32 	%r1059, %r1058, %r1057;
	shr.u32 	%r1060, %r1112, 16;
	shr.u32 	%r1061, %r1112, 21;
	or.b32  	%r1062, %r1061, 1024;
	mul.wide.u32 	%rd33, %r1062, 4;
	add.s64 	%rd34, %rd31, %rd33;
	and.b32  	%r1063, %r1060, 31;
	ld.shared.u32 	%r1064, [%rd34];
	shr.u32 	%r1065, %r1064, %r1063;
	and.b32  	%r1066, %r1113, 32736;
	shr.u32 	%r1067, %r1066, 5;
	or.b32  	%r1068, %r1067, 2048;
	mul.wide.u32 	%rd35, %r1068, 4;
	add.s64 	%rd36, %rd31, %rd35;
	and.b32  	%r1069, %r1113, 31;
	ld.shared.u32 	%r1070, [%rd36];
	shr.u32 	%r1071, %r1070, %r1069;
	shr.u32 	%r1072, %r1113, 16;
	shr.u32 	%r1073, %r1113, 21;
	or.b32  	%r1074, %r1073, 3072;
	mul.wide.u32 	%rd37, %r1074, 4;
	add.s64 	%rd38, %rd31, %rd37;
	and.b32  	%r1075, %r1072, 31;
	ld.shared.u32 	%r1076, [%rd38];
	shr.u32 	%r1077, %r1076, %r1075;
	and.b32  	%r1078, %r1065, 1;
	setp.eq.b32 	%p39, %r1078, 1;
	and.b32  	%r1079, %r1059, 1;
	setp.eq.b32 	%p40, %r1079, 1;
	and.pred  	%p41, %p40, %p39;
	and.b32  	%r1080, %r1071, 1;
	setp.eq.b32 	%p42, %r1080, 1;
	and.pred  	%p43, %p41, %p42;
	and.b32  	%r1081, %r1077, 1;
	setp.eq.b32 	%p44, %r1081, 1;
	and.pred  	%p45, %p43, %p44;
	mov.pred 	%p46, 0;
	xor.pred  	%p47, %p45, %p46;
	not.pred 	%p48, %p47;
	@%p48 bra 	$L__BB0_48;

	ld.param.u64 	%rd70, [nt_param_6];
	ld.param.u64 	%rd69, [nt_param_5];
	cvt.u64.u32 	%rd39, %r1112;
	cvt.u64.u32 	%rd40, %r1113;
	bfi.b64 	%rd41, %rd39, %rd40, 32, 32;
	mul.hi.u64 	%rd42, %rd41, 1908283869694091547;
	sub.s64 	%rd43, %rd41, %rd42;
	shr.u64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd44, %rd42;
	shr.u64 	%rd46, %rd45, 4;
	mul.lo.s64 	%rd47, %rd46, 29;
	sub.s64 	%rd48, %rd41, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd69, %rd49;
	ld.global.u32 	%rd51, [%rd50];
	add.s64 	%rd52, %rd41, %rd51;
	mul.hi.u64 	%rd53, %rd52, 5646962471543740291;
	sub.s64 	%rd54, %rd52, %rd53;
	shr.u64 	%rd55, %rd54, 1;
	add.s64 	%rd56, %rd55, %rd53;
	shr.u64 	%rd57, %rd56, 5;
	mul.lo.s64 	%rd58, %rd57, 49;
	sub.s64 	%rd9, %rd52, %rd58;
	cvt.u32.u64 	%r43, %rd9;
	shl.b64 	%rd59, %rd9, 2;
	add.s64 	%rd10, %rd70, %rd59;
	ld.global.u32 	%r1082, [%rd10];
	setp.ne.s32 	%p49, %r1082, %r1113;
	@%p49 bra 	$L__BB0_48;

	ld.global.u32 	%r1083, [%rd10+196];
	setp.ne.s32 	%p50, %r1083, %r1112;
	@%p50 bra 	$L__BB0_48;

	ld.param.u64 	%rd71, [nt_param_8];
	shr.u64 	%rd60, %rd9, 3;
	and.b64  	%rd61, %rd60, 2305843009213693948;
	add.s64 	%rd62, %rd71, %rd61;
	and.b32  	%r1084, %r43, 31;
	mov.u32 	%r1085, 1;
	shl.b32 	%r1086, %r1085, %r1084;
	atom.global.or.b32 	%r1087, [%rd62], %r1086;
	and.b32  	%r1088, %r1087, %r1086;
	setp.ne.s32 	%p51, %r1088, 0;
	@%p51 bra 	$L__BB0_48;

	mov.b32 	%r1100, %envreg3;
	mov.u32 	%r1099, %tid.x;
	add.s32 	%r1098, %r1099, %r1100;
	mov.u32 	%r1097, %ctaid.x;
	mov.u32 	%r1096, %ntid.x;
	mad.lo.s32 	%r1095, %r1096, %r1097, %r1098;
	ld.param.u64 	%rd72, [nt_param_7];
	atom.global.add.u32 	%r1089, [%rd72], 1;
	mul.lo.s32 	%r1090, %r1089, 3;
	add.s32 	%r1091, %r1090, 1;
	mul.wide.u32 	%rd63, %r1091, 4;
	add.s64 	%rd64, %rd72, %rd63;
	st.volatile.global.u32 	[%rd64], %r1095;
	add.s32 	%r1092, %r1090, 2;
	mul.wide.u32 	%rd65, %r1092, 4;
	add.s64 	%rd66, %rd72, %rd65;
	mov.u32 	%r1093, 0;
	st.volatile.global.u32 	[%rd66], %r1093;
	add.s32 	%r1094, %r1090, 3;
	mul.wide.u32 	%rd67, %r1094, 4;
	add.s64 	%rd68, %rd72, %rd67;
	st.volatile.global.u32 	[%rd68], %r43;

$L__BB0_48:
	ret;

$L__BB0_39:
	// begin inline asm
	lop3.b32 %r671, %r510, %r511, %r1113, 150;
	// end inline asm
	add.s32 	%r683, %r509, %r671;
	add.s32 	%r684, %r683, %r558;
	add.s32 	%r685, %r684, 1859775393;
	shf.l.wrap.b32 	%r681, %r685, %r685, 9;
	// begin inline asm
	lop3.b32 %r675, %r511, %r1113, %r681, 150;
	// end inline asm
	add.s32 	%r686, %r510, %r675;
	add.s32 	%r687, %r686, %r543;
	add.s32 	%r688, %r687, 1859775393;
	shf.l.wrap.b32 	%r680, %r688, %r688, 11;
	// begin inline asm
	lop3.b32 %r679, %r680, %r681, %r1113, 150;
	// end inline asm
	add.s32 	%r689, %r511, %r679;
	add.s32 	%r690, %r689, %r573;
	add.s32 	%r691, %r690, 1859775393;
	shf.l.wrap.b32 	%r692, %r691, %r691, 15;
	add.s32 	%r1111, %r1113, 1732584193;
	add.s32 	%r1110, %r692, -271733879;
	add.s32 	%r1109, %r680, -1732584194;
	add.s32 	%r1108, %r681, 271733878;
	add.s32 	%r1103, %r11, -1;
	setp.eq.s32 	%p37, %r1103, 0;
	@%p37 bra 	$L__BB0_42;

	add.u64 	%rd76, %SPL, 0;

$L__BB0_41:
	add.s64 	%rd8, %rd76, 64;
	ld.local.v4.u32 	{%r885, %r886, %r887, %r888}, [%rd76+64];
	// begin inline asm
	lop3.b32 %r693, %r1110, %r1109, %r1108, 202;
	// end inline asm
	add.s32 	%r893, %r693, %r1111;
	add.s32 	%r894, %r893, %r885;
	shf.l.wrap.b32 	%r708, %r894, %r894, 3;
	// begin inline asm
	lop3.b32 %r697, %r708, %r1110, %r1109, 202;
	// end inline asm
	add.s32 	%r895, %r697, %r1108;
	add.s32 	%r896, %r895, %r886;
	shf.l.wrap.b32 	%r712, %r896, %r896, 7;
	// begin inline asm
	lop3.b32 %r701, %r712, %r708, %r1110, 202;
	// end inline asm
	add.s32 	%r897, %r701, %r1109;
	add.s32 	%r898, %r897, %r887;
	shf.l.wrap.b32 	%r716, %r898, %r898, 11;
	// begin inline asm
	lop3.b32 %r705, %r716, %r712, %r708, 202;
	// end inline asm
	add.s32 	%r899, %r705, %r1110;
	add.s32 	%r900, %r899, %r888;
	shf.l.wrap.b32 	%r720, %r900, %r900, 19;
	// begin inline asm
	lop3.b32 %r709, %r720, %r716, %r712, 202;
	// end inline asm
	ld.local.v4.u32 	{%r901, %r902, %r903, %r904}, [%rd76+80];
	add.s32 	%r909, %r709, %r708;
	add.s32 	%r910, %r909, %r901;
	shf.l.wrap.b32 	%r724, %r910, %r910, 3;
	// begin inline asm
	lop3.b32 %r713, %r724, %r720, %r716, 202;
	// end inline asm
	add.s32 	%r911, %r713, %r712;
	add.s32 	%r912, %r911, %r902;
	shf.l.wrap.b32 	%r728, %r912, %r912, 7;
	// begin inline asm
	lop3.b32 %r717, %r728, %r724, %r720, 202;
	// end inline asm
	add.s32 	%r913, %r717, %r716;
	add.s32 	%r914, %r913, %r903;
	shf.l.wrap.b32 	%r732, %r914, %r914, 11;
	// begin inline asm
	lop3.b32 %r721, %r732, %r728, %r724, 202;
	// end inline asm
	add.s32 	%r915, %r721, %r720;
	add.s32 	%r916, %r915, %r904;
	shf.l.wrap.b32 	%r736, %r916, %r916, 19;
	// begin inline asm
	lop3.b32 %r725, %r736, %r732, %r728, 202;
	// end inline asm
	ld.local.v4.u32 	{%r917, %r918, %r919, %r920}, [%rd76+96];
	add.s32 	%r925, %r725, %r724;
	add.s32 	%r926, %r925, %r917;
	shf.l.wrap.b32 	%r740, %r926, %r926, 3;
	// begin inline asm
	lop3.b32 %r729, %r740, %r736, %r732, 202;
	// end inline asm
	add.s32 	%r927, %r729, %r728;
	add.s32 	%r928, %r927, %r918;
	shf.l.wrap.b32 	%r744, %r928, %r928, 7;
	// begin inline asm
	lop3.b32 %r733, %r744, %r740, %r736, 202;
	// end inline asm
	add.s32 	%r929, %r733, %r732;
	add.s32 	%r930, %r929, %r919;
	shf.l.wrap.b32 	%r748, %r930, %r930, 11;
	// begin inline asm
	lop3.b32 %r737, %r748, %r744, %r740, 202;
	// end inline asm
	add.s32 	%r931, %r737, %r736;
	add.s32 	%r932, %r931, %r920;
	shf.l.wrap.b32 	%r752, %r932, %r932, 19;
	// begin inline asm
	lop3.b32 %r741, %r752, %r748, %r744, 202;
	// end inline asm
	ld.local.v4.u32 	{%r933, %r934, %r935, %r936}, [%rd76+112];
	add.s32 	%r941, %r741, %r740;
	add.s32 	%r942, %r941, %r933;
	shf.l.wrap.b32 	%r756, %r942, %r942, 3;
	// begin inline asm
	lop3.b32 %r745, %r756, %r752, %r748, 202;
	// end inline asm
	add.s32 	%r943, %r745, %r744;
	add.s32 	%r944, %r943, %r934;
	shf.l.wrap.b32 	%r760, %r944, %r944, 7;
	// begin inline asm
	lop3.b32 %r749, %r760, %r756, %r752, 202;
	// end inline asm
	add.s32 	%r945, %r749, %r748;
	add.s32 	%r946, %r945, %r935;
	shf.l.wrap.b32 	%r764, %r946, %r946, 11;
	// begin inline asm
	lop3.b32 %r753, %r764, %r760, %r756, 202;
	// end inline asm
	add.s32 	%r947, %r753, %r752;
	add.s32 	%r948, %r947, %r936;
	shf.l.wrap.b32 	%r768, %r948, %r948, 19;
	// begin inline asm
	lop3.b32 %r757, %r768, %r764, %r760, 232;
	// end inline asm
	add.s32 	%r949, %r756, %r757;
	add.s32 	%r950, %r949, %r885;
	add.s32 	%r951, %r950, 1518500249;
	shf.l.wrap.b32 	%r772, %r951, %r951, 3;
	// begin inline asm
	lop3.b32 %r761, %r772, %r768, %r764, 232;
	// end inline asm
	add.s32 	%r952, %r760, %r761;
	add.s32 	%r953, %r952, %r901;
	add.s32 	%r954, %r953, 1518500249;
	shf.l.wrap.b32 	%r776, %r954, %r954, 5;
	// begin inline asm
	lop3.b32 %r765, %r776, %r772, %r768, 232;
	// end inline asm
	add.s32 	%r955, %r764, %r765;
	add.s32 	%r956, %r955, %r917;
	add.s32 	%r957, %r956, 1518500249;
	shf.l.wrap.b32 	%r780, %r957, %r957, 9;
	// begin inline asm
	lop3.b32 %r769, %r780, %r776, %r772, 232;
	// end inline asm
	add.s32 	%r958, %r768, %r769;
	add.s32 	%r959, %r958, %r933;
	add.s32 	%r960, %r959, 1518500249;
	shf.l.wrap.b32 	%r784, %r960, %r960, 13;
	// begin inline asm
	lop3.b32 %r773, %r784, %r780, %r776, 232;
	// end inline asm
	add.s32 	%r961, %r772, %r773;
	add.s32 	%r962, %r961, %r886;
	add.s32 	%r963, %r962, 1518500249;
	shf.l.wrap.b32 	%r788, %r963, %r963, 3;
	// begin inline asm
	lop3.b32 %r777, %r788, %r784, %r780, 232;
	// end inline asm
	add.s32 	%r964, %r776, %r777;
	add.s32 	%r965, %r964, %r902;
	add.s32 	%r966, %r965, 1518500249;
	shf.l.wrap.b32 	%r792, %r966, %r966, 5;
	// begin inline asm
	lop3.b32 %r781, %r792, %r788, %r784, 232;
	// end inline asm
	add.s32 	%r967, %r780, %r781;
	add.s32 	%r968, %r967, %r918;
	add.s32 	%r969, %r968, 1518500249;
	shf.l.wrap.b32 	%r796, %r969, %r969, 9;
	// begin inline asm
	lop3.b32 %r785, %r796, %r792, %r788, 232;
	// end inline asm
	add.s32 	%r970, %r784, %r785;
	add.s32 	%r971, %r970, %r934;
	add.s32 	%r972, %r971, 1518500249;
	shf.l.wrap.b32 	%r800, %r972, %r972, 13;
	// begin inline asm
	lop3.b32 %r789, %r800, %r796, %r792, 232;
	// end inline asm
	add.s32 	%r973, %r788, %r789;
	add.s32 	%r974, %r973, %r887;
	add.s32 	%r975, %r974, 1518500249;
	shf.l.wrap.b32 	%r804, %r975, %r975, 3;
	// begin inline asm
	lop3.b32 %r793, %r804, %r800, %r796, 232;
	// end inline asm
	add.s32 	%r976, %r792, %r793;
	add.s32 	%r977, %r976, %r903;
	add.s32 	%r978, %r977, 1518500249;
	shf.l.wrap.b32 	%r808, %r978, %r978, 5;
	// begin inline asm
	lop3.b32 %r797, %r808, %r804, %r800, 232;
	// end inline asm
	add.s32 	%r979, %r796, %r797;
	add.s32 	%r980, %r979, %r919;
	add.s32 	%r981, %r980, 1518500249;
	shf.l.wrap.b32 	%r812, %r981, %r981, 9;
	// begin inline asm
	lop3.b32 %r801, %r812, %r808, %r804, 232;
	// end inline asm
	add.s32 	%r982, %r800, %r801;
	add.s32 	%r983, %r982, %r935;
	add.s32 	%r984, %r983, 1518500249;
	shf.l.wrap.b32 	%r816, %r984, %r984, 13;
	// begin inline asm
	lop3.b32 %r805, %r816, %r812, %r808, 232;
	// end inline asm
	add.s32 	%r985, %r804, %r805;
	add.s32 	%r986, %r985, %r888;
	add.s32 	%r987, %r986, 1518500249;
	shf.l.wrap.b32 	%r820, %r987, %r987, 3;
	// begin inline asm
	lop3.b32 %r809, %r820, %r816, %r812, 232;
	// end inline asm
	add.s32 	%r988, %r808, %r809;
	add.s32 	%r989, %r988, %r904;
	add.s32 	%r990, %r989, 1518500249;
	shf.l.wrap.b32 	%r824, %r990, %r990, 5;
	// begin inline asm
	lop3.b32 %r813, %r824, %r820, %r816, 232;
	// end inline asm
	add.s32 	%r991, %r812, %r813;
	add.s32 	%r992, %r991, %r920;
	add.s32 	%r993, %r992, 1518500249;
	shf.l.wrap.b32 	%r828, %r993, %r993, 9;
	// begin inline asm
	lop3.b32 %r817, %r828, %r824, %r820, 232;
	// end inline asm
	add.s32 	%r994, %r816, %r817;
	add.s32 	%r995, %r994, %r936;
	add.s32 	%r996, %r995, 1518500249;
	shf.l.wrap.b32 	%r832, %r996, %r996, 13;
	// begin inline asm
	lop3.b32 %r821, %r832, %r828, %r824, 150;
	// end inline asm
	add.s32 	%r997, %r820, %r821;
	add.s32 	%r998, %r997, %r885;
	add.s32 	%r999, %r998, 1859775393;
	shf.l.wrap.b32 	%r836, %r999, %r999, 3;
	// begin inline asm
	lop3.b32 %r825, %r836, %r832, %r828, 150;
	// end inline asm
	add.s32 	%r1000, %r824, %r825;
	add.s32 	%r1001, %r1000, %r917;
	add.s32 	%r1002, %r1001, 1859775393;
	shf.l.wrap.b32 	%r840, %r1002, %r1002, 9;
	// begin inline asm
	lop3.b32 %r829, %r840, %r836, %r832, 150;
	// end inline asm
	add.s32 	%r1003, %r828, %r829;
	add.s32 	%r1004, %r1003, %r901;
	add.s32 	%r1005, %r1004, 1859775393;
	shf.l.wrap.b32 	%r844, %r1005, %r1005, 11;
	// begin inline asm
	lop3.b32 %r833, %r844, %r840, %r836, 150;
	// end inline asm
	add.s32 	%r1006, %r832, %r833;
	add.s32 	%r1007, %r1006, %r933;
	add.s32 	%r1008, %r1007, 1859775393;
	shf.l.wrap.b32 	%r848, %r1008, %r1008, 15;
	// begin inline asm
	lop3.b32 %r837, %r848, %r844, %r840, 150;
	// end inline asm
	add.s32 	%r1009, %r836, %r837;
	add.s32 	%r1010, %r1009, %r887;
	add.s32 	%r1011, %r1010, 1859775393;
	shf.l.wrap.b32 	%r852, %r1011, %r1011, 3;
	// begin inline asm
	lop3.b32 %r841, %r852, %r848, %r844, 150;
	// end inline asm
	add.s32 	%r1012, %r840, %r841;
	add.s32 	%r1013, %r1012, %r919;
	add.s32 	%r1014, %r1013, 1859775393;
	shf.l.wrap.b32 	%r856, %r1014, %r1014, 9;
	// begin inline asm
	lop3.b32 %r845, %r856, %r852, %r848, 150;
	// end inline asm
	add.s32 	%r1015, %r844, %r845;
	add.s32 	%r1016, %r1015, %r903;
	add.s32 	%r1017, %r1016, 1859775393;
	shf.l.wrap.b32 	%r860, %r1017, %r1017, 11;
	// begin inline asm
	lop3.b32 %r849, %r860, %r856, %r852, 150;
	// end inline asm
	add.s32 	%r1018, %r848, %r849;
	add.s32 	%r1019, %r1018, %r935;
	add.s32 	%r1020, %r1019, 1859775393;
	shf.l.wrap.b32 	%r864, %r1020, %r1020, 15;
	// begin inline asm
	lop3.b32 %r853, %r864, %r860, %r856, 150;
	// end inline asm
	add.s32 	%r1021, %r852, %r853;
	add.s32 	%r1022, %r1021, %r886;
	add.s32 	%r1023, %r1022, 1859775393;
	shf.l.wrap.b32 	%r868, %r1023, %r1023, 3;
	// begin inline asm
	lop3.b32 %r857, %r868, %r864, %r860, 150;
	// end inline asm
	add.s32 	%r1024, %r856, %r857;
	add.s32 	%r1025, %r1024, %r918;
	add.s32 	%r1026, %r1025, 1859775393;
	shf.l.wrap.b32 	%r870, %r1026, %r1026, 9;
	// begin inline asm
	lop3.b32 %r861, %r870, %r868, %r864, 150;
	// end inline asm
	add.s32 	%r1027, %r860, %r861;
	add.s32 	%r1028, %r1027, %r902;
	add.s32 	%r1029, %r1028, 1859775393;
	shf.l.wrap.b32 	%r874, %r1029, %r1029, 11;
	// begin inline asm
	lop3.b32 %r865, %r874, %r870, %r868, 150;
	// end inline asm
	add.s32 	%r1030, %r864, %r865;
	add.s32 	%r1031, %r1030, %r934;
	add.s32 	%r1032, %r1031, 1859775393;
	shf.l.wrap.b32 	%r878, %r1032, %r1032, 15;
	// begin inline asm
	lop3.b32 %r869, %r870, %r874, %r878, 150;
	// end inline asm
	add.s32 	%r1033, %r868, %r869;
	add.s32 	%r1034, %r1033, %r888;
	add.s32 	%r1035, %r1034, 1859775393;
	shf.l.wrap.b32 	%r884, %r1035, %r1035, 3;
	// begin inline asm
	lop3.b32 %r873, %r874, %r878, %r884, 150;
	// end inline asm
	add.s32 	%r1036, %r870, %r873;
	add.s32 	%r1037, %r1036, %r920;
	add.s32 	%r1038, %r1037, 1859775393;
	shf.l.wrap.b32 	%r883, %r1038, %r1038, 9;
	// begin inline asm
	lop3.b32 %r877, %r878, %r884, %r883, 150;
	// end inline asm
	add.s32 	%r1039, %r874, %r877;
	add.s32 	%r1040, %r1039, %r904;
	add.s32 	%r1041, %r1040, 1859775393;
	shf.l.wrap.b32 	%r882, %r1041, %r1041, 11;
	// begin inline asm
	lop3.b32 %r881, %r882, %r883, %r884, 150;
	// end inline asm
	add.s32 	%r1042, %r878, %r881;
	add.s32 	%r1043, %r1042, %r936;
	add.s32 	%r1044, %r1043, 1859775393;
	shf.l.wrap.b32 	%r1045, %r1044, %r1044, 15;
	add.s32 	%r1111, %r884, %r1111;
	add.s32 	%r1110, %r1045, %r1110;
	add.s32 	%r1109, %r882, %r1109;
	add.s32 	%r1108, %r883, %r1108;
	add.s32 	%r1103, %r1103, -1;
	setp.ne.s32 	%p38, %r1103, 0;
	mov.u64 	%rd76, %rd8;
	@%p38 bra 	$L__BB0_41;

$L__BB0_42:
	add.s32 	%r1050, %r1110, 271733879;
	shf.l.wrap.b32 	%r1051, %r1050, %r1050, 17;
	add.s32 	%r1113, %r1111, -1732584193;
	add.s32 	%r1047, %r1109, 1732584194;
	add.s32 	%r1048, %r1108, -271733878;
	// begin inline asm
	lop3.b32 %r1046, %r1047, %r1048, %r1113, 150;
	// end inline asm
	add.s32 	%r1052, %r1051, -1859775393;
	sub.s32 	%r1053, %r1052, %r1046;
	shf.l.wrap.b32 	%r1054, %r1053, %r1053, 17;
	add.s32 	%r1112, %r1054, -1859775393;
	bra.uni 	$L__BB0_43;

}

  