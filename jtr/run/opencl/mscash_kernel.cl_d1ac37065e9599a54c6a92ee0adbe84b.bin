//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.6
.target sm_86, texmode_independent
.address_size 64

	// .globl	mscash
.const .align 4 .u32 halfShift = 10;
.const .align 4 .u32 halfBase = 65536;
.const .align 4 .u32 halfMask = 1023;
.const .align 1 .b8 opt_trailingBytesUTF8[64] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5};
.const .align 4 .b8 offsetsFromUTF8[24] = {0, 0, 0, 0, 128, 48, 0, 0, 128, 32, 14, 0, 128, 32, 200, 3, 128, 32, 8, 250, 128, 32, 8, 130};

.entry mscash(
	.param .u64 .ptr .global .align 4 mscash_param_0,
	.param .u64 .ptr .global .align 4 mscash_param_1,
	.param .u64 .ptr .const .align 4 mscash_param_2,
	.param .u64 .ptr .global .align 4 mscash_param_3,
	.param .u64 .ptr .const .align 4 mscash_param_4,
	.param .u64 .ptr .global .align 4 mscash_param_5,
	.param .u64 .ptr .global .align 4 mscash_param_6,
	.param .u64 .ptr .global .align 4 mscash_param_7,
	.param .u64 .ptr .global .align 4 mscash_param_8,
	.param .u64 .ptr .global .align 4 mscash_param_9,
	.param .u64 .ptr .global .align 4 mscash_param_10
)
{
	.local .align 16 .b8 	__local_depot0[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<21>;
	.reg .b32 	%r<834>;
	.reg .b64 	%rd<114>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd38, [mscash_param_0];
	ld.param.u64 	%rd45, [mscash_param_1];
	ld.param.u64 	%rd3, [mscash_param_2];
	ld.param.u64 	%rd39, [mscash_param_5];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r25, %tid.x;
	mov.b32 	%r26, %envreg3;
	add.s32 	%r27, %r25, %r26;
	mad.lo.s32 	%r1, %r24, %r23, %r27;
	mul.wide.u32 	%rd47, %r1, 4;
	add.s64 	%rd48, %rd45, %rd47;
	ld.global.u32 	%r2, [%rd48];
	mov.u32 	%r28, 0;
	st.local.v4.u32 	[%rd1], {%r28, %r28, %r28, %r28};
	st.local.v4.u32 	[%rd1+16], {%r28, %r28, %r28, %r28};
	st.local.v4.u32 	[%rd1+32], {%r28, %r28, %r28, %r28};
	st.local.v4.u32 	[%rd1+48], {%r28, %r28, %r28, %r28};
	and.b32  	%r3, %r2, 127;
	ld.const.u32 	%r4, [%rd3+48];
	shr.u32 	%r29, %r2, 7;
	cvt.u64.u32 	%rd4, %r29;
	add.s32 	%r30, %r3, 3;
	shr.u32 	%r5, %r30, 2;
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB0_7;

	add.s32 	%r33, %r5, -1;
	and.b32  	%r833, %r5, 3;
	setp.lt.u32 	%p2, %r33, 3;
	mov.u32 	%r831, %r28;
	mov.u32 	%r832, %r28;
	@%p2 bra 	$L__BB0_4;

	sub.s32 	%r830, %r5, %r833;
	mov.u32 	%r831, 0;
	mov.u32 	%r832, %r831;

$L__BB0_3:
	cvt.u64.u32 	%rd49, %r832;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd38, %rd51;
	ld.global.u32 	%r36, [%rd52];
	and.b32  	%r37, %r36, 255;
	and.b32  	%r38, %r36, 65280;
	prmt.b32 	%r39, %r38, %r37, 8452;
	mul.wide.u32 	%rd53, %r831, 4;
	add.s64 	%rd54, %rd1, %rd53;
	st.local.u32 	[%rd54], %r39;
	and.b32  	%r40, %r36, 16711680;
	shr.u32 	%r41, %r40, 16;
	shr.u32 	%r42, %r36, 8;
	and.b32  	%r43, %r42, 16711680;
	or.b32  	%r44, %r41, %r43;
	st.local.u32 	[%rd54+4], %r44;
	ld.global.u32 	%r45, [%rd52+4];
	and.b32  	%r46, %r45, 255;
	and.b32  	%r47, %r45, 65280;
	prmt.b32 	%r48, %r47, %r46, 8452;
	st.local.u32 	[%rd54+8], %r48;
	and.b32  	%r49, %r45, 16711680;
	shr.u32 	%r50, %r49, 16;
	shr.u32 	%r51, %r45, 8;
	and.b32  	%r52, %r51, 16711680;
	or.b32  	%r53, %r50, %r52;
	st.local.u32 	[%rd54+12], %r53;
	ld.global.u32 	%r54, [%rd52+8];
	and.b32  	%r55, %r54, 255;
	and.b32  	%r56, %r54, 65280;
	prmt.b32 	%r57, %r56, %r55, 8452;
	st.local.u32 	[%rd54+16], %r57;
	and.b32  	%r58, %r54, 16711680;
	shr.u32 	%r59, %r58, 16;
	shr.u32 	%r60, %r54, 8;
	and.b32  	%r61, %r60, 16711680;
	or.b32  	%r62, %r59, %r61;
	st.local.u32 	[%rd54+20], %r62;
	ld.global.u32 	%r63, [%rd52+12];
	and.b32  	%r64, %r63, 255;
	and.b32  	%r65, %r63, 65280;
	prmt.b32 	%r66, %r65, %r64, 8452;
	st.local.u32 	[%rd54+24], %r66;
	and.b32  	%r67, %r63, 16711680;
	shr.u32 	%r68, %r67, 16;
	shr.u32 	%r69, %r63, 8;
	and.b32  	%r70, %r69, 16711680;
	or.b32  	%r71, %r68, %r70;
	add.s32 	%r831, %r831, 8;
	st.local.u32 	[%rd54+28], %r71;
	add.s32 	%r832, %r832, 4;
	add.s32 	%r830, %r830, -4;
	setp.ne.s32 	%p3, %r830, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s32 	%p4, %r833, 0;
	@%p4 bra 	$L__BB0_7;

	mul.wide.u32 	%rd55, %r831, 4;
	add.s64 	%rd56, %rd1, %rd55;
	add.s64 	%rd107, %rd56, 4;
	cvt.u64.u32 	%rd57, %r832;
	add.s64 	%rd58, %rd4, %rd57;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd106, %rd38, %rd59;

$L__BB0_6:
	.pragma "nounroll";
	ld.global.u32 	%r72, [%rd106];
	and.b32  	%r73, %r72, 255;
	and.b32  	%r74, %r72, 65280;
	prmt.b32 	%r75, %r74, %r73, 8452;
	st.local.u32 	[%rd107+-4], %r75;
	and.b32  	%r76, %r72, 16711680;
	shr.u32 	%r77, %r76, 16;
	shr.u32 	%r78, %r72, 8;
	and.b32  	%r79, %r78, 16711680;
	or.b32  	%r80, %r77, %r79;
	st.local.u32 	[%rd107], %r80;
	add.s64 	%rd107, %rd107, 8;
	add.s64 	%rd106, %rd106, 4;
	add.s32 	%r833, %r833, -1;
	setp.ne.s32 	%p5, %r833, 0;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	shr.u32 	%r461, %r3, 1;
	mul.wide.u32 	%rd60, %r461, 4;
	add.s64 	%rd61, %rd1, %rd60;
	ld.local.u16 	%r462, [%rd61];
	shl.b32 	%r463, %r2, 4;
	and.b32  	%r464, %r463, 16;
	mov.u32 	%r465, 128;
	shl.b32 	%r466, %r465, %r464;
	or.b32  	%r467, %r462, %r466;
	st.local.u32 	[%rd61], %r467;
	st.local.u32 	[%rd61+4], %r28;
	shl.b32 	%r469, %r3, 4;
	st.local.u32 	[%rd1+56], %r469;
	ld.local.v4.u32 	{%r470, %r471, %r472, %r473}, [%rd1];
	mov.u32 	%r280, -271733879;
	mov.u32 	%r276, -1732584194;
	mov.u32 	%r84, 271733878;
	// begin inline asm
	lop3.b32 %r81, %r280, %r276, %r84, 202;
	// end inline asm
	add.s32 	%r478, %r81, %r470;
	add.s32 	%r479, %r478, 1732584193;
	shf.l.wrap.b32 	%r96, %r479, %r479, 3;
	// begin inline asm
	lop3.b32 %r85, %r96, %r280, %r276, 202;
	// end inline asm
	add.s32 	%r480, %r85, %r471;
	add.s32 	%r481, %r480, 271733878;
	shf.l.wrap.b32 	%r100, %r481, %r481, 7;
	// begin inline asm
	lop3.b32 %r89, %r100, %r96, %r280, 202;
	// end inline asm
	add.s32 	%r482, %r89, %r472;
	add.s32 	%r483, %r482, -1732584194;
	shf.l.wrap.b32 	%r104, %r483, %r483, 11;
	// begin inline asm
	lop3.b32 %r93, %r104, %r100, %r96, 202;
	// end inline asm
	add.s32 	%r484, %r93, %r473;
	add.s32 	%r485, %r484, -271733879;
	shf.l.wrap.b32 	%r108, %r485, %r485, 19;
	// begin inline asm
	lop3.b32 %r97, %r108, %r104, %r100, 202;
	// end inline asm
	ld.local.v4.u32 	{%r486, %r487, %r488, %r489}, [%rd1+16];
	add.s32 	%r494, %r97, %r96;
	add.s32 	%r495, %r494, %r486;
	shf.l.wrap.b32 	%r112, %r495, %r495, 3;
	// begin inline asm
	lop3.b32 %r101, %r112, %r108, %r104, 202;
	// end inline asm
	add.s32 	%r496, %r101, %r100;
	add.s32 	%r497, %r496, %r487;
	shf.l.wrap.b32 	%r116, %r497, %r497, 7;
	// begin inline asm
	lop3.b32 %r105, %r116, %r112, %r108, 202;
	// end inline asm
	add.s32 	%r498, %r105, %r104;
	add.s32 	%r499, %r498, %r488;
	shf.l.wrap.b32 	%r120, %r499, %r499, 11;
	// begin inline asm
	lop3.b32 %r109, %r120, %r116, %r112, 202;
	// end inline asm
	add.s32 	%r500, %r109, %r108;
	add.s32 	%r501, %r500, %r489;
	shf.l.wrap.b32 	%r124, %r501, %r501, 19;
	// begin inline asm
	lop3.b32 %r113, %r124, %r120, %r116, 202;
	// end inline asm
	ld.local.v4.u32 	{%r502, %r503, %r504, %r505}, [%rd1+32];
	add.s32 	%r510, %r113, %r112;
	add.s32 	%r511, %r510, %r502;
	shf.l.wrap.b32 	%r128, %r511, %r511, 3;
	// begin inline asm
	lop3.b32 %r117, %r128, %r124, %r120, 202;
	// end inline asm
	add.s32 	%r512, %r117, %r116;
	add.s32 	%r513, %r512, %r503;
	shf.l.wrap.b32 	%r132, %r513, %r513, 7;
	// begin inline asm
	lop3.b32 %r121, %r132, %r128, %r124, 202;
	// end inline asm
	add.s32 	%r514, %r121, %r120;
	add.s32 	%r515, %r514, %r504;
	shf.l.wrap.b32 	%r136, %r515, %r515, 11;
	// begin inline asm
	lop3.b32 %r125, %r136, %r132, %r128, 202;
	// end inline asm
	add.s32 	%r516, %r125, %r124;
	add.s32 	%r517, %r516, %r505;
	shf.l.wrap.b32 	%r140, %r517, %r517, 19;
	// begin inline asm
	lop3.b32 %r129, %r140, %r136, %r132, 202;
	// end inline asm
	.pragma "used_bytes_mask 61695";
	ld.local.v4.u32 	{%r518, %r519, %r520, %r521}, [%rd1+48];
	add.s32 	%r525, %r129, %r128;
	add.s32 	%r526, %r525, %r518;
	shf.l.wrap.b32 	%r144, %r526, %r526, 3;
	// begin inline asm
	lop3.b32 %r133, %r144, %r140, %r136, 202;
	// end inline asm
	add.s32 	%r527, %r133, %r132;
	add.s32 	%r528, %r527, %r519;
	shf.l.wrap.b32 	%r148, %r528, %r528, 7;
	// begin inline asm
	lop3.b32 %r137, %r148, %r144, %r140, 202;
	// end inline asm
	add.s32 	%r529, %r137, %r136;
	add.s32 	%r530, %r529, %r469;
	shf.l.wrap.b32 	%r152, %r530, %r530, 11;
	// begin inline asm
	lop3.b32 %r141, %r152, %r148, %r144, 202;
	// end inline asm
	add.s32 	%r531, %r141, %r140;
	add.s32 	%r532, %r531, %r521;
	shf.l.wrap.b32 	%r156, %r532, %r532, 19;
	// begin inline asm
	lop3.b32 %r145, %r156, %r152, %r148, 232;
	// end inline asm
	add.s32 	%r533, %r144, %r145;
	add.s32 	%r534, %r533, %r470;
	add.s32 	%r535, %r534, 1518500249;
	shf.l.wrap.b32 	%r160, %r535, %r535, 3;
	// begin inline asm
	lop3.b32 %r149, %r160, %r156, %r152, 232;
	// end inline asm
	add.s32 	%r536, %r148, %r149;
	add.s32 	%r537, %r536, %r486;
	add.s32 	%r538, %r537, 1518500249;
	shf.l.wrap.b32 	%r164, %r538, %r538, 5;
	// begin inline asm
	lop3.b32 %r153, %r164, %r160, %r156, 232;
	// end inline asm
	add.s32 	%r539, %r152, %r153;
	add.s32 	%r540, %r539, %r502;
	add.s32 	%r541, %r540, 1518500249;
	shf.l.wrap.b32 	%r168, %r541, %r541, 9;
	// begin inline asm
	lop3.b32 %r157, %r168, %r164, %r160, 232;
	// end inline asm
	add.s32 	%r542, %r156, %r157;
	add.s32 	%r543, %r542, %r518;
	add.s32 	%r544, %r543, 1518500249;
	shf.l.wrap.b32 	%r172, %r544, %r544, 13;
	// begin inline asm
	lop3.b32 %r161, %r172, %r168, %r164, 232;
	// end inline asm
	add.s32 	%r545, %r160, %r161;
	add.s32 	%r546, %r545, %r471;
	add.s32 	%r547, %r546, 1518500249;
	shf.l.wrap.b32 	%r176, %r547, %r547, 3;
	// begin inline asm
	lop3.b32 %r165, %r176, %r172, %r168, 232;
	// end inline asm
	add.s32 	%r548, %r164, %r165;
	add.s32 	%r549, %r548, %r487;
	add.s32 	%r550, %r549, 1518500249;
	shf.l.wrap.b32 	%r180, %r550, %r550, 5;
	// begin inline asm
	lop3.b32 %r169, %r180, %r176, %r172, 232;
	// end inline asm
	add.s32 	%r551, %r168, %r169;
	add.s32 	%r552, %r551, %r503;
	add.s32 	%r553, %r552, 1518500249;
	shf.l.wrap.b32 	%r184, %r553, %r553, 9;
	// begin inline asm
	lop3.b32 %r173, %r184, %r180, %r176, 232;
	// end inline asm
	add.s32 	%r554, %r172, %r173;
	add.s32 	%r555, %r554, %r519;
	add.s32 	%r556, %r555, 1518500249;
	shf.l.wrap.b32 	%r188, %r556, %r556, 13;
	// begin inline asm
	lop3.b32 %r177, %r188, %r184, %r180, 232;
	// end inline asm
	add.s32 	%r557, %r176, %r177;
	add.s32 	%r558, %r557, %r472;
	add.s32 	%r559, %r558, 1518500249;
	shf.l.wrap.b32 	%r192, %r559, %r559, 3;
	// begin inline asm
	lop3.b32 %r181, %r192, %r188, %r184, 232;
	// end inline asm
	add.s32 	%r560, %r180, %r181;
	add.s32 	%r561, %r560, %r488;
	add.s32 	%r562, %r561, 1518500249;
	shf.l.wrap.b32 	%r196, %r562, %r562, 5;
	// begin inline asm
	lop3.b32 %r185, %r196, %r192, %r188, 232;
	// end inline asm
	add.s32 	%r563, %r184, %r185;
	add.s32 	%r564, %r563, %r504;
	add.s32 	%r565, %r564, 1518500249;
	shf.l.wrap.b32 	%r200, %r565, %r565, 9;
	// begin inline asm
	lop3.b32 %r189, %r200, %r196, %r192, 232;
	// end inline asm
	add.s32 	%r566, %r188, %r189;
	add.s32 	%r567, %r566, %r469;
	add.s32 	%r568, %r567, 1518500249;
	shf.l.wrap.b32 	%r204, %r568, %r568, 13;
	// begin inline asm
	lop3.b32 %r193, %r204, %r200, %r196, 232;
	// end inline asm
	add.s32 	%r569, %r192, %r193;
	add.s32 	%r570, %r569, %r473;
	add.s32 	%r571, %r570, 1518500249;
	shf.l.wrap.b32 	%r208, %r571, %r571, 3;
	// begin inline asm
	lop3.b32 %r197, %r208, %r204, %r200, 232;
	// end inline asm
	add.s32 	%r572, %r196, %r197;
	add.s32 	%r573, %r572, %r489;
	add.s32 	%r574, %r573, 1518500249;
	shf.l.wrap.b32 	%r212, %r574, %r574, 5;
	// begin inline asm
	lop3.b32 %r201, %r212, %r208, %r204, 232;
	// end inline asm
	add.s32 	%r575, %r200, %r201;
	add.s32 	%r576, %r575, %r505;
	add.s32 	%r577, %r576, 1518500249;
	shf.l.wrap.b32 	%r216, %r577, %r577, 9;
	// begin inline asm
	lop3.b32 %r205, %r216, %r212, %r208, 232;
	// end inline asm
	add.s32 	%r578, %r204, %r205;
	add.s32 	%r579, %r578, %r521;
	add.s32 	%r580, %r579, 1518500249;
	shf.l.wrap.b32 	%r220, %r580, %r580, 13;
	// begin inline asm
	lop3.b32 %r209, %r220, %r216, %r212, 150;
	// end inline asm
	add.s32 	%r581, %r208, %r209;
	add.s32 	%r582, %r581, %r470;
	add.s32 	%r583, %r582, 1859775393;
	shf.l.wrap.b32 	%r224, %r583, %r583, 3;
	// begin inline asm
	lop3.b32 %r213, %r224, %r220, %r216, 150;
	// end inline asm
	add.s32 	%r584, %r212, %r213;
	add.s32 	%r585, %r584, %r502;
	add.s32 	%r586, %r585, 1859775393;
	shf.l.wrap.b32 	%r228, %r586, %r586, 9;
	// begin inline asm
	lop3.b32 %r217, %r228, %r224, %r220, 150;
	// end inline asm
	add.s32 	%r587, %r216, %r217;
	add.s32 	%r588, %r587, %r486;
	add.s32 	%r589, %r588, 1859775393;
	shf.l.wrap.b32 	%r232, %r589, %r589, 11;
	// begin inline asm
	lop3.b32 %r221, %r232, %r228, %r224, 150;
	// end inline asm
	add.s32 	%r590, %r220, %r221;
	add.s32 	%r591, %r590, %r518;
	add.s32 	%r592, %r591, 1859775393;
	shf.l.wrap.b32 	%r236, %r592, %r592, 15;
	// begin inline asm
	lop3.b32 %r225, %r236, %r232, %r228, 150;
	// end inline asm
	add.s32 	%r593, %r224, %r225;
	add.s32 	%r594, %r593, %r472;
	add.s32 	%r595, %r594, 1859775393;
	shf.l.wrap.b32 	%r240, %r595, %r595, 3;
	// begin inline asm
	lop3.b32 %r229, %r240, %r236, %r232, 150;
	// end inline asm
	add.s32 	%r596, %r228, %r229;
	add.s32 	%r597, %r596, %r504;
	add.s32 	%r598, %r597, 1859775393;
	shf.l.wrap.b32 	%r244, %r598, %r598, 9;
	// begin inline asm
	lop3.b32 %r233, %r244, %r240, %r236, 150;
	// end inline asm
	add.s32 	%r599, %r232, %r233;
	add.s32 	%r600, %r599, %r488;
	add.s32 	%r601, %r600, 1859775393;
	shf.l.wrap.b32 	%r248, %r601, %r601, 11;
	// begin inline asm
	lop3.b32 %r237, %r248, %r244, %r240, 150;
	// end inline asm
	add.s32 	%r602, %r236, %r237;
	add.s32 	%r603, %r602, %r469;
	add.s32 	%r604, %r603, 1859775393;
	shf.l.wrap.b32 	%r252, %r604, %r604, 15;
	// begin inline asm
	lop3.b32 %r241, %r252, %r248, %r244, 150;
	// end inline asm
	add.s32 	%r605, %r240, %r241;
	add.s32 	%r606, %r605, %r471;
	add.s32 	%r607, %r606, 1859775393;
	shf.l.wrap.b32 	%r256, %r607, %r607, 3;
	// begin inline asm
	lop3.b32 %r245, %r256, %r252, %r248, 150;
	// end inline asm
	add.s32 	%r608, %r244, %r245;
	add.s32 	%r609, %r608, %r503;
	add.s32 	%r610, %r609, 1859775393;
	shf.l.wrap.b32 	%r260, %r610, %r610, 9;
	// begin inline asm
	lop3.b32 %r249, %r260, %r256, %r252, 150;
	// end inline asm
	add.s32 	%r611, %r248, %r249;
	add.s32 	%r612, %r611, %r487;
	add.s32 	%r613, %r612, 1859775393;
	shf.l.wrap.b32 	%r264, %r613, %r613, 11;
	// begin inline asm
	lop3.b32 %r253, %r264, %r260, %r256, 150;
	// end inline asm
	add.s32 	%r614, %r252, %r253;
	add.s32 	%r615, %r614, %r519;
	add.s32 	%r616, %r615, 1859775393;
	shf.l.wrap.b32 	%r268, %r616, %r616, 15;
	// begin inline asm
	lop3.b32 %r257, %r268, %r264, %r260, 150;
	// end inline asm
	add.s32 	%r617, %r256, %r257;
	add.s32 	%r618, %r617, %r473;
	add.s32 	%r619, %r618, 1859775393;
	shf.l.wrap.b32 	%r272, %r619, %r619, 3;
	// begin inline asm
	lop3.b32 %r261, %r272, %r268, %r264, 150;
	// end inline asm
	add.s32 	%r620, %r260, %r261;
	add.s32 	%r621, %r620, %r505;
	add.s32 	%r622, %r621, 1859775393;
	shf.l.wrap.b32 	%r271, %r622, %r622, 9;
	// begin inline asm
	lop3.b32 %r265, %r271, %r272, %r268, 150;
	// end inline asm
	add.s32 	%r623, %r264, %r265;
	add.s32 	%r624, %r623, %r489;
	add.s32 	%r625, %r624, 1859775393;
	shf.l.wrap.b32 	%r270, %r625, %r625, 11;
	// begin inline asm
	lop3.b32 %r269, %r270, %r271, %r272, 150;
	// end inline asm
	add.s32 	%r626, %r268, %r269;
	add.s32 	%r627, %r626, %r521;
	add.s32 	%r628, %r627, 1859775393;
	shf.l.wrap.b32 	%r629, %r628, %r628, 15;
	add.s32 	%r630, %r81, %r272;
	add.s32 	%r631, %r630, -829798910;
	shf.l.wrap.b32 	%r284, %r631, %r631, 3;
	// begin inline asm
	lop3.b32 %r273, %r284, %r280, %r276, 202;
	// end inline asm
	add.s32 	%r632, %r273, %r629;
	add.s32 	%r633, %r632, -1;
	shf.l.wrap.b32 	%r288, %r633, %r633, 7;
	// begin inline asm
	lop3.b32 %r277, %r288, %r284, %r280, 202;
	// end inline asm
	add.s32 	%r634, %r277, %r270;
	add.s32 	%r635, %r634, 829798908;
	shf.l.wrap.b32 	%r292, %r635, %r635, 11;
	// begin inline asm
	lop3.b32 %r281, %r292, %r288, %r284, 202;
	// end inline asm
	add.s32 	%r636, %r271, %r281;
	add.s32 	%r637, %r636, -1;
	shf.l.wrap.b32 	%r296, %r637, %r637, 19;
	// begin inline asm
	lop3.b32 %r285, %r296, %r292, %r288, 202;
	// end inline asm
	add.s32 	%r638, %r285, %r284;
	ld.const.u32 	%r639, [%rd3];
	add.s32 	%r640, %r638, %r639;
	shf.l.wrap.b32 	%r300, %r640, %r640, 3;
	// begin inline asm
	lop3.b32 %r289, %r300, %r296, %r292, 202;
	// end inline asm
	ld.const.u32 	%r641, [%rd3+4];
	add.s32 	%r642, %r641, %r289;
	add.s32 	%r643, %r642, %r288;
	shf.l.wrap.b32 	%r304, %r643, %r643, 7;
	// begin inline asm
	lop3.b32 %r293, %r304, %r300, %r296, 202;
	// end inline asm
	add.s32 	%r644, %r293, %r292;
	ld.const.u32 	%r645, [%rd3+8];
	add.s32 	%r646, %r644, %r645;
	shf.l.wrap.b32 	%r308, %r646, %r646, 11;
	// begin inline asm
	lop3.b32 %r297, %r308, %r304, %r300, 202;
	// end inline asm
	add.s32 	%r647, %r297, %r296;
	ld.const.u32 	%r648, [%rd3+12];
	add.s32 	%r649, %r647, %r648;
	shf.l.wrap.b32 	%r312, %r649, %r649, 19;
	// begin inline asm
	lop3.b32 %r301, %r312, %r308, %r304, 202;
	// end inline asm
	add.s32 	%r650, %r301, %r300;
	ld.const.u32 	%r651, [%rd3+16];
	add.s32 	%r652, %r650, %r651;
	shf.l.wrap.b32 	%r316, %r652, %r652, 3;
	// begin inline asm
	lop3.b32 %r305, %r316, %r312, %r308, 202;
	// end inline asm
	add.s32 	%r653, %r305, %r304;
	ld.const.u32 	%r654, [%rd3+20];
	add.s32 	%r655, %r653, %r654;
	shf.l.wrap.b32 	%r320, %r655, %r655, 7;
	// begin inline asm
	lop3.b32 %r309, %r320, %r316, %r312, 202;
	// end inline asm
	add.s32 	%r656, %r309, %r308;
	ld.const.u32 	%r657, [%rd3+24];
	add.s32 	%r658, %r656, %r657;
	shf.l.wrap.b32 	%r324, %r658, %r658, 11;
	// begin inline asm
	lop3.b32 %r313, %r324, %r320, %r316, 202;
	// end inline asm
	add.s32 	%r659, %r313, %r312;
	ld.const.u32 	%r660, [%rd3+28];
	add.s32 	%r661, %r659, %r660;
	shf.l.wrap.b32 	%r328, %r661, %r661, 19;
	// begin inline asm
	lop3.b32 %r317, %r328, %r324, %r320, 202;
	// end inline asm
	add.s32 	%r662, %r317, %r316;
	ld.const.u32 	%r663, [%rd3+32];
	add.s32 	%r664, %r662, %r663;
	shf.l.wrap.b32 	%r332, %r664, %r664, 3;
	// begin inline asm
	lop3.b32 %r321, %r332, %r328, %r324, 202;
	// end inline asm
	add.s32 	%r665, %r321, %r320;
	ld.const.u32 	%r666, [%rd3+36];
	add.s32 	%r667, %r665, %r666;
	shf.l.wrap.b32 	%r336, %r667, %r667, 7;
	// begin inline asm
	lop3.b32 %r325, %r336, %r332, %r328, 202;
	// end inline asm
	add.s32 	%r668, %r325, %r324;
	ld.const.u32 	%r669, [%rd3+40];
	add.s32 	%r670, %r668, %r669;
	shf.l.wrap.b32 	%r340, %r670, %r670, 11;
	// begin inline asm
	lop3.b32 %r329, %r340, %r336, %r332, 202;
	// end inline asm
	add.s32 	%r671, %r329, %r328;
	ld.const.u32 	%r672, [%rd3+44];
	add.s32 	%r673, %r671, %r672;
	shf.l.wrap.b32 	%r344, %r673, %r673, 19;
	// begin inline asm
	lop3.b32 %r333, %r344, %r340, %r336, 232;
	// end inline asm
	add.s32 	%r674, %r272, %r332;
	add.s32 	%r675, %r674, %r333;
	add.s32 	%r676, %r675, -1043882854;
	shf.l.wrap.b32 	%r348, %r676, %r676, 3;
	// begin inline asm
	lop3.b32 %r337, %r348, %r344, %r340, 232;
	// end inline asm
	add.s32 	%r677, %r336, %r337;
	add.s32 	%r678, %r677, %r639;
	add.s32 	%r679, %r678, 1518500249;
	shf.l.wrap.b32 	%r352, %r679, %r679, 5;
	// begin inline asm
	lop3.b32 %r341, %r352, %r348, %r344, 232;
	// end inline asm
	add.s32 	%r680, %r340, %r341;
	add.s32 	%r681, %r680, %r651;
	add.s32 	%r682, %r681, 1518500249;
	shf.l.wrap.b32 	%r356, %r682, %r682, 9;
	// begin inline asm
	lop3.b32 %r345, %r356, %r352, %r348, 232;
	// end inline asm
	add.s32 	%r683, %r344, %r345;
	add.s32 	%r684, %r683, %r663;
	add.s32 	%r685, %r684, 1518500249;
	shf.l.wrap.b32 	%r360, %r685, %r685, 13;
	// begin inline asm
	lop3.b32 %r349, %r360, %r356, %r352, 232;
	// end inline asm
	add.s32 	%r686, %r629, %r348;
	add.s32 	%r687, %r686, %r349;
	add.s32 	%r688, %r687, 1246766370;
	shf.l.wrap.b32 	%r364, %r688, %r688, 3;
	// begin inline asm
	lop3.b32 %r353, %r364, %r360, %r356, 232;
	// end inline asm
	add.s32 	%r689, %r352, %r353;
	add.s32 	%r690, %r689, %r641;
	add.s32 	%r691, %r690, 1518500249;
	shf.l.wrap.b32 	%r368, %r691, %r691, 5;
	// begin inline asm
	lop3.b32 %r357, %r368, %r364, %r360, 232;
	// end inline asm
	add.s32 	%r692, %r356, %r357;
	add.s32 	%r693, %r692, %r654;
	add.s32 	%r694, %r693, 1518500249;
	shf.l.wrap.b32 	%r372, %r694, %r694, 9;
	// begin inline asm
	lop3.b32 %r361, %r372, %r368, %r364, 232;
	// end inline asm
	add.s32 	%r695, %r360, %r361;
	add.s32 	%r696, %r695, %r666;
	add.s32 	%r697, %r696, 1518500249;
	shf.l.wrap.b32 	%r376, %r697, %r697, 13;
	// begin inline asm
	lop3.b32 %r365, %r376, %r372, %r368, 232;
	// end inline asm
	add.s32 	%r698, %r270, %r364;
	add.s32 	%r699, %r698, %r365;
	add.s32 	%r700, %r699, -214083945;
	shf.l.wrap.b32 	%r380, %r700, %r700, 3;
	// begin inline asm
	lop3.b32 %r369, %r380, %r376, %r372, 232;
	// end inline asm
	add.s32 	%r701, %r368, %r369;
	add.s32 	%r702, %r701, %r645;
	add.s32 	%r703, %r702, 1518500249;
	shf.l.wrap.b32 	%r384, %r703, %r703, 5;
	// begin inline asm
	lop3.b32 %r373, %r384, %r380, %r376, 232;
	// end inline asm
	add.s32 	%r704, %r372, %r373;
	add.s32 	%r705, %r704, %r657;
	add.s32 	%r706, %r705, 1518500249;
	shf.l.wrap.b32 	%r388, %r706, %r706, 9;
	// begin inline asm
	lop3.b32 %r377, %r388, %r384, %r380, 232;
	// end inline asm
	add.s32 	%r707, %r376, %r377;
	add.s32 	%r708, %r707, %r669;
	add.s32 	%r709, %r708, 1518500249;
	shf.l.wrap.b32 	%r392, %r709, %r709, 13;
	// begin inline asm
	lop3.b32 %r381, %r392, %r388, %r384, 232;
	// end inline asm
	add.s32 	%r710, %r271, %r380;
	add.s32 	%r711, %r710, %r381;
	add.s32 	%r712, %r711, 1790234127;
	shf.l.wrap.b32 	%r396, %r712, %r712, 3;
	// begin inline asm
	lop3.b32 %r385, %r396, %r392, %r388, 232;
	// end inline asm
	add.s32 	%r713, %r384, %r385;
	add.s32 	%r714, %r713, %r648;
	add.s32 	%r715, %r714, 1518500249;
	shf.l.wrap.b32 	%r400, %r715, %r715, 5;
	// begin inline asm
	lop3.b32 %r389, %r400, %r396, %r392, 232;
	// end inline asm
	add.s32 	%r716, %r388, %r389;
	add.s32 	%r717, %r716, %r660;
	add.s32 	%r718, %r717, 1518500249;
	shf.l.wrap.b32 	%r404, %r718, %r718, 9;
	// begin inline asm
	lop3.b32 %r393, %r404, %r400, %r396, 232;
	// end inline asm
	add.s32 	%r719, %r392, %r393;
	add.s32 	%r720, %r719, %r672;
	add.s32 	%r721, %r720, 1518500249;
	shf.l.wrap.b32 	%r408, %r721, %r721, 13;
	// begin inline asm
	lop3.b32 %r397, %r408, %r404, %r400, 150;
	// end inline asm
	add.s32 	%r722, %r272, %r396;
	add.s32 	%r723, %r722, %r397;
	add.s32 	%r724, %r723, -702607710;
	shf.l.wrap.b32 	%r412, %r724, %r724, 3;
	// begin inline asm
	lop3.b32 %r401, %r412, %r408, %r404, 150;
	// end inline asm
	add.s32 	%r725, %r400, %r401;
	add.s32 	%r726, %r725, %r651;
	add.s32 	%r727, %r726, 1859775393;
	shf.l.wrap.b32 	%r416, %r727, %r727, 9;
	// begin inline asm
	lop3.b32 %r405, %r416, %r412, %r408, 150;
	// end inline asm
	add.s32 	%r728, %r404, %r405;
	add.s32 	%r729, %r728, %r639;
	add.s32 	%r730, %r729, 1859775393;
	shf.l.wrap.b32 	%r420, %r730, %r730, 11;
	// begin inline asm
	lop3.b32 %r409, %r420, %r416, %r412, 150;
	// end inline asm
	add.s32 	%r731, %r408, %r409;
	add.s32 	%r732, %r731, %r663;
	add.s32 	%r733, %r732, 1859775393;
	shf.l.wrap.b32 	%r424, %r733, %r733, 15;
	// begin inline asm
	lop3.b32 %r413, %r424, %r420, %r416, 150;
	// end inline asm
	add.s32 	%r734, %r270, %r412;
	add.s32 	%r735, %r734, %r413;
	add.s32 	%r736, %r735, 127191199;
	shf.l.wrap.b32 	%r428, %r736, %r736, 3;
	// begin inline asm
	lop3.b32 %r417, %r428, %r424, %r420, 150;
	// end inline asm
	add.s32 	%r737, %r416, %r417;
	add.s32 	%r738, %r737, %r657;
	add.s32 	%r739, %r738, 1859775393;
	shf.l.wrap.b32 	%r432, %r739, %r739, 9;
	// begin inline asm
	lop3.b32 %r421, %r432, %r428, %r424, 150;
	// end inline asm
	add.s32 	%r740, %r420, %r421;
	add.s32 	%r741, %r740, %r645;
	add.s32 	%r742, %r741, 1859775393;
	shf.l.wrap.b32 	%r436, %r742, %r742, 11;
	// begin inline asm
	lop3.b32 %r425, %r436, %r432, %r428, 150;
	// end inline asm
	add.s32 	%r743, %r424, %r425;
	add.s32 	%r744, %r743, %r669;
	add.s32 	%r745, %r744, 1859775393;
	shf.l.wrap.b32 	%r440, %r745, %r745, 15;
	// begin inline asm
	lop3.b32 %r429, %r440, %r436, %r432, 150;
	// end inline asm
	add.s32 	%r746, %r629, %r428;
	add.s32 	%r747, %r746, %r429;
	add.s32 	%r748, %r747, 1588041514;
	shf.l.wrap.b32 	%r444, %r748, %r748, 3;
	// begin inline asm
	lop3.b32 %r433, %r444, %r440, %r436, 150;
	// end inline asm
	add.s32 	%r749, %r432, %r433;
	add.s32 	%r750, %r749, %r654;
	add.s32 	%r751, %r750, 1859775393;
	shf.l.wrap.b32 	%r448, %r751, %r751, 9;
	// begin inline asm
	lop3.b32 %r437, %r448, %r444, %r440, 150;
	// end inline asm
	add.s32 	%r752, %r436, %r437;
	add.s32 	%r753, %r752, %r641;
	add.s32 	%r754, %r753, 1859775393;
	shf.l.wrap.b32 	%r452, %r754, %r754, 11;
	// begin inline asm
	lop3.b32 %r441, %r452, %r448, %r444, 150;
	// end inline asm
	add.s32 	%r755, %r440, %r441;
	add.s32 	%r756, %r755, %r666;
	add.s32 	%r757, %r756, 1859775393;
	shf.l.wrap.b32 	%r456, %r757, %r757, 15;
	// begin inline asm
	lop3.b32 %r445, %r456, %r452, %r448, 150;
	// end inline asm
	add.s32 	%r758, %r271, %r444;
	add.s32 	%r759, %r758, %r445;
	add.s32 	%r760, %r759, 2131509271;
	shf.l.wrap.b32 	%r460, %r760, %r760, 3;
	// begin inline asm
	lop3.b32 %r449, %r460, %r456, %r452, 150;
	// end inline asm
	add.s32 	%r761, %r448, %r449;
	add.s32 	%r762, %r761, %r660;
	add.s32 	%r763, %r762, 1859775393;
	shf.l.wrap.b32 	%r459, %r763, %r763, 9;
	// begin inline asm
	lop3.b32 %r453, %r459, %r460, %r456, 150;
	// end inline asm
	add.s32 	%r764, %r452, %r453;
	add.s32 	%r765, %r764, %r648;
	add.s32 	%r766, %r765, 1859775393;
	shf.l.wrap.b32 	%r458, %r766, %r766, 11;
	// begin inline asm
	lop3.b32 %r457, %r458, %r459, %r460, 150;
	// end inline asm
	add.s32 	%r767, %r456, %r457;
	add.s32 	%r768, %r767, %r672;
	add.s32 	%r769, %r768, 1859775393;
	shf.l.wrap.b32 	%r770, %r769, %r769, 15;
	add.s32 	%r18, %r460, 1732584193;
	add.s32 	%r19, %r770, -271733879;
	add.s32 	%r20, %r458, -1732584194;
	add.s32 	%r21, %r459, 271733878;
	and.b32  	%r771, %r21, %r4;
	shr.u32 	%r772, %r771, 5;
	mul.wide.u32 	%rd62, %r772, 4;
	add.s64 	%rd63, %rd39, %rd62;
	and.b32  	%r773, %r771, 31;
	ld.global.u32 	%r774, [%rd63];
	shr.u32 	%r775, %r774, %r773;
	and.b32  	%r776, %r20, %r4;
	shr.u32 	%r777, %r776, 5;
	add.s32 	%r778, %r4, 1;
	shr.u32 	%r779, %r778, 5;
	add.s32 	%r780, %r777, %r779;
	mul.wide.u32 	%rd64, %r780, 4;
	add.s64 	%rd65, %rd39, %rd64;
	and.b32  	%r781, %r776, 31;
	ld.global.u32 	%r782, [%rd65];
	shr.u32 	%r783, %r782, %r781;
	and.b32  	%r784, %r783, 1;
	setp.eq.b32 	%p6, %r784, 1;
	and.b32  	%r785, %r775, 1;
	setp.eq.b32 	%p7, %r785, 1;
	and.pred  	%p8, %p7, %p6;
	mov.pred 	%p9, 0;
	xor.pred  	%p10, %p8, %p9;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB0_30;

	ld.param.u64 	%rd102, [mscash_param_2];
	cvt.u64.u32 	%rd66, %r21;
	cvt.u64.u32 	%rd67, %r20;
	bfi.b64 	%rd11, %rd66, %rd67, 32, 32;
	cvt.u64.u32 	%rd68, %r19;
	cvt.u64.u32 	%rd69, %r18;
	bfi.b64 	%rd12, %rd68, %rd69, 32, 32;
	ld.const.u32 	%rd13, [%rd102+52];
	and.b64  	%rd70, %rd11, -4294967296;
	setp.eq.s64 	%p12, %rd70, 0;
	@%p12 bra 	$L__BB0_10;

	rem.u64 	%rd108, %rd11, %rd13;
	bra.uni 	$L__BB0_11;

$L__BB0_10:
	cvt.u32.u64 	%r786, %rd13;
	cvt.u32.u64 	%r787, %rd11;
	rem.u32 	%r788, %r787, %r786;
	cvt.u64.u32 	%rd108, %r788;

$L__BB0_11:
	ld.param.u64 	%rd103, [mscash_param_2];
	ld.const.u32 	%rd71, [%rd103+60];
	mul.lo.s64 	%rd17, %rd108, %rd71;
	and.b64  	%rd72, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd72, 0;
	@%p13 bra 	$L__BB0_13;

	rem.u64 	%rd109, %rd12, %rd13;
	bra.uni 	$L__BB0_14;

$L__BB0_13:
	cvt.u32.u64 	%r789, %rd13;
	cvt.u32.u64 	%r790, %rd12;
	rem.u32 	%r791, %r790, %r789;
	cvt.u64.u32 	%rd109, %r791;

$L__BB0_14:
	add.s64 	%rd21, %rd17, %rd109;
	and.b64  	%rd73, %rd21, -4294967296;
	setp.eq.s64 	%p14, %rd73, 0;
	@%p14 bra 	$L__BB0_16;

	rem.u64 	%rd110, %rd21, %rd13;
	bra.uni 	$L__BB0_17;

$L__BB0_16:
	cvt.u32.u64 	%r792, %rd13;
	cvt.u32.u64 	%r793, %rd21;
	rem.u32 	%r794, %r793, %r792;
	cvt.u64.u32 	%rd110, %r794;

$L__BB0_17:
	ld.param.u64 	%rd104, [mscash_param_2];
	ld.param.u64 	%rd96, [mscash_param_6];
	shl.b64 	%rd74, %rd110, 2;
	add.s64 	%rd75, %rd96, %rd74;
	ld.global.u32 	%rd76, [%rd75];
	add.s64 	%rd25, %rd12, %rd76;
	ld.const.u32 	%rd26, [%rd104+56];
	@%p12 bra 	$L__BB0_19;

	rem.u64 	%rd111, %rd11, %rd26;
	bra.uni 	$L__BB0_20;

$L__BB0_19:
	cvt.u32.u64 	%r795, %rd26;
	cvt.u32.u64 	%r796, %rd11;
	rem.u32 	%r797, %r796, %r795;
	cvt.u64.u32 	%rd111, %r797;

$L__BB0_20:
	ld.param.u64 	%rd105, [mscash_param_2];
	ld.const.u32 	%rd78, [%rd105+64];
	mul.lo.s64 	%rd30, %rd111, %rd78;
	and.b64  	%rd79, %rd25, -4294967296;
	setp.eq.s64 	%p16, %rd79, 0;
	@%p16 bra 	$L__BB0_22;

	rem.u64 	%rd112, %rd25, %rd26;
	bra.uni 	$L__BB0_23;

$L__BB0_22:
	cvt.u32.u64 	%r798, %rd26;
	cvt.u32.u64 	%r799, %rd25;
	rem.u32 	%r800, %r799, %r798;
	cvt.u64.u32 	%rd112, %r800;

$L__BB0_23:
	add.s64 	%rd34, %rd30, %rd112;
	and.b64  	%rd80, %rd34, -4294967296;
	setp.eq.s64 	%p17, %rd80, 0;
	@%p17 bra 	$L__BB0_25;

	rem.u64 	%rd113, %rd34, %rd26;
	bra.uni 	$L__BB0_26;

$L__BB0_25:
	cvt.u32.u64 	%r801, %rd26;
	cvt.u32.u64 	%r802, %rd34;
	rem.u32 	%r803, %r802, %r801;
	cvt.u64.u32 	%rd113, %r803;

$L__BB0_26:
	ld.param.u64 	%rd100, [mscash_param_7];
	cvt.u32.u64 	%r22, %rd113;
	shl.b64 	%rd81, %rd113, 2;
	add.s64 	%rd82, %rd100, %rd81;
	ld.global.u32 	%r804, [%rd82];
	setp.ne.s32 	%p18, %r804, %r18;
	@%p18 bra 	$L__BB0_30;

	ld.param.u64 	%rd101, [mscash_param_7];
	cvt.u32.u64 	%r805, %rd26;
	add.s32 	%r806, %r805, %r22;
	mul.wide.u32 	%rd83, %r806, 4;
	add.s64 	%rd84, %rd101, %rd83;
	ld.global.u32 	%r807, [%rd84];
	setp.ne.s32 	%p19, %r807, %r19;
	@%p19 bra 	$L__BB0_30;

	ld.param.u64 	%rd97, [mscash_param_10];
	shr.u64 	%rd85, %rd113, 3;
	and.b64  	%rd86, %rd85, 536870908;
	add.s64 	%rd87, %rd97, %rd86;
	and.b32  	%r808, %r22, 31;
	mov.u32 	%r809, 1;
	shl.b32 	%r810, %r809, %r808;
	atom.global.or.b32 	%r811, [%rd87], %r810;
	and.b32  	%r812, %r811, %r810;
	setp.ne.s32 	%p20, %r812, 0;
	@%p20 bra 	$L__BB0_30;

	mov.b32 	%r827, %envreg3;
	mov.u32 	%r826, %tid.x;
	add.s32 	%r825, %r826, %r827;
	mov.u32 	%r824, %ctaid.x;
	mov.u32 	%r823, %ntid.x;
	mad.lo.s32 	%r822, %r823, %r824, %r825;
	ld.param.u64 	%rd99, [mscash_param_8];
	ld.param.u64 	%rd98, [mscash_param_9];
	atom.global.add.u32 	%r813, [%rd98], 1;
	mul.lo.s32 	%r814, %r813, 3;
	add.s32 	%r815, %r814, 1;
	mul.wide.u32 	%rd88, %r815, 4;
	add.s64 	%rd89, %rd98, %rd88;
	st.volatile.global.u32 	[%rd89], %r822;
	add.s32 	%r816, %r814, 2;
	mul.wide.u32 	%rd90, %r816, 4;
	add.s64 	%rd91, %rd98, %rd90;
	mov.u32 	%r817, 0;
	st.volatile.global.u32 	[%rd91], %r817;
	add.s32 	%r818, %r814, 3;
	mul.wide.u32 	%rd92, %r818, 4;
	add.s64 	%rd93, %rd98, %rd92;
	st.volatile.global.u32 	[%rd93], %r22;
	shl.b32 	%r819, %r813, 1;
	mul.wide.u32 	%rd94, %r819, 4;
	add.s64 	%rd95, %rd99, %rd94;
	add.s32 	%r820, %r458, -1732584194;
	st.global.u32 	[%rd95], %r820;
	add.s32 	%r821, %r459, 271733878;
	st.global.u32 	[%rd95+4], %r821;

$L__BB0_30:
	ret;

}

  