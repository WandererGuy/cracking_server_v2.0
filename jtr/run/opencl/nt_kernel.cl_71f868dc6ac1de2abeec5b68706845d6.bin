//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.6
.target sm_86, texmode_independent
.address_size 64

	// .globl	nt
.const .align 4 .u32 halfShift = 10;
.const .align 4 .u32 halfBase = 65536;
.const .align 4 .u32 halfMask = 1023;
.const .align 1 .b8 opt_trailingBytesUTF8[64] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5};
.const .align 4 .b8 offsetsFromUTF8[24] = {0, 0, 0, 0, 128, 48, 0, 0, 128, 32, 14, 0, 128, 32, 200, 3, 128, 32, 8, 250, 128, 32, 8, 130};
// nt_$_s_bitmaps has been demoted

.entry nt(
	.param .u64 .ptr .global .align 4 nt_param_0,
	.param .u64 .ptr .global .align 4 nt_param_1,
	.param .u64 .ptr .global .align 4 nt_param_2,
	.param .u64 .ptr .const .align 4 nt_param_3,
	.param .u64 .ptr .global .align 4 nt_param_4,
	.param .u64 .ptr .global .align 4 nt_param_5,
	.param .u64 .ptr .global .align 4 nt_param_6,
	.param .u64 .ptr .global .align 4 nt_param_7,
	.param .u64 .ptr .global .align 4 nt_param_8
)
{
	.local .align 16 .b8 	__local_depot0[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<23>;
	.reg .b32 	%r<460>;
	.reg .b64 	%rd<82>;
	// demoted variable
	.shared .align 4 .b8 nt_$_s_bitmaps[16384];

	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd12, [nt_param_0];
	ld.param.u64 	%rd18, [nt_param_1];
	ld.param.u64 	%rd13, [nt_param_4];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r28, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r453, %tid.x;
	mov.b32 	%r29, %envreg3;
	add.s32 	%r30, %r453, %r29;
	mad.lo.s32 	%r3, %r1, %r28, %r30;
	mul.wide.u32 	%rd20, %r3, 4;
	add.s64 	%rd21, %rd18, %rd20;
	ld.global.u32 	%r4, [%rd21];
	mov.u32 	%r31, 0;
	st.local.v4.u32 	[%rd1], {%r31, %r31, %r31, %r31};
	st.local.v4.u32 	[%rd1+16], {%r31, %r31, %r31, %r31};
	st.local.v4.u32 	[%rd1+32], {%r31, %r31, %r31, %r31};
	st.local.v4.u32 	[%rd1+48], {%r31, %r31, %r31, %r31};
	and.b32  	%r5, %r4, 127;
	setp.gt.u32 	%p1, %r453, 4095;
	@%p1 bra 	$L__BB0_2;

$L__BB0_1:
	mul.wide.u32 	%rd22, %r453, 4;
	add.s64 	%rd23, %rd13, %rd22;
	ld.global.u32 	%r32, [%rd23];
	mov.u64 	%rd24, nt_$_s_bitmaps;
	add.s64 	%rd25, %rd24, %rd22;
	st.shared.u32 	[%rd25], %r32;
	add.s32 	%r453, %r453, %r1;
	setp.lt.u32 	%p2, %r453, 4096;
	@%p2 bra 	$L__BB0_1;

$L__BB0_2:
	bar.sync 	0;
	shr.u32 	%r33, %r4, 7;
	cvt.u64.u32 	%rd3, %r33;
	add.s32 	%r34, %r5, 3;
	shr.u32 	%r8, %r34, 2;
	setp.eq.s32 	%p3, %r8, 0;
	@%p3 bra 	$L__BB0_9;

	add.s32 	%r37, %r8, -1;
	and.b32  	%r459, %r8, 3;
	setp.lt.u32 	%p4, %r37, 3;
	mov.u32 	%r457, 0;
	mov.u32 	%r458, %r457;
	@%p4 bra 	$L__BB0_6;

	sub.s32 	%r456, %r8, %r459;
	mov.u32 	%r457, 0;

$L__BB0_5:
	cvt.u64.u32 	%rd26, %r458;
	add.s64 	%rd27, %rd26, %rd3;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd29, %rd12, %rd28;
	ld.global.u32 	%r40, [%rd29];
	and.b32  	%r41, %r40, 255;
	and.b32  	%r42, %r40, 65280;
	prmt.b32 	%r43, %r42, %r41, 8452;
	mul.wide.u32 	%rd30, %r457, 4;
	add.s64 	%rd31, %rd1, %rd30;
	st.local.u32 	[%rd31], %r43;
	and.b32  	%r44, %r40, 16711680;
	shr.u32 	%r45, %r44, 16;
	shr.u32 	%r46, %r40, 8;
	and.b32  	%r47, %r46, 16711680;
	or.b32  	%r48, %r45, %r47;
	st.local.u32 	[%rd31+4], %r48;
	ld.global.u32 	%r49, [%rd29+4];
	and.b32  	%r50, %r49, 255;
	and.b32  	%r51, %r49, 65280;
	prmt.b32 	%r52, %r51, %r50, 8452;
	st.local.u32 	[%rd31+8], %r52;
	and.b32  	%r53, %r49, 16711680;
	shr.u32 	%r54, %r53, 16;
	shr.u32 	%r55, %r49, 8;
	and.b32  	%r56, %r55, 16711680;
	or.b32  	%r57, %r54, %r56;
	st.local.u32 	[%rd31+12], %r57;
	ld.global.u32 	%r58, [%rd29+8];
	and.b32  	%r59, %r58, 255;
	and.b32  	%r60, %r58, 65280;
	prmt.b32 	%r61, %r60, %r59, 8452;
	st.local.u32 	[%rd31+16], %r61;
	and.b32  	%r62, %r58, 16711680;
	shr.u32 	%r63, %r62, 16;
	shr.u32 	%r64, %r58, 8;
	and.b32  	%r65, %r64, 16711680;
	or.b32  	%r66, %r63, %r65;
	st.local.u32 	[%rd31+20], %r66;
	ld.global.u32 	%r67, [%rd29+12];
	and.b32  	%r68, %r67, 255;
	and.b32  	%r69, %r67, 65280;
	prmt.b32 	%r70, %r69, %r68, 8452;
	st.local.u32 	[%rd31+24], %r70;
	and.b32  	%r71, %r67, 16711680;
	shr.u32 	%r72, %r71, 16;
	shr.u32 	%r73, %r67, 8;
	and.b32  	%r74, %r73, 16711680;
	or.b32  	%r75, %r72, %r74;
	add.s32 	%r457, %r457, 8;
	st.local.u32 	[%rd31+28], %r75;
	add.s32 	%r458, %r458, 4;
	add.s32 	%r456, %r456, -4;
	setp.ne.s32 	%p5, %r456, 0;
	@%p5 bra 	$L__BB0_5;

$L__BB0_6:
	setp.eq.s32 	%p6, %r459, 0;
	@%p6 bra 	$L__BB0_9;

	mul.wide.u32 	%rd32, %r457, 4;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd81, %rd33, 4;
	cvt.u64.u32 	%rd34, %r458;
	add.s64 	%rd35, %rd3, %rd34;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd80, %rd12, %rd36;

$L__BB0_8:
	.pragma "nounroll";
	ld.global.u32 	%r76, [%rd80];
	and.b32  	%r77, %r76, 255;
	and.b32  	%r78, %r76, 65280;
	prmt.b32 	%r79, %r78, %r77, 8452;
	st.local.u32 	[%rd81+-4], %r79;
	and.b32  	%r80, %r76, 16711680;
	shr.u32 	%r81, %r80, 16;
	shr.u32 	%r82, %r76, 8;
	and.b32  	%r83, %r82, 16711680;
	or.b32  	%r84, %r81, %r83;
	st.local.u32 	[%rd81], %r84;
	add.s64 	%rd81, %rd81, 8;
	add.s64 	%rd80, %rd80, 4;
	add.s32 	%r459, %r459, -1;
	setp.ne.s32 	%p7, %r459, 0;
	@%p7 bra 	$L__BB0_8;

$L__BB0_9:
	shr.u32 	%r253, %r5, 1;
	mul.wide.u32 	%rd37, %r253, 4;
	add.s64 	%rd38, %rd1, %rd37;
	ld.local.u16 	%r254, [%rd38];
	shl.b32 	%r255, %r4, 4;
	and.b32  	%r256, %r255, 16;
	mov.u32 	%r257, 128;
	shl.b32 	%r258, %r257, %r256;
	or.b32  	%r259, %r254, %r258;
	st.local.u32 	[%rd38], %r259;
	add.s32 	%r260, %r5, 36;
	shr.u32 	%r261, %r260, 1;
	and.b32  	%r262, %r261, 112;
	add.s32 	%r263, %r262, -2;
	mul.wide.u32 	%rd39, %r263, 4;
	add.s64 	%rd40, %rd1, %rd39;
	shl.b32 	%r264, %r5, 4;
	st.local.u32 	[%rd40], %r264;
	ld.local.v4.u32 	{%r265, %r266, %r267, %r268}, [%rd1];
	add.s32 	%r272, %r265, -1;
	shf.l.wrap.b32 	%r92, %r272, %r272, 3;
	and.b32  	%r273, %r92, 2004318071;
	xor.b32  	%r274, %r273, -1732584194;
	add.s32 	%r275, %r266, %r274;
	add.s32 	%r276, %r275, 271733878;
	shf.l.wrap.b32 	%r96, %r276, %r276, 7;
	mov.u32 	%r88, -271733879;
	// begin inline asm
	lop3.b32 %r85, %r96, %r92, %r88, 202;
	// end inline asm
	add.s32 	%r277, %r85, %r267;
	add.s32 	%r278, %r277, -1732584194;
	shf.l.wrap.b32 	%r100, %r278, %r278, 11;
	// begin inline asm
	lop3.b32 %r89, %r100, %r96, %r92, 202;
	// end inline asm
	add.s32 	%r279, %r89, %r268;
	add.s32 	%r280, %r279, -271733879;
	shf.l.wrap.b32 	%r104, %r280, %r280, 19;
	// begin inline asm
	lop3.b32 %r93, %r104, %r100, %r96, 202;
	// end inline asm
	ld.local.v4.u32 	{%r281, %r282, %r283, %r284}, [%rd1+16];
	add.s32 	%r289, %r93, %r92;
	add.s32 	%r290, %r289, %r281;
	shf.l.wrap.b32 	%r108, %r290, %r290, 3;
	// begin inline asm
	lop3.b32 %r97, %r108, %r104, %r100, 202;
	// end inline asm
	add.s32 	%r291, %r97, %r96;
	add.s32 	%r292, %r291, %r282;
	shf.l.wrap.b32 	%r112, %r292, %r292, 7;
	// begin inline asm
	lop3.b32 %r101, %r112, %r108, %r104, 202;
	// end inline asm
	add.s32 	%r293, %r101, %r100;
	add.s32 	%r294, %r293, %r283;
	shf.l.wrap.b32 	%r116, %r294, %r294, 11;
	// begin inline asm
	lop3.b32 %r105, %r116, %r112, %r108, 202;
	// end inline asm
	add.s32 	%r295, %r105, %r104;
	add.s32 	%r296, %r295, %r284;
	shf.l.wrap.b32 	%r120, %r296, %r296, 19;
	// begin inline asm
	lop3.b32 %r109, %r120, %r116, %r112, 202;
	// end inline asm
	ld.local.v4.u32 	{%r297, %r298, %r299, %r300}, [%rd1+32];
	add.s32 	%r305, %r109, %r108;
	add.s32 	%r306, %r305, %r297;
	shf.l.wrap.b32 	%r124, %r306, %r306, 3;
	// begin inline asm
	lop3.b32 %r113, %r124, %r120, %r116, 202;
	// end inline asm
	add.s32 	%r307, %r113, %r112;
	add.s32 	%r308, %r307, %r298;
	shf.l.wrap.b32 	%r128, %r308, %r308, 7;
	// begin inline asm
	lop3.b32 %r117, %r128, %r124, %r120, 202;
	// end inline asm
	add.s32 	%r309, %r117, %r116;
	add.s32 	%r310, %r309, %r299;
	shf.l.wrap.b32 	%r132, %r310, %r310, 11;
	// begin inline asm
	lop3.b32 %r121, %r132, %r128, %r124, 202;
	// end inline asm
	add.s32 	%r311, %r121, %r120;
	add.s32 	%r312, %r311, %r300;
	shf.l.wrap.b32 	%r136, %r312, %r312, 19;
	// begin inline asm
	lop3.b32 %r125, %r136, %r132, %r128, 202;
	// end inline asm
	ld.local.v2.u32 	{%r313, %r314}, [%rd1+48];
	add.s32 	%r317, %r125, %r124;
	add.s32 	%r318, %r317, %r313;
	shf.l.wrap.b32 	%r140, %r318, %r318, 3;
	// begin inline asm
	lop3.b32 %r129, %r140, %r136, %r132, 202;
	// end inline asm
	add.s32 	%r319, %r129, %r128;
	add.s32 	%r320, %r319, %r314;
	shf.l.wrap.b32 	%r144, %r320, %r320, 7;
	// begin inline asm
	lop3.b32 %r133, %r144, %r140, %r136, 202;
	// end inline asm
	add.s32 	%r321, %r132, %r264;
	add.s32 	%r322, %r321, %r133;
	shf.l.wrap.b32 	%r148, %r322, %r322, 11;
	// begin inline asm
	lop3.b32 %r137, %r148, %r144, %r140, 202;
	// end inline asm
	add.s32 	%r323, %r137, %r136;
	shf.l.wrap.b32 	%r152, %r323, %r323, 19;
	// begin inline asm
	lop3.b32 %r141, %r152, %r148, %r144, 232;
	// end inline asm
	add.s32 	%r324, %r140, %r141;
	add.s32 	%r325, %r324, %r265;
	add.s32 	%r326, %r325, 1518500249;
	shf.l.wrap.b32 	%r156, %r326, %r326, 3;
	// begin inline asm
	lop3.b32 %r145, %r156, %r152, %r148, 232;
	// end inline asm
	add.s32 	%r327, %r144, %r145;
	add.s32 	%r328, %r327, %r281;
	add.s32 	%r329, %r328, 1518500249;
	shf.l.wrap.b32 	%r160, %r329, %r329, 5;
	// begin inline asm
	lop3.b32 %r149, %r160, %r156, %r152, 232;
	// end inline asm
	add.s32 	%r330, %r148, %r149;
	add.s32 	%r331, %r330, %r297;
	add.s32 	%r332, %r331, 1518500249;
	shf.l.wrap.b32 	%r164, %r332, %r332, 9;
	// begin inline asm
	lop3.b32 %r153, %r164, %r160, %r156, 232;
	// end inline asm
	add.s32 	%r333, %r152, %r153;
	add.s32 	%r334, %r333, %r313;
	add.s32 	%r335, %r334, 1518500249;
	shf.l.wrap.b32 	%r168, %r335, %r335, 13;
	// begin inline asm
	lop3.b32 %r157, %r168, %r164, %r160, 232;
	// end inline asm
	add.s32 	%r336, %r156, %r157;
	add.s32 	%r337, %r336, %r266;
	add.s32 	%r338, %r337, 1518500249;
	shf.l.wrap.b32 	%r172, %r338, %r338, 3;
	// begin inline asm
	lop3.b32 %r161, %r172, %r168, %r164, 232;
	// end inline asm
	add.s32 	%r339, %r160, %r161;
	add.s32 	%r340, %r339, %r282;
	add.s32 	%r341, %r340, 1518500249;
	shf.l.wrap.b32 	%r176, %r341, %r341, 5;
	// begin inline asm
	lop3.b32 %r165, %r176, %r172, %r168, 232;
	// end inline asm
	add.s32 	%r342, %r164, %r165;
	add.s32 	%r343, %r342, %r298;
	add.s32 	%r344, %r343, 1518500249;
	shf.l.wrap.b32 	%r180, %r344, %r344, 9;
	// begin inline asm
	lop3.b32 %r169, %r180, %r176, %r172, 232;
	// end inline asm
	add.s32 	%r345, %r168, %r169;
	add.s32 	%r346, %r345, %r314;
	add.s32 	%r347, %r346, 1518500249;
	shf.l.wrap.b32 	%r184, %r347, %r347, 13;
	// begin inline asm
	lop3.b32 %r173, %r184, %r180, %r176, 232;
	// end inline asm
	add.s32 	%r348, %r172, %r173;
	add.s32 	%r349, %r348, %r267;
	add.s32 	%r350, %r349, 1518500249;
	shf.l.wrap.b32 	%r188, %r350, %r350, 3;
	// begin inline asm
	lop3.b32 %r177, %r188, %r184, %r180, 232;
	// end inline asm
	add.s32 	%r351, %r176, %r177;
	add.s32 	%r352, %r351, %r283;
	add.s32 	%r353, %r352, 1518500249;
	shf.l.wrap.b32 	%r192, %r353, %r353, 5;
	// begin inline asm
	lop3.b32 %r181, %r192, %r188, %r184, 232;
	// end inline asm
	add.s32 	%r354, %r180, %r181;
	add.s32 	%r355, %r354, %r299;
	add.s32 	%r356, %r355, 1518500249;
	shf.l.wrap.b32 	%r196, %r356, %r356, 9;
	// begin inline asm
	lop3.b32 %r185, %r196, %r192, %r188, 232;
	// end inline asm
	add.s32 	%r357, %r264, %r184;
	add.s32 	%r358, %r357, %r185;
	add.s32 	%r359, %r358, 1518500249;
	shf.l.wrap.b32 	%r200, %r359, %r359, 13;
	// begin inline asm
	lop3.b32 %r189, %r200, %r196, %r192, 232;
	// end inline asm
	add.s32 	%r360, %r188, %r189;
	add.s32 	%r361, %r360, %r268;
	add.s32 	%r362, %r361, 1518500249;
	shf.l.wrap.b32 	%r204, %r362, %r362, 3;
	// begin inline asm
	lop3.b32 %r193, %r204, %r200, %r196, 232;
	// end inline asm
	add.s32 	%r363, %r192, %r193;
	add.s32 	%r364, %r363, %r284;
	add.s32 	%r365, %r364, 1518500249;
	shf.l.wrap.b32 	%r208, %r365, %r365, 5;
	// begin inline asm
	lop3.b32 %r197, %r208, %r204, %r200, 232;
	// end inline asm
	add.s32 	%r366, %r196, %r197;
	add.s32 	%r367, %r366, %r300;
	add.s32 	%r368, %r367, 1518500249;
	shf.l.wrap.b32 	%r212, %r368, %r368, 9;
	// begin inline asm
	lop3.b32 %r201, %r212, %r208, %r204, 232;
	// end inline asm
	add.s32 	%r369, %r200, %r201;
	add.s32 	%r370, %r369, 1518500249;
	shf.l.wrap.b32 	%r216, %r370, %r370, 13;
	// begin inline asm
	lop3.b32 %r205, %r216, %r212, %r208, 150;
	// end inline asm
	add.s32 	%r371, %r204, %r205;
	add.s32 	%r372, %r371, %r265;
	add.s32 	%r373, %r372, 1859775393;
	shf.l.wrap.b32 	%r220, %r373, %r373, 3;
	// begin inline asm
	lop3.b32 %r209, %r220, %r216, %r212, 150;
	// end inline asm
	add.s32 	%r374, %r208, %r209;
	add.s32 	%r375, %r374, %r297;
	add.s32 	%r376, %r375, 1859775393;
	shf.l.wrap.b32 	%r224, %r376, %r376, 9;
	// begin inline asm
	lop3.b32 %r213, %r224, %r220, %r216, 150;
	// end inline asm
	add.s32 	%r377, %r212, %r213;
	add.s32 	%r378, %r377, %r281;
	add.s32 	%r379, %r378, 1859775393;
	shf.l.wrap.b32 	%r228, %r379, %r379, 11;
	// begin inline asm
	lop3.b32 %r217, %r228, %r224, %r220, 150;
	// end inline asm
	add.s32 	%r380, %r216, %r217;
	add.s32 	%r381, %r380, %r313;
	add.s32 	%r382, %r381, 1859775393;
	shf.l.wrap.b32 	%r232, %r382, %r382, 15;
	// begin inline asm
	lop3.b32 %r221, %r232, %r228, %r224, 150;
	// end inline asm
	add.s32 	%r383, %r220, %r221;
	add.s32 	%r384, %r383, %r267;
	add.s32 	%r385, %r384, 1859775393;
	shf.l.wrap.b32 	%r236, %r385, %r385, 3;
	// begin inline asm
	lop3.b32 %r225, %r236, %r232, %r228, 150;
	// end inline asm
	add.s32 	%r386, %r224, %r225;
	add.s32 	%r387, %r386, %r299;
	add.s32 	%r388, %r387, 1859775393;
	shf.l.wrap.b32 	%r240, %r388, %r388, 9;
	// begin inline asm
	lop3.b32 %r229, %r240, %r236, %r232, 150;
	// end inline asm
	add.s32 	%r389, %r228, %r229;
	add.s32 	%r390, %r389, %r283;
	add.s32 	%r391, %r390, 1859775393;
	shf.l.wrap.b32 	%r244, %r391, %r391, 11;
	// begin inline asm
	lop3.b32 %r233, %r244, %r240, %r236, 150;
	// end inline asm
	add.s32 	%r392, %r264, %r232;
	add.s32 	%r393, %r392, %r233;
	add.s32 	%r394, %r393, 1859775393;
	shf.l.wrap.b32 	%r248, %r394, %r394, 15;
	// begin inline asm
	lop3.b32 %r237, %r248, %r244, %r240, 150;
	// end inline asm
	add.s32 	%r395, %r236, %r237;
	add.s32 	%r396, %r395, %r266;
	add.s32 	%r397, %r396, 1859775393;
	shf.l.wrap.b32 	%r252, %r397, %r397, 3;
	// begin inline asm
	lop3.b32 %r241, %r252, %r248, %r244, 150;
	// end inline asm
	add.s32 	%r398, %r240, %r241;
	add.s32 	%r399, %r398, %r298;
	add.s32 	%r400, %r399, 1859775393;
	shf.l.wrap.b32 	%r251, %r400, %r400, 9;
	// begin inline asm
	lop3.b32 %r245, %r251, %r252, %r248, 150;
	// end inline asm
	add.s32 	%r401, %r244, %r245;
	add.s32 	%r402, %r401, %r282;
	add.s32 	%r403, %r402, 1859775393;
	shf.l.wrap.b32 	%r250, %r403, %r403, 11;
	// begin inline asm
	lop3.b32 %r249, %r250, %r251, %r252, 150;
	// end inline asm
	add.s32 	%r404, %r249, %r248;
	add.s32 	%r25, %r404, %r314;
	and.b32  	%r405, %r25, 32736;
	shr.u32 	%r406, %r405, 5;
	mul.wide.u32 	%rd41, %r406, 4;
	mov.u64 	%rd42, nt_$_s_bitmaps;
	add.s64 	%rd43, %rd42, %rd41;
	and.b32  	%r407, %r25, 31;
	ld.shared.u32 	%r408, [%rd43];
	shr.u32 	%r409, %r408, %r407;
	shr.u32 	%r410, %r25, 16;
	shr.u32 	%r411, %r25, 21;
	or.b32  	%r412, %r411, 1024;
	mul.wide.u32 	%rd44, %r412, 4;
	add.s64 	%rd45, %rd42, %rd44;
	and.b32  	%r413, %r410, 31;
	ld.shared.u32 	%r414, [%rd45];
	shr.u32 	%r415, %r414, %r413;
	and.b32  	%r416, %r415, 1;
	setp.eq.b32 	%p8, %r416, 1;
	and.b32  	%r417, %r409, 1;
	setp.eq.b32 	%p9, %r417, 1;
	and.pred  	%p10, %p9, %p8;
	mov.pred 	%p11, 0;
	xor.pred  	%p12, %p10, %p11;
	not.pred 	%p13, %p12;
	@%p13 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_10;

$L__BB0_15:
	ret;

$L__BB0_10:
	add.s32 	%r422, %r25, 1859775393;
	shf.l.wrap.b32 	%r421, %r422, %r422, 15;
	// begin inline asm
	lop3.b32 %r418, %r251, %r250, %r421, 150;
	// end inline asm
	add.s32 	%r423, %r252, %r418;
	add.s32 	%r424, %r423, %r268;
	add.s32 	%r425, %r424, 1859775393;
	shf.l.wrap.b32 	%r26, %r425, %r425, 3;
	and.b32  	%r426, %r26, 32736;
	shr.u32 	%r427, %r426, 5;
	or.b32  	%r428, %r427, 2048;
	mul.wide.u32 	%rd46, %r428, 4;
	add.s64 	%rd48, %rd42, %rd46;
	and.b32  	%r429, %r26, 31;
	ld.shared.u32 	%r430, [%rd48];
	shr.u32 	%r431, %r430, %r429;
	shr.u32 	%r432, %r26, 16;
	shr.u32 	%r433, %r26, 21;
	or.b32  	%r434, %r433, 3072;
	mul.wide.u32 	%rd49, %r434, 4;
	add.s64 	%rd50, %rd42, %rd49;
	and.b32  	%r435, %r432, 31;
	ld.shared.u32 	%r436, [%rd50];
	shr.u32 	%r437, %r436, %r435;
	and.b32  	%r438, %r437, 1;
	setp.eq.b32 	%p14, %r438, 1;
	and.b32  	%r439, %r431, 1;
	setp.eq.b32 	%p15, %r439, 1;
	and.pred  	%p16, %p15, %p14;
	xor.pred  	%p18, %p16, %p11;
	not.pred 	%p19, %p18;
	@%p19 bra 	$L__BB0_15;

	ld.param.u64 	%rd77, [nt_param_6];
	ld.param.u64 	%rd76, [nt_param_5];
	cvt.u64.u32 	%rd51, %r25;
	cvt.u64.u32 	%rd52, %r26;
	bfi.b64 	%rd53, %rd51, %rd52, 32, 32;
	shr.u64 	%rd54, %rd53, 2;
	mul.hi.u64 	%rd55, %rd54, 5270498306774157605;
	shr.u64 	%rd56, %rd55, 1;
	mul.lo.s64 	%rd57, %rd56, 28;
	sub.s64 	%rd58, %rd53, %rd57;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd60, %rd76, %rd59;
	ld.global.u32 	%rd61, [%rd60];
	add.s64 	%rd62, %rd53, %rd61;
	mul.hi.u64 	%rd63, %rd62, 3279421168659475843;
	shr.u64 	%rd64, %rd63, 3;
	mul.lo.s64 	%rd65, %rd64, 45;
	sub.s64 	%rd10, %rd62, %rd65;
	cvt.u32.u64 	%r27, %rd10;
	shl.b64 	%rd66, %rd10, 2;
	add.s64 	%rd11, %rd77, %rd66;
	ld.global.u32 	%r440, [%rd11];
	setp.ne.s32 	%p20, %r440, %r26;
	@%p20 bra 	$L__BB0_15;

	ld.global.u32 	%r441, [%rd11+180];
	setp.ne.s32 	%p21, %r441, %r25;
	@%p21 bra 	$L__BB0_15;

	ld.param.u64 	%rd78, [nt_param_8];
	shr.u64 	%rd67, %rd10, 3;
	and.b64  	%rd68, %rd67, 2305843009213693948;
	add.s64 	%rd69, %rd78, %rd68;
	and.b32  	%r442, %r27, 31;
	mov.u32 	%r443, 1;
	shl.b32 	%r444, %r443, %r442;
	atom.global.or.b32 	%r445, [%rd69], %r444;
	and.b32  	%r446, %r445, %r444;
	setp.ne.s32 	%p22, %r446, 0;
	@%p22 bra 	$L__BB0_15;

	ld.param.u64 	%rd79, [nt_param_7];
	atom.global.add.u32 	%r447, [%rd79], 1;
	mul.lo.s32 	%r448, %r447, 3;
	add.s32 	%r449, %r448, 1;
	mul.wide.u32 	%rd70, %r449, 4;
	add.s64 	%rd71, %rd79, %rd70;
	st.volatile.global.u32 	[%rd71], %r3;
	add.s32 	%r450, %r448, 2;
	mul.wide.u32 	%rd72, %r450, 4;
	add.s64 	%rd73, %rd79, %rd72;
	mov.u32 	%r451, 0;
	st.volatile.global.u32 	[%rd73], %r451;
	add.s32 	%r452, %r448, 3;
	mul.wide.u32 	%rd74, %r452, 4;
	add.s64 	%rd75, %rd79, %rd74;
	st.volatile.global.u32 	[%rd75], %r27;
	bra.uni 	$L__BB0_15;

}

  